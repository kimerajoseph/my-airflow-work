[2022-12-08 15:33:49,505] {processor.py:153} INFO - Started process (PID=681) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:33:49,505] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 15:33:49,506] {logging_mixin.py:115} INFO - [2022-12-08 15:33:49,506] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:33:50,030] {logging_mixin.py:115} INFO - [2022-12-08 15:33:50,030] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:33:50,602] {logging_mixin.py:115} INFO - [2022-12-08 15:33:50,602] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:33:50,755] {logging_mixin.py:115} INFO - [2022-12-08 15:33:50,750] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 16, in read_from_s3_and_insert_redshift
    conn = wr.redshift.connect(glue_connection)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 511, in connect
    connection=connection, secret_id=secret_id, catalog_id=catalog_id, dbname=dbname, boto3_session=boto3_session
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_databases.py", line 113, in get_connection_attributes
    connection=connection, catalog_id=catalog_id, dbname=dbname, boto3_session=boto3_session
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_databases.py", line 41, in _get_connection_attributes_from_catalog
    details: Dict[str, Any] = get_connection(name=connection, catalog_id=catalog_id, boto3_session=boto3_session)[
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/catalog/_get.py", line 612, in get_connection
    client_glue: boto3.client = _utils.client(service_name="glue", session=boto3_session)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_utils.py", line 119, in client
    verify=verify or _config.config.verify,
  File "/home/airflow/.local/lib/python3.7/site-packages/boto3/session.py", line 309, in client
    config=config,
  File "/home/airflow/.local/lib/python3.7/site-packages/botocore/session.py", line 888, in create_client
    client_config=config, api_version=api_version)
  File "/home/airflow/.local/lib/python3.7/site-packages/botocore/client.py", line 104, in create_client
    verify, credentials, scoped_config, client_config, endpoint_bridge)
  File "/home/airflow/.local/lib/python3.7/site-packages/botocore/client.py", line 386, in _get_client_args
    verify, credentials, scoped_config, client_config, endpoint_bridge)
  File "/home/airflow/.local/lib/python3.7/site-packages/botocore/args.py", line 73, in get_client_args
    endpoint_url, is_secure, scoped_config)
  File "/home/airflow/.local/lib/python3.7/site-packages/botocore/args.py", line 154, in compute_client_args
    s3_config=s3_config,
  File "/home/airflow/.local/lib/python3.7/site-packages/botocore/args.py", line 234, in _compute_endpoint_config
    return self._resolve_endpoint(**resolve_endpoint_kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/botocore/args.py", line 321, in _resolve_endpoint
    service_name, region_name, endpoint_url, is_secure)
  File "/home/airflow/.local/lib/python3.7/site-packages/botocore/client.py", line 468, in resolve
    use_fips_endpoint=use_fips_endpoint,
  File "/home/airflow/.local/lib/python3.7/site-packages/botocore/regions.py", line 198, in construct_endpoint
    use_fips_endpoint
  File "/home/airflow/.local/lib/python3.7/site-packages/botocore/regions.py", line 230, in _endpoint_for_partition
    raise NoRegionError()
botocore.exceptions.NoRegionError: You must specify a region.
[2022-12-08 15:33:50,756] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:33:50,764] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.261 seconds
[2022-12-08 15:34:21,441] {processor.py:153} INFO - Started process (PID=752) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:34:21,441] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 15:34:21,441] {logging_mixin.py:115} INFO - [2022-12-08 15:34:21,441] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:34:21,956] {logging_mixin.py:115} INFO - [2022-12-08 15:34:21,956] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:34:22,515] {logging_mixin.py:115} INFO - [2022-12-08 15:34:22,515] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:34:22,660] {logging_mixin.py:115} INFO - [2022-12-08 15:34:22,658] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 16, in read_from_s3_and_insert_redshift
    conn = wr.redshift.connect(glue_connection)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 511, in connect
    connection=connection, secret_id=secret_id, catalog_id=catalog_id, dbname=dbname, boto3_session=boto3_session
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_databases.py", line 113, in get_connection_attributes
    connection=connection, catalog_id=catalog_id, dbname=dbname, boto3_session=boto3_session
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_databases.py", line 41, in _get_connection_attributes_from_catalog
    details: Dict[str, Any] = get_connection(name=connection, catalog_id=catalog_id, boto3_session=boto3_session)[
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/catalog/_get.py", line 612, in get_connection
    client_glue: boto3.client = _utils.client(service_name="glue", session=boto3_session)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_utils.py", line 119, in client
    verify=verify or _config.config.verify,
  File "/home/airflow/.local/lib/python3.7/site-packages/boto3/session.py", line 309, in client
    config=config,
  File "/home/airflow/.local/lib/python3.7/site-packages/botocore/session.py", line 888, in create_client
    client_config=config, api_version=api_version)
  File "/home/airflow/.local/lib/python3.7/site-packages/botocore/client.py", line 104, in create_client
    verify, credentials, scoped_config, client_config, endpoint_bridge)
  File "/home/airflow/.local/lib/python3.7/site-packages/botocore/client.py", line 386, in _get_client_args
    verify, credentials, scoped_config, client_config, endpoint_bridge)
  File "/home/airflow/.local/lib/python3.7/site-packages/botocore/args.py", line 73, in get_client_args
    endpoint_url, is_secure, scoped_config)
  File "/home/airflow/.local/lib/python3.7/site-packages/botocore/args.py", line 154, in compute_client_args
    s3_config=s3_config,
  File "/home/airflow/.local/lib/python3.7/site-packages/botocore/args.py", line 234, in _compute_endpoint_config
    return self._resolve_endpoint(**resolve_endpoint_kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/botocore/args.py", line 321, in _resolve_endpoint
    service_name, region_name, endpoint_url, is_secure)
  File "/home/airflow/.local/lib/python3.7/site-packages/botocore/client.py", line 468, in resolve
    use_fips_endpoint=use_fips_endpoint,
  File "/home/airflow/.local/lib/python3.7/site-packages/botocore/regions.py", line 198, in construct_endpoint
    use_fips_endpoint
  File "/home/airflow/.local/lib/python3.7/site-packages/botocore/regions.py", line 230, in _endpoint_for_partition
    raise NoRegionError()
botocore.exceptions.NoRegionError: You must specify a region.
[2022-12-08 15:34:22,661] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:34:22,668] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.230 seconds
[2022-12-08 15:34:53,339] {processor.py:153} INFO - Started process (PID=823) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:34:53,340] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 15:34:53,340] {logging_mixin.py:115} INFO - [2022-12-08 15:34:53,340] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:34:53,852] {logging_mixin.py:115} INFO - [2022-12-08 15:34:53,852] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:34:54,441] {logging_mixin.py:115} INFO - [2022-12-08 15:34:54,440] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:34:54,587] {logging_mixin.py:115} INFO - [2022-12-08 15:34:54,585] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 16, in read_from_s3_and_insert_redshift
    conn = wr.redshift.connect(glue_connection)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 511, in connect
    connection=connection, secret_id=secret_id, catalog_id=catalog_id, dbname=dbname, boto3_session=boto3_session
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_databases.py", line 113, in get_connection_attributes
    connection=connection, catalog_id=catalog_id, dbname=dbname, boto3_session=boto3_session
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_databases.py", line 41, in _get_connection_attributes_from_catalog
    details: Dict[str, Any] = get_connection(name=connection, catalog_id=catalog_id, boto3_session=boto3_session)[
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/catalog/_get.py", line 612, in get_connection
    client_glue: boto3.client = _utils.client(service_name="glue", session=boto3_session)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_utils.py", line 119, in client
    verify=verify or _config.config.verify,
  File "/home/airflow/.local/lib/python3.7/site-packages/boto3/session.py", line 309, in client
    config=config,
  File "/home/airflow/.local/lib/python3.7/site-packages/botocore/session.py", line 888, in create_client
    client_config=config, api_version=api_version)
  File "/home/airflow/.local/lib/python3.7/site-packages/botocore/client.py", line 104, in create_client
    verify, credentials, scoped_config, client_config, endpoint_bridge)
  File "/home/airflow/.local/lib/python3.7/site-packages/botocore/client.py", line 386, in _get_client_args
    verify, credentials, scoped_config, client_config, endpoint_bridge)
  File "/home/airflow/.local/lib/python3.7/site-packages/botocore/args.py", line 73, in get_client_args
    endpoint_url, is_secure, scoped_config)
  File "/home/airflow/.local/lib/python3.7/site-packages/botocore/args.py", line 154, in compute_client_args
    s3_config=s3_config,
  File "/home/airflow/.local/lib/python3.7/site-packages/botocore/args.py", line 234, in _compute_endpoint_config
    return self._resolve_endpoint(**resolve_endpoint_kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/botocore/args.py", line 321, in _resolve_endpoint
    service_name, region_name, endpoint_url, is_secure)
  File "/home/airflow/.local/lib/python3.7/site-packages/botocore/client.py", line 468, in resolve
    use_fips_endpoint=use_fips_endpoint,
  File "/home/airflow/.local/lib/python3.7/site-packages/botocore/regions.py", line 198, in construct_endpoint
    use_fips_endpoint
  File "/home/airflow/.local/lib/python3.7/site-packages/botocore/regions.py", line 230, in _endpoint_for_partition
    raise NoRegionError()
botocore.exceptions.NoRegionError: You must specify a region.
[2022-12-08 15:34:54,588] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:34:54,595] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.258 seconds
[2022-12-08 15:36:05,409] {processor.py:153} INFO - Started process (PID=47) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:36:05,410] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 15:36:05,410] {logging_mixin.py:115} INFO - [2022-12-08 15:36:05,410] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:36:06,780] {logging_mixin.py:115} INFO - [2022-12-08 15:36:06,780] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:36:07,296] {logging_mixin.py:115} INFO - [2022-12-08 15:36:07,296] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:36:07,838] {logging_mixin.py:115} INFO - [2022-12-08 15:36:07,838] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:36:07,839] {logging_mixin.py:115} INFO - [2022-12-08 15:36:07,838] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:36:07,840] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:36:07,849] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 2.442 seconds
[2022-12-08 15:36:38,052] {processor.py:153} INFO - Started process (PID=126) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:36:38,052] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 15:36:38,053] {logging_mixin.py:115} INFO - [2022-12-08 15:36:38,053] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:36:38,570] {logging_mixin.py:115} INFO - [2022-12-08 15:36:38,570] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:36:39,042] {logging_mixin.py:115} INFO - [2022-12-08 15:36:39,042] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:36:39,582] {logging_mixin.py:115} INFO - [2022-12-08 15:36:39,582] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:36:39,583] {logging_mixin.py:115} INFO - [2022-12-08 15:36:39,582] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:36:39,584] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:36:39,593] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.544 seconds
[2022-12-08 15:37:10,401] {processor.py:153} INFO - Started process (PID=212) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:37:10,401] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 15:37:10,402] {logging_mixin.py:115} INFO - [2022-12-08 15:37:10,402] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:37:10,932] {logging_mixin.py:115} INFO - [2022-12-08 15:37:10,932] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:37:11,413] {logging_mixin.py:115} INFO - [2022-12-08 15:37:11,413] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:37:11,900] {logging_mixin.py:115} INFO - [2022-12-08 15:37:11,900] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:37:11,901] {logging_mixin.py:115} INFO - [2022-12-08 15:37:11,900] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:37:11,902] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:37:11,911] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.513 seconds
[2022-12-08 15:37:42,735] {processor.py:153} INFO - Started process (PID=298) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:37:42,735] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 15:37:42,736] {logging_mixin.py:115} INFO - [2022-12-08 15:37:42,736] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:37:43,258] {logging_mixin.py:115} INFO - [2022-12-08 15:37:43,258] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:37:43,729] {logging_mixin.py:115} INFO - [2022-12-08 15:37:43,729] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:37:44,248] {logging_mixin.py:115} INFO - [2022-12-08 15:37:44,248] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:37:44,249] {logging_mixin.py:115} INFO - [2022-12-08 15:37:44,249] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:37:44,250] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:37:44,260] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.527 seconds
[2022-12-08 15:38:14,551] {processor.py:153} INFO - Started process (PID=369) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:38:14,551] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 15:38:14,552] {logging_mixin.py:115} INFO - [2022-12-08 15:38:14,552] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:38:15,059] {logging_mixin.py:115} INFO - [2022-12-08 15:38:15,059] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:38:15,524] {logging_mixin.py:115} INFO - [2022-12-08 15:38:15,524] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:38:16,056] {logging_mixin.py:115} INFO - [2022-12-08 15:38:16,056] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:38:16,057] {logging_mixin.py:115} INFO - [2022-12-08 15:38:16,056] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:38:16,058] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:38:16,067] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.519 seconds
[2022-12-08 15:38:46,953] {processor.py:153} INFO - Started process (PID=440) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:38:46,953] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 15:38:46,954] {logging_mixin.py:115} INFO - [2022-12-08 15:38:46,954] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:38:47,475] {logging_mixin.py:115} INFO - [2022-12-08 15:38:47,475] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:38:47,938] {logging_mixin.py:115} INFO - [2022-12-08 15:38:47,938] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:38:48,445] {logging_mixin.py:115} INFO - [2022-12-08 15:38:48,445] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:38:48,446] {logging_mixin.py:115} INFO - [2022-12-08 15:38:48,445] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:38:48,447] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:38:48,455] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.505 seconds
[2022-12-08 15:39:19,375] {processor.py:153} INFO - Started process (PID=509) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:39:19,376] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 15:39:19,376] {logging_mixin.py:115} INFO - [2022-12-08 15:39:19,376] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:39:19,891] {logging_mixin.py:115} INFO - [2022-12-08 15:39:19,891] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:39:20,357] {logging_mixin.py:115} INFO - [2022-12-08 15:39:20,357] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:39:20,854] {logging_mixin.py:115} INFO - [2022-12-08 15:39:20,854] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:39:20,855] {logging_mixin.py:115} INFO - [2022-12-08 15:39:20,854] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:39:20,856] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:39:20,865] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.492 seconds
[2022-12-08 15:39:51,807] {processor.py:153} INFO - Started process (PID=573) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:39:51,808] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 15:39:51,808] {logging_mixin.py:115} INFO - [2022-12-08 15:39:51,808] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:39:52,334] {logging_mixin.py:115} INFO - [2022-12-08 15:39:52,334] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:39:52,798] {logging_mixin.py:115} INFO - [2022-12-08 15:39:52,798] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:39:53,355] {logging_mixin.py:115} INFO - [2022-12-08 15:39:53,355] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:39:53,356] {logging_mixin.py:115} INFO - [2022-12-08 15:39:53,355] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:39:53,357] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:39:53,367] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.562 seconds
[2022-12-08 15:40:24,352] {processor.py:153} INFO - Started process (PID=644) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:40:24,353] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 15:40:24,353] {logging_mixin.py:115} INFO - [2022-12-08 15:40:24,353] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:40:24,858] {logging_mixin.py:115} INFO - [2022-12-08 15:40:24,858] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:40:25,361] {logging_mixin.py:115} INFO - [2022-12-08 15:40:25,361] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:40:25,850] {logging_mixin.py:115} INFO - [2022-12-08 15:40:25,850] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:40:25,851] {logging_mixin.py:115} INFO - [2022-12-08 15:40:25,850] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:40:25,852] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:40:25,862] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.512 seconds
[2022-12-08 15:40:56,681] {processor.py:153} INFO - Started process (PID=715) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:40:56,681] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 15:40:56,682] {logging_mixin.py:115} INFO - [2022-12-08 15:40:56,682] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:40:57,202] {logging_mixin.py:115} INFO - [2022-12-08 15:40:57,201] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:40:57,664] {logging_mixin.py:115} INFO - [2022-12-08 15:40:57,664] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:40:58,145] {logging_mixin.py:115} INFO - [2022-12-08 15:40:58,145] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:40:58,146] {logging_mixin.py:115} INFO - [2022-12-08 15:40:58,145] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:40:58,147] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:40:58,156] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.477 seconds
[2022-12-08 15:41:29,092] {processor.py:153} INFO - Started process (PID=786) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:41:29,093] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 15:41:29,093] {logging_mixin.py:115} INFO - [2022-12-08 15:41:29,093] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:41:29,596] {logging_mixin.py:115} INFO - [2022-12-08 15:41:29,595] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:41:30,068] {logging_mixin.py:115} INFO - [2022-12-08 15:41:30,068] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:41:30,580] {logging_mixin.py:115} INFO - [2022-12-08 15:41:30,580] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:41:30,581] {logging_mixin.py:115} INFO - [2022-12-08 15:41:30,580] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:41:30,582] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:41:30,590] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.500 seconds
[2022-12-08 15:42:01,438] {processor.py:153} INFO - Started process (PID=857) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:42:01,438] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 15:42:01,438] {logging_mixin.py:115} INFO - [2022-12-08 15:42:01,438] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:42:01,957] {logging_mixin.py:115} INFO - [2022-12-08 15:42:01,957] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:42:02,400] {logging_mixin.py:115} INFO - [2022-12-08 15:42:02,400] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:42:02,922] {logging_mixin.py:115} INFO - [2022-12-08 15:42:02,922] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:42:02,923] {logging_mixin.py:115} INFO - [2022-12-08 15:42:02,922] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:42:02,924] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:42:02,932] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.497 seconds
[2022-12-08 15:42:33,783] {processor.py:153} INFO - Started process (PID=928) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:42:33,784] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 15:42:33,784] {logging_mixin.py:115} INFO - [2022-12-08 15:42:33,784] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:42:34,310] {logging_mixin.py:115} INFO - [2022-12-08 15:42:34,309] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:42:34,764] {logging_mixin.py:115} INFO - [2022-12-08 15:42:34,764] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:42:35,264] {logging_mixin.py:115} INFO - [2022-12-08 15:42:35,264] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:42:35,265] {logging_mixin.py:115} INFO - [2022-12-08 15:42:35,264] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:42:35,266] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:42:35,274] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.493 seconds
[2022-12-08 15:43:06,222] {processor.py:153} INFO - Started process (PID=999) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:43:06,222] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 15:43:06,223] {logging_mixin.py:115} INFO - [2022-12-08 15:43:06,223] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:43:06,724] {logging_mixin.py:115} INFO - [2022-12-08 15:43:06,724] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:43:07,189] {logging_mixin.py:115} INFO - [2022-12-08 15:43:07,189] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:43:07,722] {logging_mixin.py:115} INFO - [2022-12-08 15:43:07,722] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:43:07,723] {logging_mixin.py:115} INFO - [2022-12-08 15:43:07,722] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:43:07,724] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:43:07,732] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.512 seconds
[2022-12-08 15:43:38,619] {processor.py:153} INFO - Started process (PID=1068) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:43:38,619] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 15:43:38,620] {logging_mixin.py:115} INFO - [2022-12-08 15:43:38,620] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:43:39,134] {logging_mixin.py:115} INFO - [2022-12-08 15:43:39,133] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:43:39,622] {logging_mixin.py:115} INFO - [2022-12-08 15:43:39,622] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:43:40,131] {logging_mixin.py:115} INFO - [2022-12-08 15:43:40,130] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:43:40,131] {logging_mixin.py:115} INFO - [2022-12-08 15:43:40,131] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:43:40,132] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:43:40,140] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.523 seconds
[2022-12-08 15:44:11,044] {processor.py:153} INFO - Started process (PID=1132) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:44:11,045] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 15:44:11,045] {logging_mixin.py:115} INFO - [2022-12-08 15:44:11,045] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:44:11,563] {logging_mixin.py:115} INFO - [2022-12-08 15:44:11,563] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:44:12,007] {logging_mixin.py:115} INFO - [2022-12-08 15:44:12,007] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:44:12,527] {logging_mixin.py:115} INFO - [2022-12-08 15:44:12,527] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:44:12,528] {logging_mixin.py:115} INFO - [2022-12-08 15:44:12,527] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:44:12,529] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:44:12,536] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.494 seconds
[2022-12-08 15:44:43,460] {processor.py:153} INFO - Started process (PID=1204) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:44:43,460] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 15:44:43,460] {logging_mixin.py:115} INFO - [2022-12-08 15:44:43,460] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:44:43,966] {logging_mixin.py:115} INFO - [2022-12-08 15:44:43,966] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:44:44,455] {logging_mixin.py:115} INFO - [2022-12-08 15:44:44,455] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:44:44,974] {logging_mixin.py:115} INFO - [2022-12-08 15:44:44,974] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:44:44,975] {logging_mixin.py:115} INFO - [2022-12-08 15:44:44,974] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:44:44,976] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:44:44,984] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.526 seconds
[2022-12-08 15:45:15,506] {processor.py:153} INFO - Started process (PID=1275) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:45:15,506] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 15:45:15,506] {logging_mixin.py:115} INFO - [2022-12-08 15:45:15,506] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:45:16,009] {logging_mixin.py:115} INFO - [2022-12-08 15:45:16,009] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:45:16,474] {logging_mixin.py:115} INFO - [2022-12-08 15:45:16,474] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:45:16,993] {logging_mixin.py:115} INFO - [2022-12-08 15:45:16,993] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:45:16,994] {logging_mixin.py:115} INFO - [2022-12-08 15:45:16,993] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:45:16,995] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:45:17,003] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.500 seconds
[2022-12-08 15:45:47,673] {processor.py:153} INFO - Started process (PID=1350) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:45:47,673] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 15:45:47,674] {logging_mixin.py:115} INFO - [2022-12-08 15:45:47,674] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:45:48,192] {logging_mixin.py:115} INFO - [2022-12-08 15:45:48,192] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:45:48,648] {logging_mixin.py:115} INFO - [2022-12-08 15:45:48,648] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:45:49,142] {logging_mixin.py:115} INFO - [2022-12-08 15:45:49,141] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:45:49,142] {logging_mixin.py:115} INFO - [2022-12-08 15:45:49,142] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:45:49,143] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:45:49,151] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.481 seconds
[2022-12-08 15:46:19,576] {processor.py:153} INFO - Started process (PID=1421) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:46:19,576] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 15:46:19,577] {logging_mixin.py:115} INFO - [2022-12-08 15:46:19,577] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:46:20,076] {logging_mixin.py:115} INFO - [2022-12-08 15:46:20,076] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:46:20,535] {logging_mixin.py:115} INFO - [2022-12-08 15:46:20,535] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:46:21,017] {logging_mixin.py:115} INFO - [2022-12-08 15:46:21,017] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:46:21,018] {logging_mixin.py:115} INFO - [2022-12-08 15:46:21,017] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:46:21,019] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:46:21,026] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.452 seconds
[2022-12-08 15:46:51,515] {processor.py:153} INFO - Started process (PID=1492) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:46:51,515] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 15:46:51,516] {logging_mixin.py:115} INFO - [2022-12-08 15:46:51,516] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:46:52,031] {logging_mixin.py:115} INFO - [2022-12-08 15:46:52,031] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:46:52,478] {logging_mixin.py:115} INFO - [2022-12-08 15:46:52,478] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:46:52,970] {logging_mixin.py:115} INFO - [2022-12-08 15:46:52,970] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:46:52,971] {logging_mixin.py:115} INFO - [2022-12-08 15:46:52,970] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:46:52,972] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:46:52,979] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.467 seconds
[2022-12-08 15:47:23,480] {processor.py:153} INFO - Started process (PID=1554) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:47:23,480] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 15:47:23,481] {logging_mixin.py:115} INFO - [2022-12-08 15:47:23,480] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:47:24,020] {logging_mixin.py:115} INFO - [2022-12-08 15:47:24,020] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:47:24,463] {logging_mixin.py:115} INFO - [2022-12-08 15:47:24,463] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:47:24,958] {logging_mixin.py:115} INFO - [2022-12-08 15:47:24,958] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:47:24,959] {logging_mixin.py:115} INFO - [2022-12-08 15:47:24,959] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:47:24,961] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:47:24,968] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.491 seconds
[2022-12-08 15:47:55,398] {processor.py:153} INFO - Started process (PID=1625) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:47:55,399] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 15:47:55,399] {logging_mixin.py:115} INFO - [2022-12-08 15:47:55,399] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:47:55,913] {logging_mixin.py:115} INFO - [2022-12-08 15:47:55,913] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:47:56,388] {logging_mixin.py:115} INFO - [2022-12-08 15:47:56,388] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:47:56,896] {logging_mixin.py:115} INFO - [2022-12-08 15:47:56,896] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:47:56,897] {logging_mixin.py:115} INFO - [2022-12-08 15:47:56,896] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:47:56,898] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:47:56,905] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.510 seconds
[2022-12-08 15:48:27,301] {processor.py:153} INFO - Started process (PID=1696) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:48:27,301] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 15:48:27,302] {logging_mixin.py:115} INFO - [2022-12-08 15:48:27,302] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:48:27,817] {logging_mixin.py:115} INFO - [2022-12-08 15:48:27,817] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:48:28,276] {logging_mixin.py:115} INFO - [2022-12-08 15:48:28,276] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:48:28,783] {logging_mixin.py:115} INFO - [2022-12-08 15:48:28,783] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:48:28,784] {logging_mixin.py:115} INFO - [2022-12-08 15:48:28,784] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:48:28,785] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:48:28,793] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.494 seconds
[2022-12-08 15:48:59,264] {processor.py:153} INFO - Started process (PID=1766) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:48:59,265] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 15:48:59,266] {logging_mixin.py:115} INFO - [2022-12-08 15:48:59,265] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:48:59,766] {logging_mixin.py:115} INFO - [2022-12-08 15:48:59,766] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:49:00,245] {logging_mixin.py:115} INFO - [2022-12-08 15:49:00,245] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:49:00,735] {logging_mixin.py:115} INFO - [2022-12-08 15:49:00,734] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:49:00,735] {logging_mixin.py:115} INFO - [2022-12-08 15:49:00,735] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:49:00,737] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:49:00,744] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.482 seconds
[2022-12-08 15:49:31,384] {processor.py:153} INFO - Started process (PID=1837) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:49:31,384] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 15:49:31,385] {logging_mixin.py:115} INFO - [2022-12-08 15:49:31,385] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:49:31,911] {logging_mixin.py:115} INFO - [2022-12-08 15:49:31,911] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:49:32,384] {logging_mixin.py:115} INFO - [2022-12-08 15:49:32,384] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:49:32,879] {logging_mixin.py:115} INFO - [2022-12-08 15:49:32,879] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:49:32,880] {logging_mixin.py:115} INFO - [2022-12-08 15:49:32,879] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:49:32,882] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:49:32,889] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.508 seconds
[2022-12-08 15:50:03,031] {processor.py:153} INFO - Started process (PID=1908) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:50:03,031] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 15:50:03,032] {logging_mixin.py:115} INFO - [2022-12-08 15:50:03,032] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:50:03,555] {logging_mixin.py:115} INFO - [2022-12-08 15:50:03,555] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:50:04,007] {logging_mixin.py:115} INFO - [2022-12-08 15:50:04,007] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:50:04,504] {logging_mixin.py:115} INFO - [2022-12-08 15:50:04,504] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:50:04,505] {logging_mixin.py:115} INFO - [2022-12-08 15:50:04,504] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:50:04,507] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:50:04,515] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.486 seconds
[2022-12-08 15:50:35,375] {processor.py:153} INFO - Started process (PID=1970) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:50:35,376] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 15:50:35,376] {logging_mixin.py:115} INFO - [2022-12-08 15:50:35,376] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:50:35,909] {logging_mixin.py:115} INFO - [2022-12-08 15:50:35,909] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:50:36,386] {logging_mixin.py:115} INFO - [2022-12-08 15:50:36,386] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:50:36,897] {logging_mixin.py:115} INFO - [2022-12-08 15:50:36,897] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:50:36,898] {logging_mixin.py:115} INFO - [2022-12-08 15:50:36,897] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:50:36,900] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:50:36,907] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.534 seconds
[2022-12-08 15:51:07,710] {processor.py:153} INFO - Started process (PID=2041) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:51:07,710] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 15:51:07,711] {logging_mixin.py:115} INFO - [2022-12-08 15:51:07,711] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:51:08,246] {logging_mixin.py:115} INFO - [2022-12-08 15:51:08,246] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:51:08,703] {logging_mixin.py:115} INFO - [2022-12-08 15:51:08,703] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:51:09,205] {logging_mixin.py:115} INFO - [2022-12-08 15:51:09,205] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:51:09,206] {logging_mixin.py:115} INFO - [2022-12-08 15:51:09,205] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:51:09,207] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:51:09,215] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.507 seconds
[2022-12-08 15:51:39,853] {processor.py:153} INFO - Started process (PID=2126) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:51:39,854] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 15:51:39,854] {logging_mixin.py:115} INFO - [2022-12-08 15:51:39,854] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:51:40,407] {logging_mixin.py:115} INFO - [2022-12-08 15:51:40,406] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:51:40,938] {logging_mixin.py:115} INFO - [2022-12-08 15:51:40,938] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:51:41,446] {logging_mixin.py:115} INFO - [2022-12-08 15:51:41,446] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:51:41,447] {logging_mixin.py:115} INFO - [2022-12-08 15:51:41,446] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:51:41,448] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:51:41,456] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.605 seconds
[2022-12-08 15:52:11,537] {processor.py:153} INFO - Started process (PID=2201) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:52:11,538] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 15:52:11,538] {logging_mixin.py:115} INFO - [2022-12-08 15:52:11,538] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:52:12,055] {logging_mixin.py:115} INFO - [2022-12-08 15:52:12,055] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:52:12,526] {logging_mixin.py:115} INFO - [2022-12-08 15:52:12,526] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:52:13,026] {logging_mixin.py:115} INFO - [2022-12-08 15:52:13,026] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:52:13,027] {logging_mixin.py:115} INFO - [2022-12-08 15:52:13,026] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:52:13,029] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:52:13,036] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.501 seconds
[2022-12-08 15:52:43,860] {processor.py:153} INFO - Started process (PID=2288) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:52:43,860] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 15:52:43,861] {logging_mixin.py:115} INFO - [2022-12-08 15:52:43,861] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:52:44,369] {logging_mixin.py:115} INFO - [2022-12-08 15:52:44,369] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:52:44,853] {logging_mixin.py:115} INFO - [2022-12-08 15:52:44,853] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:52:45,357] {logging_mixin.py:115} INFO - [2022-12-08 15:52:45,357] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:52:45,358] {logging_mixin.py:115} INFO - [2022-12-08 15:52:45,357] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:52:45,360] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:52:45,368] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.510 seconds
[2022-12-08 15:53:16,226] {processor.py:153} INFO - Started process (PID=2359) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:53:16,226] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 15:53:16,227] {logging_mixin.py:115} INFO - [2022-12-08 15:53:16,227] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:53:16,744] {logging_mixin.py:115} INFO - [2022-12-08 15:53:16,744] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:53:17,193] {logging_mixin.py:115} INFO - [2022-12-08 15:53:17,192] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:53:17,699] {logging_mixin.py:115} INFO - [2022-12-08 15:53:17,699] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:53:17,700] {logging_mixin.py:115} INFO - [2022-12-08 15:53:17,699] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:53:17,702] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:53:17,711] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.488 seconds
[2022-12-08 15:53:48,570] {processor.py:153} INFO - Started process (PID=2430) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:53:48,570] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 15:53:48,570] {logging_mixin.py:115} INFO - [2022-12-08 15:53:48,570] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:53:49,071] {logging_mixin.py:115} INFO - [2022-12-08 15:53:49,071] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:53:49,540] {logging_mixin.py:115} INFO - [2022-12-08 15:53:49,540] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:53:50,054] {logging_mixin.py:115} INFO - [2022-12-08 15:53:50,054] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:53:50,055] {logging_mixin.py:115} INFO - [2022-12-08 15:53:50,054] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:53:50,057] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:53:50,065] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.497 seconds
[2022-12-08 15:54:20,927] {processor.py:153} INFO - Started process (PID=2492) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:54:20,927] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 15:54:20,928] {logging_mixin.py:115} INFO - [2022-12-08 15:54:20,928] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:54:21,458] {logging_mixin.py:115} INFO - [2022-12-08 15:54:21,458] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:54:21,920] {logging_mixin.py:115} INFO - [2022-12-08 15:54:21,920] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:54:22,419] {logging_mixin.py:115} INFO - [2022-12-08 15:54:22,419] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:54:22,420] {logging_mixin.py:115} INFO - [2022-12-08 15:54:22,420] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:54:22,422] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:54:22,429] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.505 seconds
[2022-12-08 15:54:53,291] {processor.py:153} INFO - Started process (PID=2563) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:54:53,292] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 15:54:53,292] {logging_mixin.py:115} INFO - [2022-12-08 15:54:53,292] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:54:53,794] {logging_mixin.py:115} INFO - [2022-12-08 15:54:53,794] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:54:54,321] {logging_mixin.py:115} INFO - [2022-12-08 15:54:54,321] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:54:54,834] {logging_mixin.py:115} INFO - [2022-12-08 15:54:54,834] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:54:54,835] {logging_mixin.py:115} INFO - [2022-12-08 15:54:54,834] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:54:54,837] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:54:54,844] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.555 seconds
[2022-12-08 15:55:25,643] {processor.py:153} INFO - Started process (PID=2635) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:55:25,644] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 15:55:25,644] {logging_mixin.py:115} INFO - [2022-12-08 15:55:25,644] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:55:26,172] {logging_mixin.py:115} INFO - [2022-12-08 15:55:26,172] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:55:26,605] {logging_mixin.py:115} INFO - [2022-12-08 15:55:26,605] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:55:27,127] {logging_mixin.py:115} INFO - [2022-12-08 15:55:27,126] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:55:27,128] {logging_mixin.py:115} INFO - [2022-12-08 15:55:27,127] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:55:27,129] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:55:27,137] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.496 seconds
[2022-12-08 15:55:57,978] {processor.py:153} INFO - Started process (PID=2706) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:55:57,979] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 15:55:57,979] {logging_mixin.py:115} INFO - [2022-12-08 15:55:57,979] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:55:58,491] {logging_mixin.py:115} INFO - [2022-12-08 15:55:58,491] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:55:58,933] {logging_mixin.py:115} INFO - [2022-12-08 15:55:58,933] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:55:59,433] {logging_mixin.py:115} INFO - [2022-12-08 15:55:59,433] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:55:59,434] {logging_mixin.py:115} INFO - [2022-12-08 15:55:59,433] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:55:59,435] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:55:59,443] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.467 seconds
[2022-12-08 15:56:30,333] {processor.py:153} INFO - Started process (PID=2781) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:56:30,333] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 15:56:30,333] {logging_mixin.py:115} INFO - [2022-12-08 15:56:30,333] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:56:30,851] {logging_mixin.py:115} INFO - [2022-12-08 15:56:30,851] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:56:31,319] {logging_mixin.py:115} INFO - [2022-12-08 15:56:31,319] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:56:31,826] {logging_mixin.py:115} INFO - [2022-12-08 15:56:31,826] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:56:31,827] {logging_mixin.py:115} INFO - [2022-12-08 15:56:31,826] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:56:31,828] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:56:31,836] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.506 seconds
[2022-12-08 15:57:02,774] {processor.py:153} INFO - Started process (PID=2852) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:57:02,775] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 15:57:02,775] {logging_mixin.py:115} INFO - [2022-12-08 15:57:02,775] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:57:03,279] {logging_mixin.py:115} INFO - [2022-12-08 15:57:03,279] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:57:03,754] {logging_mixin.py:115} INFO - [2022-12-08 15:57:03,753] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:57:04,236] {logging_mixin.py:115} INFO - [2022-12-08 15:57:04,236] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:57:04,237] {logging_mixin.py:115} INFO - [2022-12-08 15:57:04,236] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:57:04,238] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:57:04,246] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.474 seconds
[2022-12-08 15:57:35,140] {processor.py:153} INFO - Started process (PID=2923) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:57:35,141] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 15:57:35,141] {logging_mixin.py:115} INFO - [2022-12-08 15:57:35,141] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:57:35,662] {logging_mixin.py:115} INFO - [2022-12-08 15:57:35,662] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:57:36,105] {logging_mixin.py:115} INFO - [2022-12-08 15:57:36,105] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:57:36,603] {logging_mixin.py:115} INFO - [2022-12-08 15:57:36,603] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:57:36,604] {logging_mixin.py:115} INFO - [2022-12-08 15:57:36,603] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:57:36,605] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:57:36,613] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.475 seconds
[2022-12-08 15:58:07,573] {processor.py:153} INFO - Started process (PID=2994) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:58:07,573] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 15:58:07,573] {logging_mixin.py:115} INFO - [2022-12-08 15:58:07,573] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:58:08,104] {logging_mixin.py:115} INFO - [2022-12-08 15:58:08,104] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:58:08,571] {logging_mixin.py:115} INFO - [2022-12-08 15:58:08,571] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:58:09,057] {logging_mixin.py:115} INFO - [2022-12-08 15:58:09,057] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:58:09,058] {logging_mixin.py:115} INFO - [2022-12-08 15:58:09,057] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:58:09,060] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:58:09,068] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.497 seconds
[2022-12-08 15:58:40,031] {processor.py:153} INFO - Started process (PID=3056) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:58:40,031] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 15:58:40,032] {logging_mixin.py:115} INFO - [2022-12-08 15:58:40,032] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:58:40,558] {logging_mixin.py:115} INFO - [2022-12-08 15:58:40,558] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:58:41,043] {logging_mixin.py:115} INFO - [2022-12-08 15:58:41,043] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:58:41,539] {logging_mixin.py:115} INFO - [2022-12-08 15:58:41,539] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:58:41,540] {logging_mixin.py:115} INFO - [2022-12-08 15:58:41,539] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:58:41,542] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:58:41,549] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.521 seconds
[2022-12-08 15:59:12,402] {processor.py:153} INFO - Started process (PID=3127) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:59:12,403] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 15:59:12,403] {logging_mixin.py:115} INFO - [2022-12-08 15:59:12,403] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:59:12,916] {logging_mixin.py:115} INFO - [2022-12-08 15:59:12,916] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:59:13,370] {logging_mixin.py:115} INFO - [2022-12-08 15:59:13,370] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:59:13,865] {logging_mixin.py:115} INFO - [2022-12-08 15:59:13,865] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:59:13,866] {logging_mixin.py:115} INFO - [2022-12-08 15:59:13,865] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:59:13,867] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:59:13,874] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.474 seconds
[2022-12-08 15:59:44,734] {processor.py:153} INFO - Started process (PID=3198) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:59:44,734] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 15:59:44,734] {logging_mixin.py:115} INFO - [2022-12-08 15:59:44,734] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:59:45,256] {logging_mixin.py:115} INFO - [2022-12-08 15:59:45,256] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:59:45,720] {logging_mixin.py:115} INFO - [2022-12-08 15:59:45,720] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 15:59:46,208] {logging_mixin.py:115} INFO - [2022-12-08 15:59:46,208] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:59:46,209] {logging_mixin.py:115} INFO - [2022-12-08 15:59:46,208] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 15:59:46,210] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 15:59:46,218] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.487 seconds
[2022-12-08 16:00:16,773] {processor.py:153} INFO - Started process (PID=3283) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:00:16,774] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:00:16,774] {logging_mixin.py:115} INFO - [2022-12-08 16:00:16,774] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:00:17,294] {logging_mixin.py:115} INFO - [2022-12-08 16:00:17,294] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:00:17,744] {logging_mixin.py:115} INFO - [2022-12-08 16:00:17,744] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:00:18,226] {logging_mixin.py:115} INFO - [2022-12-08 16:00:18,226] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 16:00:18,227] {logging_mixin.py:115} INFO - [2022-12-08 16:00:18,226] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 16:00:18,229] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:00:18,236] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.465 seconds
[2022-12-08 16:00:48,742] {processor.py:153} INFO - Started process (PID=3354) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:00:48,743] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:00:48,743] {logging_mixin.py:115} INFO - [2022-12-08 16:00:48,743] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:00:49,273] {logging_mixin.py:115} INFO - [2022-12-08 16:00:49,273] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:00:49,760] {logging_mixin.py:115} INFO - [2022-12-08 16:00:49,760] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:00:50,254] {logging_mixin.py:115} INFO - [2022-12-08 16:00:50,253] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 16:00:50,254] {logging_mixin.py:115} INFO - [2022-12-08 16:00:50,254] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 16:00:50,256] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:00:50,263] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.524 seconds
[2022-12-08 16:01:21,156] {processor.py:153} INFO - Started process (PID=3425) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:01:21,156] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:01:21,157] {logging_mixin.py:115} INFO - [2022-12-08 16:01:21,157] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:01:21,688] {logging_mixin.py:115} INFO - [2022-12-08 16:01:21,688] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:01:22,155] {logging_mixin.py:115} INFO - [2022-12-08 16:01:22,155] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:01:22,654] {logging_mixin.py:115} INFO - [2022-12-08 16:01:22,654] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 16:01:22,655] {logging_mixin.py:115} INFO - [2022-12-08 16:01:22,654] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 16:01:22,657] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:01:22,664] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.510 seconds
[2022-12-08 16:01:52,704] {processor.py:153} INFO - Started process (PID=3494) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:01:52,705] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:01:52,705] {logging_mixin.py:115} INFO - [2022-12-08 16:01:52,705] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:01:53,224] {logging_mixin.py:115} INFO - [2022-12-08 16:01:53,224] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:01:53,702] {logging_mixin.py:115} INFO - [2022-12-08 16:01:53,702] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:01:54,210] {logging_mixin.py:115} INFO - [2022-12-08 16:01:54,210] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 16:01:54,211] {logging_mixin.py:115} INFO - [2022-12-08 16:01:54,210] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 16:01:54,212] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:01:54,220] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.518 seconds
[2022-12-08 16:02:24,338] {processor.py:153} INFO - Started process (PID=3558) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:02:24,338] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:02:24,338] {logging_mixin.py:115} INFO - [2022-12-08 16:02:24,338] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:02:24,854] {logging_mixin.py:115} INFO - [2022-12-08 16:02:24,854] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:02:25,312] {logging_mixin.py:115} INFO - [2022-12-08 16:02:25,311] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:02:25,806] {logging_mixin.py:115} INFO - [2022-12-08 16:02:25,806] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 16:02:25,807] {logging_mixin.py:115} INFO - [2022-12-08 16:02:25,806] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 16:02:25,809] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:02:25,816] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.481 seconds
[2022-12-08 16:02:56,287] {processor.py:153} INFO - Started process (PID=3629) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:02:56,288] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:02:56,289] {logging_mixin.py:115} INFO - [2022-12-08 16:02:56,289] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:02:56,815] {logging_mixin.py:115} INFO - [2022-12-08 16:02:56,815] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:02:57,290] {logging_mixin.py:115} INFO - [2022-12-08 16:02:57,290] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:02:57,776] {logging_mixin.py:115} INFO - [2022-12-08 16:02:57,776] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 16:02:57,777] {logging_mixin.py:115} INFO - [2022-12-08 16:02:57,776] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 16:02:57,778] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:02:57,786] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.501 seconds
[2022-12-08 16:03:28,721] {processor.py:153} INFO - Started process (PID=3700) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:03:28,721] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:03:28,721] {logging_mixin.py:115} INFO - [2022-12-08 16:03:28,721] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:03:29,241] {logging_mixin.py:115} INFO - [2022-12-08 16:03:29,241] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:03:29,710] {logging_mixin.py:115} INFO - [2022-12-08 16:03:29,710] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:03:30,200] {logging_mixin.py:115} INFO - [2022-12-08 16:03:30,200] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 16:03:30,201] {logging_mixin.py:115} INFO - [2022-12-08 16:03:30,200] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 16:03:30,202] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:03:30,209] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.491 seconds
[2022-12-08 16:04:01,163] {processor.py:153} INFO - Started process (PID=3771) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:04:01,164] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:04:01,164] {logging_mixin.py:115} INFO - [2022-12-08 16:04:01,164] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:04:01,691] {logging_mixin.py:115} INFO - [2022-12-08 16:04:01,691] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:04:02,187] {logging_mixin.py:115} INFO - [2022-12-08 16:04:02,187] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:04:02,678] {logging_mixin.py:115} INFO - [2022-12-08 16:04:02,678] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 16:04:02,679] {logging_mixin.py:115} INFO - [2022-12-08 16:04:02,678] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 16:04:02,681] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:04:02,691] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.530 seconds
[2022-12-08 16:04:33,140] {processor.py:153} INFO - Started process (PID=3843) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:04:33,141] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:04:33,142] {logging_mixin.py:115} INFO - [2022-12-08 16:04:33,142] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:04:33,649] {logging_mixin.py:115} INFO - [2022-12-08 16:04:33,649] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:04:34,141] {logging_mixin.py:115} INFO - [2022-12-08 16:04:34,140] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:04:34,630] {logging_mixin.py:115} INFO - [2022-12-08 16:04:34,630] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 16:04:34,631] {logging_mixin.py:115} INFO - [2022-12-08 16:04:34,630] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 16:04:34,633] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:04:34,640] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.502 seconds
[2022-12-08 16:05:05,149] {processor.py:153} INFO - Started process (PID=3914) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:05:05,149] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:05:05,150] {logging_mixin.py:115} INFO - [2022-12-08 16:05:05,150] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:05:05,676] {logging_mixin.py:115} INFO - [2022-12-08 16:05:05,675] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:05:06,110] {logging_mixin.py:115} INFO - [2022-12-08 16:05:06,110] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:05:06,604] {logging_mixin.py:115} INFO - [2022-12-08 16:05:06,604] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 16:05:06,605] {logging_mixin.py:115} INFO - [2022-12-08 16:05:06,604] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 16:05:06,607] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:05:06,615] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.469 seconds
[2022-12-08 16:05:37,129] {processor.py:153} INFO - Started process (PID=3976) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:05:37,129] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:05:37,129] {logging_mixin.py:115} INFO - [2022-12-08 16:05:37,129] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:05:37,664] {logging_mixin.py:115} INFO - [2022-12-08 16:05:37,664] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:05:38,203] {logging_mixin.py:115} INFO - [2022-12-08 16:05:38,203] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:05:38,735] {logging_mixin.py:115} INFO - [2022-12-08 16:05:38,735] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 16:05:38,736] {logging_mixin.py:115} INFO - [2022-12-08 16:05:38,735] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 16:05:38,737] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:05:38,745] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.619 seconds
[2022-12-08 16:06:08,828] {processor.py:153} INFO - Started process (PID=4047) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:06:08,829] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:06:08,829] {logging_mixin.py:115} INFO - [2022-12-08 16:06:08,829] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:06:09,343] {logging_mixin.py:115} INFO - [2022-12-08 16:06:09,343] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:06:09,787] {logging_mixin.py:115} INFO - [2022-12-08 16:06:09,787] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:06:10,290] {logging_mixin.py:115} INFO - [2022-12-08 16:06:10,290] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 16:06:10,291] {logging_mixin.py:115} INFO - [2022-12-08 16:06:10,290] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 16:06:10,292] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:06:10,300] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.474 seconds
[2022-12-08 16:06:40,764] {processor.py:153} INFO - Started process (PID=4118) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:06:40,764] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:06:40,765] {logging_mixin.py:115} INFO - [2022-12-08 16:06:40,765] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:06:41,263] {logging_mixin.py:115} INFO - [2022-12-08 16:06:41,263] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:06:41,738] {logging_mixin.py:115} INFO - [2022-12-08 16:06:41,738] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:06:42,249] {logging_mixin.py:115} INFO - [2022-12-08 16:06:42,248] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 16:06:42,249] {logging_mixin.py:115} INFO - [2022-12-08 16:06:42,249] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 16:06:42,251] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:06:42,259] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.498 seconds
[2022-12-08 16:07:12,797] {processor.py:153} INFO - Started process (PID=4189) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:07:12,798] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:07:12,798] {logging_mixin.py:115} INFO - [2022-12-08 16:07:12,798] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:07:13,305] {logging_mixin.py:115} INFO - [2022-12-08 16:07:13,305] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:07:13,750] {logging_mixin.py:115} INFO - [2022-12-08 16:07:13,750] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:07:14,248] {logging_mixin.py:115} INFO - [2022-12-08 16:07:14,248] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 16:07:14,249] {logging_mixin.py:115} INFO - [2022-12-08 16:07:14,248] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 16:07:14,250] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:07:14,258] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.464 seconds
[2022-12-08 16:07:44,778] {processor.py:153} INFO - Started process (PID=4260) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:07:44,778] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:07:44,779] {logging_mixin.py:115} INFO - [2022-12-08 16:07:44,778] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:07:45,282] {logging_mixin.py:115} INFO - [2022-12-08 16:07:45,282] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:07:45,755] {logging_mixin.py:115} INFO - [2022-12-08 16:07:45,754] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:07:46,248] {logging_mixin.py:115} INFO - [2022-12-08 16:07:46,248] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 16:07:46,249] {logging_mixin.py:115} INFO - [2022-12-08 16:07:46,248] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 16:07:46,250] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:07:46,257] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.482 seconds
[2022-12-08 16:08:16,670] {processor.py:153} INFO - Started process (PID=4331) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:08:16,671] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:08:16,671] {logging_mixin.py:115} INFO - [2022-12-08 16:08:16,671] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:08:17,202] {logging_mixin.py:115} INFO - [2022-12-08 16:08:17,202] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:08:17,646] {logging_mixin.py:115} INFO - [2022-12-08 16:08:17,646] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:08:18,157] {logging_mixin.py:115} INFO - [2022-12-08 16:08:18,157] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 16:08:18,158] {logging_mixin.py:115} INFO - [2022-12-08 16:08:18,157] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 16:08:18,160] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:08:18,167] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.500 seconds
[2022-12-08 16:08:48,686] {processor.py:153} INFO - Started process (PID=4393) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:08:48,687] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:08:48,687] {logging_mixin.py:115} INFO - [2022-12-08 16:08:48,687] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:08:49,222] {logging_mixin.py:115} INFO - [2022-12-08 16:08:49,221] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:08:49,675] {logging_mixin.py:115} INFO - [2022-12-08 16:08:49,675] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:08:50,190] {logging_mixin.py:115} INFO - [2022-12-08 16:08:50,190] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 16:08:50,191] {logging_mixin.py:115} INFO - [2022-12-08 16:08:50,190] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 16:08:50,193] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:08:50,200] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.517 seconds
[2022-12-08 16:10:13,299] {processor.py:153} INFO - Started process (PID=47) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:10:13,299] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:10:13,300] {logging_mixin.py:115} INFO - [2022-12-08 16:10:13,300] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:10:14,631] {logging_mixin.py:115} INFO - [2022-12-08 16:10:14,631] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:10:15,164] {logging_mixin.py:115} INFO - [2022-12-08 16:10:15,164] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:10:15,687] {logging_mixin.py:115} INFO - [2022-12-08 16:10:15,687] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 16:10:15,688] {logging_mixin.py:115} INFO - [2022-12-08 16:10:15,687] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 16:10:15,689] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:10:15,698] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 2.401 seconds
[2022-12-08 16:10:45,786] {processor.py:153} INFO - Started process (PID=125) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:10:45,786] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:10:45,787] {logging_mixin.py:115} INFO - [2022-12-08 16:10:45,787] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:10:46,309] {logging_mixin.py:115} INFO - [2022-12-08 16:10:46,309] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:10:46,757] {logging_mixin.py:115} INFO - [2022-12-08 16:10:46,756] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:10:47,274] {logging_mixin.py:115} INFO - [2022-12-08 16:10:47,274] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 16:10:47,275] {logging_mixin.py:115} INFO - [2022-12-08 16:10:47,274] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 16:10:47,276] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:10:47,286] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.502 seconds
[2022-12-08 16:11:17,333] {processor.py:153} INFO - Started process (PID=211) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:11:17,334] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:11:17,334] {logging_mixin.py:115} INFO - [2022-12-08 16:11:17,334] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:11:17,844] {logging_mixin.py:115} INFO - [2022-12-08 16:11:17,844] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:11:18,311] {logging_mixin.py:115} INFO - [2022-12-08 16:11:18,311] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:11:18,817] {logging_mixin.py:115} INFO - [2022-12-08 16:11:18,817] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 16:11:18,818] {logging_mixin.py:115} INFO - [2022-12-08 16:11:18,817] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 16:11:18,819] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:11:18,828] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.497 seconds
[2022-12-08 16:11:49,299] {processor.py:153} INFO - Started process (PID=294) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:11:49,299] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:11:49,299] {logging_mixin.py:115} INFO - [2022-12-08 16:11:49,299] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:11:49,820] {logging_mixin.py:115} INFO - [2022-12-08 16:11:49,820] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:11:50,253] {logging_mixin.py:115} INFO - [2022-12-08 16:11:50,253] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:11:50,755] {logging_mixin.py:115} INFO - [2022-12-08 16:11:50,755] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 16:11:50,756] {logging_mixin.py:115} INFO - [2022-12-08 16:11:50,755] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 16:11:50,757] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:11:50,766] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.470 seconds
[2022-12-08 16:12:21,621] {processor.py:153} INFO - Started process (PID=369) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:12:21,621] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:12:21,622] {logging_mixin.py:115} INFO - [2022-12-08 16:12:21,622] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:12:22,143] {logging_mixin.py:115} INFO - [2022-12-08 16:12:22,142] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:12:22,618] {logging_mixin.py:115} INFO - [2022-12-08 16:12:22,618] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:12:23,127] {logging_mixin.py:115} INFO - [2022-12-08 16:12:23,127] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 16:12:23,128] {logging_mixin.py:115} INFO - [2022-12-08 16:12:23,127] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 16:12:23,129] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:12:23,138] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.520 seconds
[2022-12-08 16:12:53,601] {processor.py:153} INFO - Started process (PID=440) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:12:53,601] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:12:53,602] {logging_mixin.py:115} INFO - [2022-12-08 16:12:53,602] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:12:54,116] {logging_mixin.py:115} INFO - [2022-12-08 16:12:54,116] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:12:54,617] {logging_mixin.py:115} INFO - [2022-12-08 16:12:54,617] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:12:55,132] {logging_mixin.py:115} INFO - [2022-12-08 16:12:55,131] {redshift.py:938} ERROR - {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 16:12:55,133] {logging_mixin.py:115} INFO - [2022-12-08 16:12:55,132] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 21, in read_from_s3_and_insert_redshift
    con=conn
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/_config.py", line 461, in wrapper
    return function(**args)
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 907, in to_sql
    lock=lock,
  File "/home/airflow/.local/lib/python3.7/site-packages/awswrangler/redshift.py", line 393, in _create_table
    cursor.execute(sql)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/cursor.py", line 231, in execute
    self._c.execute(self, operation, args)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1800, in execute
    self.handle_messages(cursor)
  File "/home/airflow/.local/lib/python3.7/site-packages/redshift_connector/core.py", line 1986, in handle_messages
    raise self.error
redshift_connector.error.ProgrammingError: {'S': 'ERROR', 'C': '42701', 'M': 'column "vendorid" duplicated', 'F': '../src/pg/src/backend/commands/tablecmds.c', 'L': '1602', 'R': 'MergeAttributes'}
[2022-12-08 16:12:55,133] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:12:55,142] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.544 seconds
[2022-12-08 16:13:18,211] {processor.py:153} INFO - Started process (PID=464) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:13:18,211] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:13:18,211] {logging_mixin.py:115} INFO - [2022-12-08 16:13:18,211] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:13:18,740] {logging_mixin.py:115} INFO - [2022-12-08 16:13:18,740] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:13:18,908] {logging_mixin.py:115} INFO - [2022-12-08 16:13:18,904] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/opt/airflow/dags/python_scripts/move_data_between_s3_and_redshift.py", line 14, in read_from_s3_and_insert_redshift
    "RatecodeID","PULocationID","DOLocationID","passenger_count"], axis=1)
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/util/_decorators.py", line 311, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/core/frame.py", line 4913, in drop
    errors=errors,
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/core/generic.py", line 4150, in drop
    obj = obj._drop_axis(labels, axis, level=level, errors=errors)
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/core/generic.py", line 4185, in _drop_axis
    new_axis = axis.drop(labels, errors=errors)
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/core/indexes/base.py", line 6017, in drop
    raise KeyError(f"{labels[mask]} not found in axis")
KeyError: "['payment_type' 'trip_type' 'congestion_surcharge' 'ehail_fee'\n 'store_and_fwd_flag' 'RatecodeID' 'PULocationID' 'DOLocationID'\n 'passenger_count'] not found in axis"
[2022-12-08 16:13:18,909] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:13:18,916] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 0.707 seconds
[2022-12-08 16:14:59,288] {processor.py:153} INFO - Started process (PID=46) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:14:59,289] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:14:59,290] {logging_mixin.py:115} INFO - [2022-12-08 16:14:59,289] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:15:00,633] {logging_mixin.py:115} INFO - [2022-12-08 16:15:00,633] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:15:00,806] {logging_mixin.py:115} INFO - [2022-12-08 16:15:00,806] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:15:01,626] {logging_mixin.py:115} INFO - [2022-12-08 16:15:01,625] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 158, in __init__
    raise AirflowException('`python_callable` param must be callable')
airflow.exceptions.AirflowException: `python_callable` param must be callable
[2022-12-08 16:15:01,626] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:15:01,636] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 2.350 seconds
[2022-12-08 16:15:31,807] {processor.py:153} INFO - Started process (PID=122) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:15:31,807] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:15:31,808] {logging_mixin.py:115} INFO - [2022-12-08 16:15:31,808] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:15:32,326] {logging_mixin.py:115} INFO - [2022-12-08 16:15:32,325] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:15:32,515] {logging_mixin.py:115} INFO - [2022-12-08 16:15:32,515] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:15:33,196] {logging_mixin.py:115} INFO - [2022-12-08 16:15:33,195] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 158, in __init__
    raise AirflowException('`python_callable` param must be callable')
airflow.exceptions.AirflowException: `python_callable` param must be callable
[2022-12-08 16:15:33,196] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:15:33,206] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.401 seconds
[2022-12-08 16:16:03,290] {processor.py:153} INFO - Started process (PID=204) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:16:03,291] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:16:03,291] {logging_mixin.py:115} INFO - [2022-12-08 16:16:03,291] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:16:03,805] {logging_mixin.py:115} INFO - [2022-12-08 16:16:03,805] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:16:04,004] {logging_mixin.py:115} INFO - [2022-12-08 16:16:04,004] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:16:04,627] {logging_mixin.py:115} INFO - [2022-12-08 16:16:04,626] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 158, in __init__
    raise AirflowException('`python_callable` param must be callable')
airflow.exceptions.AirflowException: `python_callable` param must be callable
[2022-12-08 16:16:04,627] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:16:04,635] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.347 seconds
[2022-12-08 16:16:35,424] {processor.py:153} INFO - Started process (PID=285) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:16:35,424] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:16:35,425] {logging_mixin.py:115} INFO - [2022-12-08 16:16:35,425] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:16:35,939] {logging_mixin.py:115} INFO - [2022-12-08 16:16:35,938] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:16:36,082] {logging_mixin.py:115} INFO - [2022-12-08 16:16:36,082] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:16:36,694] {logging_mixin.py:115} INFO - [2022-12-08 16:16:36,693] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 158, in __init__
    raise AirflowException('`python_callable` param must be callable')
airflow.exceptions.AirflowException: `python_callable` param must be callable
[2022-12-08 16:16:36,694] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:16:36,701] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.279 seconds
[2022-12-08 16:17:07,024] {processor.py:153} INFO - Started process (PID=357) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:17:07,025] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:17:07,025] {logging_mixin.py:115} INFO - [2022-12-08 16:17:07,025] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:17:07,525] {logging_mixin.py:115} INFO - [2022-12-08 16:17:07,525] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:17:07,723] {logging_mixin.py:115} INFO - [2022-12-08 16:17:07,723] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:17:08,311] {logging_mixin.py:115} INFO - [2022-12-08 16:17:08,310] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 36, in <module>
    python_callable=move_data_between_s3_and_redshift.read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 158, in __init__
    raise AirflowException('`python_callable` param must be callable')
airflow.exceptions.AirflowException: `python_callable` param must be callable
[2022-12-08 16:17:08,312] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:17:08,319] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.297 seconds
[2022-12-08 16:17:25,075] {processor.py:153} INFO - Started process (PID=368) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:17:25,075] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:17:25,075] {logging_mixin.py:115} INFO - [2022-12-08 16:17:25,075] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:17:25,590] {logging_mixin.py:115} INFO - [2022-12-08 16:17:25,590] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:17:25,783] {logging_mixin.py:115} INFO - [2022-12-08 16:17:25,783] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:17:26,380] {logging_mixin.py:115} INFO - [2022-12-08 16:17:26,379] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 79, in <module>
    python_callable=read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 158, in __init__
    raise AirflowException('`python_callable` param must be callable')
airflow.exceptions.AirflowException: `python_callable` param must be callable
[2022-12-08 16:17:26,381] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:17:26,387] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.315 seconds
[2022-12-08 16:17:57,223] {processor.py:153} INFO - Started process (PID=436) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:17:57,224] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:17:57,225] {logging_mixin.py:115} INFO - [2022-12-08 16:17:57,224] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:17:57,741] {logging_mixin.py:115} INFO - [2022-12-08 16:17:57,741] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:17:57,942] {logging_mixin.py:115} INFO - [2022-12-08 16:17:57,942] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:17:58,548] {logging_mixin.py:115} INFO - [2022-12-08 16:17:58,547] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 79, in <module>
    python_callable=read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 158, in __init__
    raise AirflowException('`python_callable` param must be callable')
airflow.exceptions.AirflowException: `python_callable` param must be callable
[2022-12-08 16:17:58,548] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:17:58,555] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.334 seconds
[2022-12-08 16:18:28,929] {processor.py:153} INFO - Started process (PID=504) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:18:28,929] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:18:28,930] {logging_mixin.py:115} INFO - [2022-12-08 16:18:28,930] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:18:29,463] {logging_mixin.py:115} INFO - [2022-12-08 16:18:29,462] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:18:29,649] {logging_mixin.py:115} INFO - [2022-12-08 16:18:29,649] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:18:30,717] {logging_mixin.py:115} INFO - [2022-12-08 16:18:30,717] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 79, in <module>
    python_callable=read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 158, in __init__
    raise AirflowException('`python_callable` param must be callable')
airflow.exceptions.AirflowException: `python_callable` param must be callable
[2022-12-08 16:18:30,718] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:18:30,725] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.798 seconds
[2022-12-08 16:18:43,886] {processor.py:153} INFO - Started process (PID=554) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:18:43,886] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:18:43,886] {logging_mixin.py:115} INFO - [2022-12-08 16:18:43,886] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:18:44,405] {logging_mixin.py:115} INFO - [2022-12-08 16:18:44,405] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:18:44,590] {logging_mixin.py:115} INFO - [2022-12-08 16:18:44,590] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:18:45,192] {logging_mixin.py:115} INFO - [2022-12-08 16:18:45,191] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 79, in <module>
    python_callable=read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 158, in __init__
    raise AirflowException('`python_callable` param must be callable')
airflow.exceptions.AirflowException: `python_callable` param must be callable
[2022-12-08 16:18:45,192] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:18:45,199] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.316 seconds
[2022-12-08 16:19:15,872] {processor.py:153} INFO - Started process (PID=622) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:19:15,873] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:19:15,873] {logging_mixin.py:115} INFO - [2022-12-08 16:19:15,873] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:19:16,393] {logging_mixin.py:115} INFO - [2022-12-08 16:19:16,393] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:19:16,564] {logging_mixin.py:115} INFO - [2022-12-08 16:19:16,564] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:19:17,179] {logging_mixin.py:115} INFO - [2022-12-08 16:19:17,178] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 79, in <module>
    python_callable=read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 158, in __init__
    raise AirflowException('`python_callable` param must be callable')
airflow.exceptions.AirflowException: `python_callable` param must be callable
[2022-12-08 16:19:17,179] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:19:17,186] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.316 seconds
[2022-12-08 16:19:42,889] {processor.py:153} INFO - Started process (PID=687) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:19:42,890] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:19:42,890] {logging_mixin.py:115} INFO - [2022-12-08 16:19:42,890] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:19:43,406] {logging_mixin.py:115} INFO - [2022-12-08 16:19:43,405] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:19:43,585] {logging_mixin.py:115} INFO - [2022-12-08 16:19:43,585] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:19:44,192] {logging_mixin.py:115} INFO - [2022-12-08 16:19:44,191] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 79, in <module>
    python_callable=read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 158, in __init__
    raise AirflowException('`python_callable` param must be callable')
airflow.exceptions.AirflowException: `python_callable` param must be callable
[2022-12-08 16:19:44,193] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:19:44,200] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.313 seconds
[2022-12-08 16:20:08,188] {processor.py:153} INFO - Started process (PID=733) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:20:08,189] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:20:08,189] {logging_mixin.py:115} INFO - [2022-12-08 16:20:08,189] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:20:08,708] {logging_mixin.py:115} INFO - [2022-12-08 16:20:08,707] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:20:08,914] {logging_mixin.py:115} INFO - [2022-12-08 16:20:08,913] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:20:09,555] {logging_mixin.py:115} INFO - [2022-12-08 16:20:09,554] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 79, in <module>
    python_callable=read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 158, in __init__
    raise AirflowException('`python_callable` param must be callable')
airflow.exceptions.AirflowException: `python_callable` param must be callable
[2022-12-08 16:20:09,555] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:20:09,563] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.377 seconds
[2022-12-08 16:20:39,849] {processor.py:153} INFO - Started process (PID=808) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:20:39,849] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:20:39,850] {logging_mixin.py:115} INFO - [2022-12-08 16:20:39,849] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:20:40,379] {logging_mixin.py:115} INFO - [2022-12-08 16:20:40,379] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:20:40,569] {logging_mixin.py:115} INFO - [2022-12-08 16:20:40,569] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:20:41,171] {logging_mixin.py:115} INFO - [2022-12-08 16:20:41,170] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 79, in <module>
    python_callable=read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 158, in __init__
    raise AirflowException('`python_callable` param must be callable')
airflow.exceptions.AirflowException: `python_callable` param must be callable
[2022-12-08 16:20:41,171] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:20:41,178] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.331 seconds
[2022-12-08 16:21:11,554] {processor.py:153} INFO - Started process (PID=870) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:21:11,554] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:21:11,555] {logging_mixin.py:115} INFO - [2022-12-08 16:21:11,555] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:21:12,074] {logging_mixin.py:115} INFO - [2022-12-08 16:21:12,074] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:21:12,263] {logging_mixin.py:115} INFO - [2022-12-08 16:21:12,263] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:21:12,875] {logging_mixin.py:115} INFO - [2022-12-08 16:21:12,874] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 79, in <module>
    python_callable=read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 158, in __init__
    raise AirflowException('`python_callable` param must be callable')
airflow.exceptions.AirflowException: `python_callable` param must be callable
[2022-12-08 16:21:12,875] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:21:12,883] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.331 seconds
[2022-12-08 16:21:43,197] {processor.py:153} INFO - Started process (PID=944) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:21:43,198] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:21:43,198] {logging_mixin.py:115} INFO - [2022-12-08 16:21:43,198] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:21:43,710] {logging_mixin.py:115} INFO - [2022-12-08 16:21:43,709] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:21:43,908] {logging_mixin.py:115} INFO - [2022-12-08 16:21:43,908] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:21:44,547] {logging_mixin.py:115} INFO - [2022-12-08 16:21:44,546] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 79, in <module>
    python_callable=read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 158, in __init__
    raise AirflowException('`python_callable` param must be callable')
airflow.exceptions.AirflowException: `python_callable` param must be callable
[2022-12-08 16:21:44,547] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:21:44,554] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.359 seconds
[2022-12-08 16:22:15,061] {processor.py:153} INFO - Started process (PID=1012) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:22:15,062] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:22:15,062] {logging_mixin.py:115} INFO - [2022-12-08 16:22:15,062] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:22:15,578] {logging_mixin.py:115} INFO - [2022-12-08 16:22:15,578] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:22:15,748] {logging_mixin.py:115} INFO - [2022-12-08 16:22:15,748] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:22:16,359] {logging_mixin.py:115} INFO - [2022-12-08 16:22:16,358] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 79, in <module>
    python_callable=read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 158, in __init__
    raise AirflowException('`python_callable` param must be callable')
airflow.exceptions.AirflowException: `python_callable` param must be callable
[2022-12-08 16:22:16,359] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:22:16,366] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.307 seconds
[2022-12-08 16:22:47,189] {processor.py:153} INFO - Started process (PID=1080) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:22:47,190] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:22:47,190] {logging_mixin.py:115} INFO - [2022-12-08 16:22:47,190] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:22:47,707] {logging_mixin.py:115} INFO - [2022-12-08 16:22:47,707] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:22:47,870] {logging_mixin.py:115} INFO - [2022-12-08 16:22:47,870] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:22:48,673] {logging_mixin.py:115} INFO - [2022-12-08 16:22:48,673] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 79, in <module>
    python_callable=read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 158, in __init__
    raise AirflowException('`python_callable` param must be callable')
airflow.exceptions.AirflowException: `python_callable` param must be callable
[2022-12-08 16:22:48,674] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:22:48,680] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.493 seconds
[2022-12-08 16:23:19,509] {processor.py:153} INFO - Started process (PID=1148) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:23:19,509] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:23:19,509] {logging_mixin.py:115} INFO - [2022-12-08 16:23:19,509] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:23:20,032] {logging_mixin.py:115} INFO - [2022-12-08 16:23:20,032] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:23:20,248] {logging_mixin.py:115} INFO - [2022-12-08 16:23:20,248] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:23:20,859] {logging_mixin.py:115} INFO - [2022-12-08 16:23:20,858] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 79, in <module>
    python_callable=read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 158, in __init__
    raise AirflowException('`python_callable` param must be callable')
airflow.exceptions.AirflowException: `python_callable` param must be callable
[2022-12-08 16:23:20,859] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:23:20,866] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.360 seconds
[2022-12-08 16:23:51,638] {processor.py:153} INFO - Started process (PID=1216) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:23:51,638] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:23:51,639] {logging_mixin.py:115} INFO - [2022-12-08 16:23:51,639] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:23:52,166] {logging_mixin.py:115} INFO - [2022-12-08 16:23:52,166] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:23:52,328] {logging_mixin.py:115} INFO - [2022-12-08 16:23:52,328] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:23:52,931] {logging_mixin.py:115} INFO - [2022-12-08 16:23:52,930] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 79, in <module>
    python_callable=read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 158, in __init__
    raise AirflowException('`python_callable` param must be callable')
airflow.exceptions.AirflowException: `python_callable` param must be callable
[2022-12-08 16:23:52,931] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:23:52,937] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.302 seconds
[2022-12-08 16:24:23,774] {processor.py:153} INFO - Started process (PID=1275) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:24:23,775] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:24:23,775] {logging_mixin.py:115} INFO - [2022-12-08 16:24:23,775] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:24:24,292] {logging_mixin.py:115} INFO - [2022-12-08 16:24:24,292] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:24:24,482] {logging_mixin.py:115} INFO - [2022-12-08 16:24:24,482] {credentials.py:1120} INFO - Found credentials in environment variables.
[2022-12-08 16:24:25,113] {logging_mixin.py:115} INFO - [2022-12-08 16:24:25,112] {dagbag.py:320} ERROR - Failed to import: /opt/airflow/dags/move_data_s3redshift.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/move_data_s3redshift.py", line 79, in <module>
    python_callable=read_from_s3_and_insert_redshift(BUCKET_NAME,filename,GLUE_CONNECTION,'airflow_trips')
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 158, in __init__
    raise AirflowException('`python_callable` param must be callable')
airflow.exceptions.AirflowException: `python_callable` param must be callable
[2022-12-08 16:24:25,113] {processor.py:653} WARNING - No viable dags retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:24:25,120] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 1.348 seconds
[2022-12-08 16:24:48,412] {processor.py:153} INFO - Started process (PID=1339) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:24:48,413] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:24:48,413] {logging_mixin.py:115} INFO - [2022-12-08 16:24:48,413] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:24:48,921] {processor.py:651} INFO - DAG(s) dict_keys(['move_data_from_s3_to_redshidt_with_awswrangler']) retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:24:48,980] {logging_mixin.py:115} INFO - [2022-12-08 16:24:48,979] {manager.py:508} INFO - Created Permission View: can read on DAG:move_data_from_s3_to_redshidt_with_awswrangler
[2022-12-08 16:24:48,986] {logging_mixin.py:115} INFO - [2022-12-08 16:24:48,985] {manager.py:508} INFO - Created Permission View: can delete on DAG:move_data_from_s3_to_redshidt_with_awswrangler
[2022-12-08 16:24:48,990] {logging_mixin.py:115} INFO - [2022-12-08 16:24:48,990] {manager.py:508} INFO - Created Permission View: can edit on DAG:move_data_from_s3_to_redshidt_with_awswrangler
[2022-12-08 16:24:48,990] {logging_mixin.py:115} INFO - [2022-12-08 16:24:48,990] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-08 16:24:48,996] {logging_mixin.py:115} INFO - [2022-12-08 16:24:48,996] {dag.py:2390} INFO - Creating ORM DAG for move_data_from_s3_to_redshidt_with_awswrangler
[2022-12-08 16:24:49,006] {logging_mixin.py:115} INFO - [2022-12-08 16:24:49,005] {dag.py:2919} INFO - Setting next_dagrun for move_data_from_s3_to_redshidt_with_awswrangler to 2022-12-07T00:00:00+00:00, run_after=2022-12-08T00:00:00+00:00
[2022-12-08 16:24:49,018] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 0.608 seconds
[2022-12-08 16:25:19,940] {processor.py:153} INFO - Started process (PID=1407) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:25:19,940] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:25:19,941] {logging_mixin.py:115} INFO - [2022-12-08 16:25:19,941] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:25:20,481] {processor.py:651} INFO - DAG(s) dict_keys(['move_data_from_s3_to_redshidt_with_awswrangler']) retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:25:20,490] {logging_mixin.py:115} INFO - [2022-12-08 16:25:20,489] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-08 16:25:20,506] {logging_mixin.py:115} INFO - [2022-12-08 16:25:20,506] {dag.py:2919} INFO - Setting next_dagrun for move_data_from_s3_to_redshidt_with_awswrangler to 2022-12-08T00:00:00+00:00, run_after=2022-12-09T00:00:00+00:00
[2022-12-08 16:25:20,513] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 0.576 seconds
[2022-12-08 16:25:51,102] {processor.py:153} INFO - Started process (PID=1475) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:25:51,102] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:25:51,103] {logging_mixin.py:115} INFO - [2022-12-08 16:25:51,103] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:25:51,609] {processor.py:651} INFO - DAG(s) dict_keys(['move_data_from_s3_to_redshidt_with_awswrangler']) retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:25:51,617] {logging_mixin.py:115} INFO - [2022-12-08 16:25:51,617] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-08 16:25:51,633] {logging_mixin.py:115} INFO - [2022-12-08 16:25:51,633] {dag.py:2919} INFO - Setting next_dagrun for move_data_from_s3_to_redshidt_with_awswrangler to 2022-12-08T00:00:00+00:00, run_after=2022-12-09T00:00:00+00:00
[2022-12-08 16:25:51,640] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 0.541 seconds
[2022-12-08 16:26:00,217] {processor.py:153} INFO - Started process (PID=1490) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:26:00,217] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:26:00,217] {logging_mixin.py:115} INFO - [2022-12-08 16:26:00,217] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:26:00,753] {processor.py:651} INFO - DAG(s) dict_keys(['move_data_from_s3_to_redshidt_with_awswrangler']) retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:26:00,793] {logging_mixin.py:115} INFO - [2022-12-08 16:26:00,793] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-08 16:26:00,808] {logging_mixin.py:115} INFO - [2022-12-08 16:26:00,808] {dag.py:2919} INFO - Setting next_dagrun for move_data_from_s3_to_redshidt_with_awswrangler to 2022-12-08T00:00:00+00:00, run_after=2022-12-09T00:00:00+00:00
[2022-12-08 16:26:00,819] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 0.605 seconds
[2022-12-08 16:26:30,868] {processor.py:153} INFO - Started process (PID=1559) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:26:30,869] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:26:30,869] {logging_mixin.py:115} INFO - [2022-12-08 16:26:30,869] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:26:31,382] {processor.py:651} INFO - DAG(s) dict_keys(['move_data_from_s3_to_redshidt_with_awswrangler']) retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:26:31,390] {logging_mixin.py:115} INFO - [2022-12-08 16:26:31,390] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-08 16:26:31,407] {logging_mixin.py:115} INFO - [2022-12-08 16:26:31,407] {dag.py:2919} INFO - Setting next_dagrun for move_data_from_s3_to_redshidt_with_awswrangler to 2022-12-08T00:00:00+00:00, run_after=2022-12-09T00:00:00+00:00
[2022-12-08 16:26:31,414] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 0.548 seconds
[2022-12-08 16:27:01,798] {processor.py:153} INFO - Started process (PID=1618) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:27:01,798] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:27:01,799] {logging_mixin.py:115} INFO - [2022-12-08 16:27:01,799] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:27:02,291] {processor.py:651} INFO - DAG(s) dict_keys(['move_data_from_s3_to_redshidt_with_awswrangler']) retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:27:02,299] {logging_mixin.py:115} INFO - [2022-12-08 16:27:02,299] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-08 16:27:02,316] {logging_mixin.py:115} INFO - [2022-12-08 16:27:02,316] {dag.py:2919} INFO - Setting next_dagrun for move_data_from_s3_to_redshidt_with_awswrangler to 2022-12-08T00:00:00+00:00, run_after=2022-12-09T00:00:00+00:00
[2022-12-08 16:27:02,323] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 0.527 seconds
[2022-12-08 16:27:33,105] {processor.py:153} INFO - Started process (PID=1686) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:27:33,106] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:27:33,106] {logging_mixin.py:115} INFO - [2022-12-08 16:27:33,106] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:27:33,605] {processor.py:651} INFO - DAG(s) dict_keys(['move_data_from_s3_to_redshidt_with_awswrangler']) retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:27:33,613] {logging_mixin.py:115} INFO - [2022-12-08 16:27:33,612] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-08 16:27:33,629] {logging_mixin.py:115} INFO - [2022-12-08 16:27:33,629] {dag.py:2919} INFO - Setting next_dagrun for move_data_from_s3_to_redshidt_with_awswrangler to 2022-12-08T00:00:00+00:00, run_after=2022-12-09T00:00:00+00:00
[2022-12-08 16:27:33,636] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 0.534 seconds
[2022-12-08 16:28:04,414] {processor.py:153} INFO - Started process (PID=1754) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:28:04,414] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:28:04,414] {logging_mixin.py:115} INFO - [2022-12-08 16:28:04,414] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:28:04,903] {processor.py:651} INFO - DAG(s) dict_keys(['move_data_from_s3_to_redshidt_with_awswrangler']) retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:28:04,911] {logging_mixin.py:115} INFO - [2022-12-08 16:28:04,911] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-08 16:28:04,927] {logging_mixin.py:115} INFO - [2022-12-08 16:28:04,927] {dag.py:2919} INFO - Setting next_dagrun for move_data_from_s3_to_redshidt_with_awswrangler to 2022-12-08T00:00:00+00:00, run_after=2022-12-09T00:00:00+00:00
[2022-12-08 16:28:04,934] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 0.522 seconds
[2022-12-08 16:28:17,210] {processor.py:153} INFO - Started process (PID=1800) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:28:17,211] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:28:17,211] {logging_mixin.py:115} INFO - [2022-12-08 16:28:17,211] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:28:17,702] {processor.py:651} INFO - DAG(s) dict_keys(['move_data_from_s3_to_redshidt_with_awswrangler']) retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:28:17,740] {logging_mixin.py:115} INFO - [2022-12-08 16:28:17,740] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-08 16:28:17,755] {logging_mixin.py:115} INFO - [2022-12-08 16:28:17,755] {dag.py:2919} INFO - Setting next_dagrun for move_data_from_s3_to_redshidt_with_awswrangler to 2022-12-08T00:00:00+00:00, run_after=2022-12-09T00:00:00+00:00
[2022-12-08 16:28:17,764] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 0.556 seconds
[2022-12-08 16:28:27,357] {processor.py:153} INFO - Started process (PID=1815) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:28:27,357] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:28:27,358] {logging_mixin.py:115} INFO - [2022-12-08 16:28:27,358] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:28:27,848] {processor.py:651} INFO - DAG(s) dict_keys(['move_data_from_s3_to_redshidt_with_awswrangler']) retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:28:27,850] {logging_mixin.py:115} INFO - [2022-12-08 16:28:27,850] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-08 16:28:27,867] {logging_mixin.py:115} INFO - [2022-12-08 16:28:27,867] {dag.py:2919} INFO - Setting next_dagrun for move_data_from_s3_to_redshidt_with_awswrangler to 2022-12-08T00:00:00+00:00, run_after=2022-12-09T00:00:00+00:00
[2022-12-08 16:28:27,875] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 0.520 seconds
[2022-12-08 16:28:58,139] {processor.py:153} INFO - Started process (PID=1883) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:28:58,140] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:28:58,140] {logging_mixin.py:115} INFO - [2022-12-08 16:28:58,140] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:28:58,634] {processor.py:651} INFO - DAG(s) dict_keys(['move_data_from_s3_to_redshidt_with_awswrangler']) retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:28:58,642] {logging_mixin.py:115} INFO - [2022-12-08 16:28:58,642] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-08 16:28:58,659] {logging_mixin.py:115} INFO - [2022-12-08 16:28:58,658] {dag.py:2919} INFO - Setting next_dagrun for move_data_from_s3_to_redshidt_with_awswrangler to 2022-12-08T00:00:00+00:00, run_after=2022-12-09T00:00:00+00:00
[2022-12-08 16:28:58,666] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 0.529 seconds
[2022-12-08 16:29:28,921] {processor.py:153} INFO - Started process (PID=1940) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:29:28,922] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:29:28,922] {logging_mixin.py:115} INFO - [2022-12-08 16:29:28,922] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:29:29,434] {processor.py:651} INFO - DAG(s) dict_keys(['move_data_from_s3_to_redshidt_with_awswrangler']) retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:29:29,442] {logging_mixin.py:115} INFO - [2022-12-08 16:29:29,442] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-08 16:29:29,459] {logging_mixin.py:115} INFO - [2022-12-08 16:29:29,459] {dag.py:2919} INFO - Setting next_dagrun for move_data_from_s3_to_redshidt_with_awswrangler to 2022-12-08T00:00:00+00:00, run_after=2022-12-09T00:00:00+00:00
[2022-12-08 16:29:29,466] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 0.547 seconds
[2022-12-08 16:30:00,180] {processor.py:153} INFO - Started process (PID=2006) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:30:00,181] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:30:00,181] {logging_mixin.py:115} INFO - [2022-12-08 16:30:00,181] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:30:00,715] {processor.py:651} INFO - DAG(s) dict_keys(['move_data_from_s3_to_redshidt_with_awswrangler']) retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:30:00,723] {logging_mixin.py:115} INFO - [2022-12-08 16:30:00,723] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-08 16:30:00,740] {logging_mixin.py:115} INFO - [2022-12-08 16:30:00,740] {dag.py:2919} INFO - Setting next_dagrun for move_data_from_s3_to_redshidt_with_awswrangler to 2022-12-08T00:00:00+00:00, run_after=2022-12-09T00:00:00+00:00
[2022-12-08 16:30:00,747] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 0.569 seconds
[2022-12-08 16:30:31,409] {processor.py:153} INFO - Started process (PID=2074) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:30:31,409] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:30:31,410] {logging_mixin.py:115} INFO - [2022-12-08 16:30:31,410] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:30:31,941] {processor.py:651} INFO - DAG(s) dict_keys(['move_data_from_s3_to_redshidt_with_awswrangler']) retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:30:31,950] {logging_mixin.py:115} INFO - [2022-12-08 16:30:31,950] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-08 16:30:31,966] {logging_mixin.py:115} INFO - [2022-12-08 16:30:31,966] {dag.py:2919} INFO - Setting next_dagrun for move_data_from_s3_to_redshidt_with_awswrangler to 2022-12-08T00:00:00+00:00, run_after=2022-12-09T00:00:00+00:00
[2022-12-08 16:30:31,973] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 0.567 seconds
[2022-12-08 16:31:02,750] {processor.py:153} INFO - Started process (PID=2142) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:31:02,751] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:31:02,751] {logging_mixin.py:115} INFO - [2022-12-08 16:31:02,751] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:31:03,260] {processor.py:651} INFO - DAG(s) dict_keys(['move_data_from_s3_to_redshidt_with_awswrangler']) retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:31:03,269] {logging_mixin.py:115} INFO - [2022-12-08 16:31:03,268] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-08 16:31:03,284] {logging_mixin.py:115} INFO - [2022-12-08 16:31:03,284] {dag.py:2919} INFO - Setting next_dagrun for move_data_from_s3_to_redshidt_with_awswrangler to 2022-12-08T00:00:00+00:00, run_after=2022-12-09T00:00:00+00:00
[2022-12-08 16:31:03,292] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 0.544 seconds
[2022-12-08 16:31:34,155] {processor.py:153} INFO - Started process (PID=2210) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:31:34,155] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:31:34,156] {logging_mixin.py:115} INFO - [2022-12-08 16:31:34,156] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:31:34,675] {processor.py:651} INFO - DAG(s) dict_keys(['move_data_from_s3_to_redshidt_with_awswrangler']) retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:31:34,684] {logging_mixin.py:115} INFO - [2022-12-08 16:31:34,684] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-08 16:31:34,701] {logging_mixin.py:115} INFO - [2022-12-08 16:31:34,701] {dag.py:2919} INFO - Setting next_dagrun for move_data_from_s3_to_redshidt_with_awswrangler to 2022-12-08T00:00:00+00:00, run_after=2022-12-09T00:00:00+00:00
[2022-12-08 16:31:34,708] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 0.556 seconds
[2022-12-08 16:32:05,414] {processor.py:153} INFO - Started process (PID=2269) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:32:05,415] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:32:05,415] {logging_mixin.py:115} INFO - [2022-12-08 16:32:05,415] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:32:05,921] {processor.py:651} INFO - DAG(s) dict_keys(['move_data_from_s3_to_redshidt_with_awswrangler']) retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:32:05,930] {logging_mixin.py:115} INFO - [2022-12-08 16:32:05,930] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-08 16:32:05,946] {logging_mixin.py:115} INFO - [2022-12-08 16:32:05,946] {dag.py:2919} INFO - Setting next_dagrun for move_data_from_s3_to_redshidt_with_awswrangler to 2022-12-08T00:00:00+00:00, run_after=2022-12-09T00:00:00+00:00
[2022-12-08 16:32:05,953] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 0.542 seconds
[2022-12-08 16:32:36,727] {processor.py:153} INFO - Started process (PID=2337) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:32:36,727] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:32:36,728] {logging_mixin.py:115} INFO - [2022-12-08 16:32:36,728] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:32:37,235] {processor.py:651} INFO - DAG(s) dict_keys(['move_data_from_s3_to_redshidt_with_awswrangler']) retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:32:37,245] {logging_mixin.py:115} INFO - [2022-12-08 16:32:37,244] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-08 16:32:37,260] {logging_mixin.py:115} INFO - [2022-12-08 16:32:37,260] {dag.py:2919} INFO - Setting next_dagrun for move_data_from_s3_to_redshidt_with_awswrangler to 2022-12-08T00:00:00+00:00, run_after=2022-12-09T00:00:00+00:00
[2022-12-08 16:32:37,267] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 0.542 seconds
[2022-12-08 16:33:08,067] {processor.py:153} INFO - Started process (PID=2405) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:33:08,068] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:33:08,069] {logging_mixin.py:115} INFO - [2022-12-08 16:33:08,068] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:33:08,575] {processor.py:651} INFO - DAG(s) dict_keys(['move_data_from_s3_to_redshidt_with_awswrangler']) retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:33:08,585] {logging_mixin.py:115} INFO - [2022-12-08 16:33:08,584] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-08 16:33:08,600] {logging_mixin.py:115} INFO - [2022-12-08 16:33:08,600] {dag.py:2919} INFO - Setting next_dagrun for move_data_from_s3_to_redshidt_with_awswrangler to 2022-12-08T00:00:00+00:00, run_after=2022-12-09T00:00:00+00:00
[2022-12-08 16:33:08,607] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 0.542 seconds
[2022-12-08 16:33:39,331] {processor.py:153} INFO - Started process (PID=2473) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:33:39,332] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:33:39,332] {logging_mixin.py:115} INFO - [2022-12-08 16:33:39,332] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:33:39,838] {processor.py:651} INFO - DAG(s) dict_keys(['move_data_from_s3_to_redshidt_with_awswrangler']) retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:33:39,847] {logging_mixin.py:115} INFO - [2022-12-08 16:33:39,846] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-08 16:33:39,862] {logging_mixin.py:115} INFO - [2022-12-08 16:33:39,862] {dag.py:2919} INFO - Setting next_dagrun for move_data_from_s3_to_redshidt_with_awswrangler to 2022-12-08T00:00:00+00:00, run_after=2022-12-09T00:00:00+00:00
[2022-12-08 16:33:39,869] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 0.540 seconds
[2022-12-08 16:34:10,619] {processor.py:153} INFO - Started process (PID=2532) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:34:10,619] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:34:10,620] {logging_mixin.py:115} INFO - [2022-12-08 16:34:10,620] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:34:11,153] {processor.py:651} INFO - DAG(s) dict_keys(['move_data_from_s3_to_redshidt_with_awswrangler']) retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:34:11,162] {logging_mixin.py:115} INFO - [2022-12-08 16:34:11,161] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-08 16:34:11,177] {logging_mixin.py:115} INFO - [2022-12-08 16:34:11,177] {dag.py:2919} INFO - Setting next_dagrun for move_data_from_s3_to_redshidt_with_awswrangler to 2022-12-08T00:00:00+00:00, run_after=2022-12-09T00:00:00+00:00
[2022-12-08 16:34:11,185] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 0.568 seconds
[2022-12-08 16:34:41,919] {processor.py:153} INFO - Started process (PID=2600) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:34:41,919] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:34:41,920] {logging_mixin.py:115} INFO - [2022-12-08 16:34:41,920] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:34:42,423] {processor.py:651} INFO - DAG(s) dict_keys(['move_data_from_s3_to_redshidt_with_awswrangler']) retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:34:42,432] {logging_mixin.py:115} INFO - [2022-12-08 16:34:42,432] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-08 16:34:42,448] {logging_mixin.py:115} INFO - [2022-12-08 16:34:42,448] {dag.py:2919} INFO - Setting next_dagrun for move_data_from_s3_to_redshidt_with_awswrangler to 2022-12-08T00:00:00+00:00, run_after=2022-12-09T00:00:00+00:00
[2022-12-08 16:34:42,455] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 0.539 seconds
[2022-12-08 16:35:13,014] {processor.py:153} INFO - Started process (PID=2681) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:35:13,014] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:35:13,015] {logging_mixin.py:115} INFO - [2022-12-08 16:35:13,015] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:35:13,519] {processor.py:651} INFO - DAG(s) dict_keys(['move_data_from_s3_to_redshidt_with_awswrangler']) retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:35:13,527] {logging_mixin.py:115} INFO - [2022-12-08 16:35:13,527] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-08 16:35:13,543] {logging_mixin.py:115} INFO - [2022-12-08 16:35:13,543] {dag.py:2919} INFO - Setting next_dagrun for move_data_from_s3_to_redshidt_with_awswrangler to 2022-12-08T00:00:00+00:00, run_after=2022-12-09T00:00:00+00:00
[2022-12-08 16:35:13,550] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 0.538 seconds
[2022-12-08 16:35:44,427] {processor.py:153} INFO - Started process (PID=2761) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:35:44,427] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:35:44,428] {logging_mixin.py:115} INFO - [2022-12-08 16:35:44,427] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:35:44,930] {processor.py:651} INFO - DAG(s) dict_keys(['move_data_from_s3_to_redshidt_with_awswrangler']) retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:35:44,938] {logging_mixin.py:115} INFO - [2022-12-08 16:35:44,937] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-08 16:35:44,954] {logging_mixin.py:115} INFO - [2022-12-08 16:35:44,953] {dag.py:2919} INFO - Setting next_dagrun for move_data_from_s3_to_redshidt_with_awswrangler to 2022-12-08T00:00:00+00:00, run_after=2022-12-09T00:00:00+00:00
[2022-12-08 16:35:44,961] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 0.536 seconds
[2022-12-08 16:36:14,984] {processor.py:153} INFO - Started process (PID=2839) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:36:14,985] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:36:14,985] {logging_mixin.py:115} INFO - [2022-12-08 16:36:14,985] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:36:15,499] {processor.py:651} INFO - DAG(s) dict_keys(['move_data_from_s3_to_redshidt_with_awswrangler']) retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:36:15,507] {logging_mixin.py:115} INFO - [2022-12-08 16:36:15,507] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-08 16:36:15,523] {logging_mixin.py:115} INFO - [2022-12-08 16:36:15,523] {dag.py:2919} INFO - Setting next_dagrun for move_data_from_s3_to_redshidt_with_awswrangler to 2022-12-08T00:00:00+00:00, run_after=2022-12-09T00:00:00+00:00
[2022-12-08 16:36:15,530] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 0.548 seconds
[2022-12-08 16:36:46,507] {processor.py:153} INFO - Started process (PID=2911) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:36:46,508] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:36:46,508] {logging_mixin.py:115} INFO - [2022-12-08 16:36:46,508] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:36:47,024] {processor.py:651} INFO - DAG(s) dict_keys(['move_data_from_s3_to_redshidt_with_awswrangler']) retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:36:47,032] {logging_mixin.py:115} INFO - [2022-12-08 16:36:47,031] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-08 16:36:47,047] {logging_mixin.py:115} INFO - [2022-12-08 16:36:47,047] {dag.py:2919} INFO - Setting next_dagrun for move_data_from_s3_to_redshidt_with_awswrangler to 2022-12-08T00:00:00+00:00, run_after=2022-12-09T00:00:00+00:00
[2022-12-08 16:36:47,054] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 0.549 seconds
[2022-12-08 16:37:17,948] {processor.py:153} INFO - Started process (PID=2989) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:37:17,948] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:37:17,949] {logging_mixin.py:115} INFO - [2022-12-08 16:37:17,949] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:37:18,456] {processor.py:651} INFO - DAG(s) dict_keys(['move_data_from_s3_to_redshidt_with_awswrangler']) retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:37:18,465] {logging_mixin.py:115} INFO - [2022-12-08 16:37:18,464] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-08 16:37:18,480] {logging_mixin.py:115} INFO - [2022-12-08 16:37:18,480] {dag.py:2919} INFO - Setting next_dagrun for move_data_from_s3_to_redshidt_with_awswrangler to 2022-12-08T00:00:00+00:00, run_after=2022-12-09T00:00:00+00:00
[2022-12-08 16:37:18,488] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 0.542 seconds
[2022-12-08 16:37:49,475] {processor.py:153} INFO - Started process (PID=3065) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:37:49,476] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:37:49,476] {logging_mixin.py:115} INFO - [2022-12-08 16:37:49,476] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:37:49,996] {processor.py:651} INFO - DAG(s) dict_keys(['move_data_from_s3_to_redshidt_with_awswrangler']) retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:37:50,004] {logging_mixin.py:115} INFO - [2022-12-08 16:37:50,004] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-08 16:37:50,020] {logging_mixin.py:115} INFO - [2022-12-08 16:37:50,020] {dag.py:2919} INFO - Setting next_dagrun for move_data_from_s3_to_redshidt_with_awswrangler to 2022-12-08T00:00:00+00:00, run_after=2022-12-09T00:00:00+00:00
[2022-12-08 16:37:50,027] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 0.554 seconds
[2022-12-08 16:38:20,237] {processor.py:153} INFO - Started process (PID=3141) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:38:20,237] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:38:20,237] {logging_mixin.py:115} INFO - [2022-12-08 16:38:20,237] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:38:20,741] {processor.py:651} INFO - DAG(s) dict_keys(['move_data_from_s3_to_redshidt_with_awswrangler']) retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:38:20,749] {logging_mixin.py:115} INFO - [2022-12-08 16:38:20,748] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-08 16:38:20,764] {logging_mixin.py:115} INFO - [2022-12-08 16:38:20,764] {dag.py:2919} INFO - Setting next_dagrun for move_data_from_s3_to_redshidt_with_awswrangler to 2022-12-08T00:00:00+00:00, run_after=2022-12-09T00:00:00+00:00
[2022-12-08 16:38:20,771] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 0.537 seconds
[2022-12-08 16:38:51,744] {processor.py:153} INFO - Started process (PID=3208) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:38:51,745] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:38:51,745] {logging_mixin.py:115} INFO - [2022-12-08 16:38:51,745] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:38:52,254] {processor.py:651} INFO - DAG(s) dict_keys(['move_data_from_s3_to_redshidt_with_awswrangler']) retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:38:52,263] {logging_mixin.py:115} INFO - [2022-12-08 16:38:52,262] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-08 16:38:52,278] {logging_mixin.py:115} INFO - [2022-12-08 16:38:52,278] {dag.py:2919} INFO - Setting next_dagrun for move_data_from_s3_to_redshidt_with_awswrangler to 2022-12-08T00:00:00+00:00, run_after=2022-12-09T00:00:00+00:00
[2022-12-08 16:38:52,286] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 0.544 seconds
[2022-12-08 16:39:22,342] {processor.py:153} INFO - Started process (PID=3284) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:39:22,342] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:39:22,343] {logging_mixin.py:115} INFO - [2022-12-08 16:39:22,343] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:39:22,847] {processor.py:651} INFO - DAG(s) dict_keys(['move_data_from_s3_to_redshidt_with_awswrangler']) retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:39:22,855] {logging_mixin.py:115} INFO - [2022-12-08 16:39:22,855] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-08 16:39:22,871] {logging_mixin.py:115} INFO - [2022-12-08 16:39:22,871] {dag.py:2919} INFO - Setting next_dagrun for move_data_from_s3_to_redshidt_with_awswrangler to 2022-12-08T00:00:00+00:00, run_after=2022-12-09T00:00:00+00:00
[2022-12-08 16:39:22,878] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 0.539 seconds
[2022-12-08 16:39:53,847] {processor.py:153} INFO - Started process (PID=3360) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:39:53,847] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:39:53,848] {logging_mixin.py:115} INFO - [2022-12-08 16:39:53,848] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:39:54,355] {processor.py:651} INFO - DAG(s) dict_keys(['move_data_from_s3_to_redshidt_with_awswrangler']) retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:39:54,363] {logging_mixin.py:115} INFO - [2022-12-08 16:39:54,362] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-08 16:39:54,378] {logging_mixin.py:115} INFO - [2022-12-08 16:39:54,378] {dag.py:2919} INFO - Setting next_dagrun for move_data_from_s3_to_redshidt_with_awswrangler to 2022-12-08T00:00:00+00:00, run_after=2022-12-09T00:00:00+00:00
[2022-12-08 16:39:54,386] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 0.541 seconds
[2022-12-08 16:40:24,473] {processor.py:153} INFO - Started process (PID=3436) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:40:24,474] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:40:24,474] {logging_mixin.py:115} INFO - [2022-12-08 16:40:24,474] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:40:24,978] {processor.py:651} INFO - DAG(s) dict_keys(['move_data_from_s3_to_redshidt_with_awswrangler']) retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:40:24,987] {logging_mixin.py:115} INFO - [2022-12-08 16:40:24,986] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-08 16:40:25,002] {logging_mixin.py:115} INFO - [2022-12-08 16:40:25,002] {dag.py:2919} INFO - Setting next_dagrun for move_data_from_s3_to_redshidt_with_awswrangler to 2022-12-08T00:00:00+00:00, run_after=2022-12-09T00:00:00+00:00
[2022-12-08 16:40:25,009] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 0.538 seconds
[2022-12-08 16:40:55,894] {processor.py:153} INFO - Started process (PID=3503) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:40:55,895] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:40:55,895] {logging_mixin.py:115} INFO - [2022-12-08 16:40:55,895] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:40:56,401] {processor.py:651} INFO - DAG(s) dict_keys(['move_data_from_s3_to_redshidt_with_awswrangler']) retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:40:56,410] {logging_mixin.py:115} INFO - [2022-12-08 16:40:56,409] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-08 16:40:56,425] {logging_mixin.py:115} INFO - [2022-12-08 16:40:56,425] {dag.py:2919} INFO - Setting next_dagrun for move_data_from_s3_to_redshidt_with_awswrangler to 2022-12-08T00:00:00+00:00, run_after=2022-12-09T00:00:00+00:00
[2022-12-08 16:40:56,431] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 0.539 seconds
[2022-12-08 16:41:27,432] {processor.py:153} INFO - Started process (PID=3579) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:41:27,432] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:41:27,433] {logging_mixin.py:115} INFO - [2022-12-08 16:41:27,433] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:41:27,938] {processor.py:651} INFO - DAG(s) dict_keys(['move_data_from_s3_to_redshidt_with_awswrangler']) retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:41:27,946] {logging_mixin.py:115} INFO - [2022-12-08 16:41:27,946] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-08 16:41:27,962] {logging_mixin.py:115} INFO - [2022-12-08 16:41:27,962] {dag.py:2919} INFO - Setting next_dagrun for move_data_from_s3_to_redshidt_with_awswrangler to 2022-12-08T00:00:00+00:00, run_after=2022-12-09T00:00:00+00:00
[2022-12-08 16:41:27,968] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 0.539 seconds
[2022-12-08 16:41:58,843] {processor.py:153} INFO - Started process (PID=3655) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:41:58,843] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:41:58,844] {logging_mixin.py:115} INFO - [2022-12-08 16:41:58,844] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:41:59,345] {processor.py:651} INFO - DAG(s) dict_keys(['move_data_from_s3_to_redshidt_with_awswrangler']) retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:41:59,354] {logging_mixin.py:115} INFO - [2022-12-08 16:41:59,353] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-08 16:41:59,369] {logging_mixin.py:115} INFO - [2022-12-08 16:41:59,369] {dag.py:2919} INFO - Setting next_dagrun for move_data_from_s3_to_redshidt_with_awswrangler to 2022-12-08T00:00:00+00:00, run_after=2022-12-09T00:00:00+00:00
[2022-12-08 16:41:59,375] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 0.535 seconds
[2022-12-08 16:42:30,162] {processor.py:153} INFO - Started process (PID=3731) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:42:30,162] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:42:30,162] {logging_mixin.py:115} INFO - [2022-12-08 16:42:30,162] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:42:30,667] {processor.py:651} INFO - DAG(s) dict_keys(['move_data_from_s3_to_redshidt_with_awswrangler']) retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:42:30,675] {logging_mixin.py:115} INFO - [2022-12-08 16:42:30,675] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-08 16:42:30,691] {logging_mixin.py:115} INFO - [2022-12-08 16:42:30,691] {dag.py:2919} INFO - Setting next_dagrun for move_data_from_s3_to_redshidt_with_awswrangler to 2022-12-08T00:00:00+00:00, run_after=2022-12-09T00:00:00+00:00
[2022-12-08 16:42:30,697] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 0.538 seconds
[2022-12-08 16:43:01,523] {processor.py:153} INFO - Started process (PID=3807) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:43:01,524] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:43:01,524] {logging_mixin.py:115} INFO - [2022-12-08 16:43:01,524] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:43:02,051] {processor.py:651} INFO - DAG(s) dict_keys(['move_data_from_s3_to_redshidt_with_awswrangler']) retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:43:02,060] {logging_mixin.py:115} INFO - [2022-12-08 16:43:02,059] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-08 16:43:02,075] {logging_mixin.py:115} INFO - [2022-12-08 16:43:02,075] {dag.py:2919} INFO - Setting next_dagrun for move_data_from_s3_to_redshidt_with_awswrangler to 2022-12-08T00:00:00+00:00, run_after=2022-12-09T00:00:00+00:00
[2022-12-08 16:43:02,082] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 0.561 seconds
[2022-12-08 16:43:32,191] {processor.py:153} INFO - Started process (PID=3874) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:43:32,191] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:43:32,192] {logging_mixin.py:115} INFO - [2022-12-08 16:43:32,191] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:43:32,695] {processor.py:651} INFO - DAG(s) dict_keys(['move_data_from_s3_to_redshidt_with_awswrangler']) retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:43:32,703] {logging_mixin.py:115} INFO - [2022-12-08 16:43:32,703] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-08 16:43:32,718] {logging_mixin.py:115} INFO - [2022-12-08 16:43:32,718] {dag.py:2919} INFO - Setting next_dagrun for move_data_from_s3_to_redshidt_with_awswrangler to 2022-12-08T00:00:00+00:00, run_after=2022-12-09T00:00:00+00:00
[2022-12-08 16:43:32,725] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 0.536 seconds
[2022-12-08 16:44:03,571] {processor.py:153} INFO - Started process (PID=3950) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:44:03,571] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:44:03,572] {logging_mixin.py:115} INFO - [2022-12-08 16:44:03,572] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:44:04,077] {processor.py:651} INFO - DAG(s) dict_keys(['move_data_from_s3_to_redshidt_with_awswrangler']) retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:44:04,085] {logging_mixin.py:115} INFO - [2022-12-08 16:44:04,085] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-08 16:44:04,100] {logging_mixin.py:115} INFO - [2022-12-08 16:44:04,100] {dag.py:2919} INFO - Setting next_dagrun for move_data_from_s3_to_redshidt_with_awswrangler to 2022-12-08T00:00:00+00:00, run_after=2022-12-09T00:00:00+00:00
[2022-12-08 16:44:04,107] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 0.539 seconds
[2022-12-08 16:44:35,043] {processor.py:153} INFO - Started process (PID=4026) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:44:35,043] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:44:35,044] {logging_mixin.py:115} INFO - [2022-12-08 16:44:35,044] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:44:35,551] {processor.py:651} INFO - DAG(s) dict_keys(['move_data_from_s3_to_redshidt_with_awswrangler']) retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:44:35,560] {logging_mixin.py:115} INFO - [2022-12-08 16:44:35,559] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-08 16:44:35,575] {logging_mixin.py:115} INFO - [2022-12-08 16:44:35,575] {dag.py:2919} INFO - Setting next_dagrun for move_data_from_s3_to_redshidt_with_awswrangler to 2022-12-08T00:00:00+00:00, run_after=2022-12-09T00:00:00+00:00
[2022-12-08 16:44:35,582] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 0.541 seconds
[2022-12-08 16:45:06,559] {processor.py:153} INFO - Started process (PID=4102) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:45:06,560] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:45:06,560] {logging_mixin.py:115} INFO - [2022-12-08 16:45:06,560] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:45:07,068] {processor.py:651} INFO - DAG(s) dict_keys(['move_data_from_s3_to_redshidt_with_awswrangler']) retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:45:07,076] {logging_mixin.py:115} INFO - [2022-12-08 16:45:07,076] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-08 16:45:07,092] {logging_mixin.py:115} INFO - [2022-12-08 16:45:07,091] {dag.py:2919} INFO - Setting next_dagrun for move_data_from_s3_to_redshidt_with_awswrangler to 2022-12-08T00:00:00+00:00, run_after=2022-12-09T00:00:00+00:00
[2022-12-08 16:45:07,098] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 0.542 seconds
[2022-12-08 16:45:38,030] {processor.py:153} INFO - Started process (PID=4169) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:45:38,030] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:45:38,030] {logging_mixin.py:115} INFO - [2022-12-08 16:45:38,030] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:45:38,562] {processor.py:651} INFO - DAG(s) dict_keys(['move_data_from_s3_to_redshidt_with_awswrangler']) retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:45:38,570] {logging_mixin.py:115} INFO - [2022-12-08 16:45:38,570] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-08 16:45:38,586] {logging_mixin.py:115} INFO - [2022-12-08 16:45:38,586] {dag.py:2919} INFO - Setting next_dagrun for move_data_from_s3_to_redshidt_with_awswrangler to 2022-12-08T00:00:00+00:00, run_after=2022-12-09T00:00:00+00:00
[2022-12-08 16:45:38,593] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 0.566 seconds
[2022-12-08 16:46:08,653] {processor.py:153} INFO - Started process (PID=4245) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:46:08,654] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:46:08,654] {logging_mixin.py:115} INFO - [2022-12-08 16:46:08,654] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:46:09,165] {processor.py:651} INFO - DAG(s) dict_keys(['move_data_from_s3_to_redshidt_with_awswrangler']) retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:46:09,173] {logging_mixin.py:115} INFO - [2022-12-08 16:46:09,173] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-08 16:46:09,188] {logging_mixin.py:115} INFO - [2022-12-08 16:46:09,188] {dag.py:2919} INFO - Setting next_dagrun for move_data_from_s3_to_redshidt_with_awswrangler to 2022-12-08T00:00:00+00:00, run_after=2022-12-09T00:00:00+00:00
[2022-12-08 16:46:09,195] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 0.545 seconds
[2022-12-08 16:46:40,178] {processor.py:153} INFO - Started process (PID=4321) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:46:40,178] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:46:40,178] {logging_mixin.py:115} INFO - [2022-12-08 16:46:40,178] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:46:40,704] {processor.py:651} INFO - DAG(s) dict_keys(['move_data_from_s3_to_redshidt_with_awswrangler']) retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:46:40,713] {logging_mixin.py:115} INFO - [2022-12-08 16:46:40,712] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-08 16:46:40,728] {logging_mixin.py:115} INFO - [2022-12-08 16:46:40,728] {dag.py:2919} INFO - Setting next_dagrun for move_data_from_s3_to_redshidt_with_awswrangler to 2022-12-08T00:00:00+00:00, run_after=2022-12-09T00:00:00+00:00
[2022-12-08 16:46:40,734] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 0.559 seconds
[2022-12-08 16:47:10,897] {processor.py:153} INFO - Started process (PID=4397) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:47:10,898] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:47:10,898] {logging_mixin.py:115} INFO - [2022-12-08 16:47:10,898] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:47:11,400] {processor.py:651} INFO - DAG(s) dict_keys(['move_data_from_s3_to_redshidt_with_awswrangler']) retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:47:11,408] {logging_mixin.py:115} INFO - [2022-12-08 16:47:11,408] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-08 16:47:11,424] {logging_mixin.py:115} INFO - [2022-12-08 16:47:11,424] {dag.py:2919} INFO - Setting next_dagrun for move_data_from_s3_to_redshidt_with_awswrangler to 2022-12-08T00:00:00+00:00, run_after=2022-12-09T00:00:00+00:00
[2022-12-08 16:47:11,430] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 0.535 seconds
[2022-12-08 16:47:42,223] {processor.py:153} INFO - Started process (PID=4464) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:47:42,223] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:47:42,223] {logging_mixin.py:115} INFO - [2022-12-08 16:47:42,223] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:47:42,734] {processor.py:651} INFO - DAG(s) dict_keys(['move_data_from_s3_to_redshidt_with_awswrangler']) retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:47:42,743] {logging_mixin.py:115} INFO - [2022-12-08 16:47:42,742] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-08 16:47:42,758] {logging_mixin.py:115} INFO - [2022-12-08 16:47:42,758] {dag.py:2919} INFO - Setting next_dagrun for move_data_from_s3_to_redshidt_with_awswrangler to 2022-12-08T00:00:00+00:00, run_after=2022-12-09T00:00:00+00:00
[2022-12-08 16:47:42,765] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 0.544 seconds
[2022-12-08 16:48:13,639] {processor.py:153} INFO - Started process (PID=4540) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:48:13,639] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:48:13,640] {logging_mixin.py:115} INFO - [2022-12-08 16:48:13,640] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:48:14,141] {processor.py:651} INFO - DAG(s) dict_keys(['move_data_from_s3_to_redshidt_with_awswrangler']) retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:48:14,150] {logging_mixin.py:115} INFO - [2022-12-08 16:48:14,150] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-08 16:48:14,165] {logging_mixin.py:115} INFO - [2022-12-08 16:48:14,165] {dag.py:2919} INFO - Setting next_dagrun for move_data_from_s3_to_redshidt_with_awswrangler to 2022-12-08T00:00:00+00:00, run_after=2022-12-09T00:00:00+00:00
[2022-12-08 16:48:14,172] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 0.536 seconds
[2022-12-08 16:48:45,031] {processor.py:153} INFO - Started process (PID=4616) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:48:45,031] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:48:45,031] {logging_mixin.py:115} INFO - [2022-12-08 16:48:45,031] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:48:45,546] {processor.py:651} INFO - DAG(s) dict_keys(['move_data_from_s3_to_redshidt_with_awswrangler']) retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:48:45,554] {logging_mixin.py:115} INFO - [2022-12-08 16:48:45,554] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-08 16:48:45,569] {logging_mixin.py:115} INFO - [2022-12-08 16:48:45,569] {dag.py:2919} INFO - Setting next_dagrun for move_data_from_s3_to_redshidt_with_awswrangler to 2022-12-08T00:00:00+00:00, run_after=2022-12-09T00:00:00+00:00
[2022-12-08 16:48:45,576] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 0.548 seconds
[2022-12-08 16:49:15,793] {processor.py:153} INFO - Started process (PID=4692) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:49:15,794] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:49:15,794] {logging_mixin.py:115} INFO - [2022-12-08 16:49:15,794] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:49:16,298] {processor.py:651} INFO - DAG(s) dict_keys(['move_data_from_s3_to_redshidt_with_awswrangler']) retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:49:16,306] {logging_mixin.py:115} INFO - [2022-12-08 16:49:16,305] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-08 16:49:16,321] {logging_mixin.py:115} INFO - [2022-12-08 16:49:16,321] {dag.py:2919} INFO - Setting next_dagrun for move_data_from_s3_to_redshidt_with_awswrangler to 2022-12-08T00:00:00+00:00, run_after=2022-12-09T00:00:00+00:00
[2022-12-08 16:49:16,328] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 0.537 seconds
[2022-12-08 16:49:47,324] {processor.py:153} INFO - Started process (PID=4772) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:49:47,325] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:49:47,325] {logging_mixin.py:115} INFO - [2022-12-08 16:49:47,325] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:49:47,860] {processor.py:651} INFO - DAG(s) dict_keys(['move_data_from_s3_to_redshidt_with_awswrangler']) retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:49:47,868] {logging_mixin.py:115} INFO - [2022-12-08 16:49:47,868] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-08 16:49:47,884] {logging_mixin.py:115} INFO - [2022-12-08 16:49:47,884] {dag.py:2919} INFO - Setting next_dagrun for move_data_from_s3_to_redshidt_with_awswrangler to 2022-12-08T00:00:00+00:00, run_after=2022-12-09T00:00:00+00:00
[2022-12-08 16:49:47,890] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 0.568 seconds
[2022-12-08 16:50:17,919] {processor.py:153} INFO - Started process (PID=4839) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:50:17,919] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:50:17,920] {logging_mixin.py:115} INFO - [2022-12-08 16:50:17,920] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:50:18,415] {processor.py:651} INFO - DAG(s) dict_keys(['move_data_from_s3_to_redshidt_with_awswrangler']) retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:50:18,424] {logging_mixin.py:115} INFO - [2022-12-08 16:50:18,423] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-08 16:50:18,439] {logging_mixin.py:115} INFO - [2022-12-08 16:50:18,439] {dag.py:2919} INFO - Setting next_dagrun for move_data_from_s3_to_redshidt_with_awswrangler to 2022-12-08T00:00:00+00:00, run_after=2022-12-09T00:00:00+00:00
[2022-12-08 16:50:18,445] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 0.529 seconds
[2022-12-08 16:50:48,738] {processor.py:153} INFO - Started process (PID=4915) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:50:48,738] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:50:48,739] {logging_mixin.py:115} INFO - [2022-12-08 16:50:48,738] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:50:49,232] {processor.py:651} INFO - DAG(s) dict_keys(['move_data_from_s3_to_redshidt_with_awswrangler']) retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:50:49,240] {logging_mixin.py:115} INFO - [2022-12-08 16:50:49,239] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-08 16:50:49,255] {logging_mixin.py:115} INFO - [2022-12-08 16:50:49,255] {dag.py:2919} INFO - Setting next_dagrun for move_data_from_s3_to_redshidt_with_awswrangler to 2022-12-08T00:00:00+00:00, run_after=2022-12-09T00:00:00+00:00
[2022-12-08 16:50:49,261] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 0.526 seconds
[2022-12-08 16:51:19,543] {processor.py:153} INFO - Started process (PID=4991) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:51:19,543] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:51:19,543] {logging_mixin.py:115} INFO - [2022-12-08 16:51:19,543] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:51:20,052] {processor.py:651} INFO - DAG(s) dict_keys(['move_data_from_s3_to_redshidt_with_awswrangler']) retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:51:20,060] {logging_mixin.py:115} INFO - [2022-12-08 16:51:20,059] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-08 16:51:20,075] {logging_mixin.py:115} INFO - [2022-12-08 16:51:20,075] {dag.py:2919} INFO - Setting next_dagrun for move_data_from_s3_to_redshidt_with_awswrangler to 2022-12-08T00:00:00+00:00, run_after=2022-12-09T00:00:00+00:00
[2022-12-08 16:51:20,082] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 0.542 seconds
[2022-12-08 16:51:50,293] {processor.py:153} INFO - Started process (PID=5059) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:51:50,294] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:51:50,294] {logging_mixin.py:115} INFO - [2022-12-08 16:51:50,294] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:51:50,797] {processor.py:651} INFO - DAG(s) dict_keys(['move_data_from_s3_to_redshidt_with_awswrangler']) retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:51:50,805] {logging_mixin.py:115} INFO - [2022-12-08 16:51:50,805] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-08 16:51:50,820] {logging_mixin.py:115} INFO - [2022-12-08 16:51:50,820] {dag.py:2919} INFO - Setting next_dagrun for move_data_from_s3_to_redshidt_with_awswrangler to 2022-12-08T00:00:00+00:00, run_after=2022-12-09T00:00:00+00:00
[2022-12-08 16:51:50,827] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 0.536 seconds
[2022-12-08 16:52:21,162] {processor.py:153} INFO - Started process (PID=5135) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:52:21,162] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:52:21,162] {logging_mixin.py:115} INFO - [2022-12-08 16:52:21,162] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:52:21,669] {processor.py:651} INFO - DAG(s) dict_keys(['move_data_from_s3_to_redshidt_with_awswrangler']) retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:52:21,677] {logging_mixin.py:115} INFO - [2022-12-08 16:52:21,676] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-08 16:52:21,692] {logging_mixin.py:115} INFO - [2022-12-08 16:52:21,692] {dag.py:2919} INFO - Setting next_dagrun for move_data_from_s3_to_redshidt_with_awswrangler to 2022-12-08T00:00:00+00:00, run_after=2022-12-09T00:00:00+00:00
[2022-12-08 16:52:21,699] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 0.540 seconds
[2022-12-08 16:52:51,932] {processor.py:153} INFO - Started process (PID=5211) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:52:51,932] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:52:51,933] {logging_mixin.py:115} INFO - [2022-12-08 16:52:51,933] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:52:52,445] {processor.py:651} INFO - DAG(s) dict_keys(['move_data_from_s3_to_redshidt_with_awswrangler']) retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:52:52,453] {logging_mixin.py:115} INFO - [2022-12-08 16:52:52,452] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-08 16:52:52,468] {logging_mixin.py:115} INFO - [2022-12-08 16:52:52,468] {dag.py:2919} INFO - Setting next_dagrun for move_data_from_s3_to_redshidt_with_awswrangler to 2022-12-08T00:00:00+00:00, run_after=2022-12-09T00:00:00+00:00
[2022-12-08 16:52:52,475] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 0.545 seconds
[2022-12-08 16:53:22,833] {processor.py:153} INFO - Started process (PID=5288) to work on /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:53:22,833] {processor.py:641} INFO - Processing file /opt/airflow/dags/move_data_s3redshift.py for tasks to queue
[2022-12-08 16:53:22,833] {logging_mixin.py:115} INFO - [2022-12-08 16:53:22,833] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:53:23,344] {processor.py:651} INFO - DAG(s) dict_keys(['move_data_from_s3_to_redshidt_with_awswrangler']) retrieved from /opt/airflow/dags/move_data_s3redshift.py
[2022-12-08 16:53:23,352] {logging_mixin.py:115} INFO - [2022-12-08 16:53:23,351] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-08 16:53:23,367] {logging_mixin.py:115} INFO - [2022-12-08 16:53:23,367] {dag.py:2919} INFO - Setting next_dagrun for move_data_from_s3_to_redshidt_with_awswrangler to 2022-12-08T00:00:00+00:00, run_after=2022-12-09T00:00:00+00:00
[2022-12-08 16:53:23,374] {processor.py:161} INFO - Processing /opt/airflow/dags/move_data_s3redshift.py took 0.543 seconds
