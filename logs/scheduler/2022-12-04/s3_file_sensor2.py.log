[2022-12-04 18:33:45,168] {processor.py:163} INFO - Started process (PID=30) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:33:45,169] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:33:45,169] {logging_mixin.py:109} INFO - [2022-12-04 18:33:45,169] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:33:45,514] {logging_mixin.py:109} INFO - [2022-12-04 18:33:45,513] {dagbag.py:334} ERROR - Failed to import: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 331, in _load_modules_from_file
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/s3_file_sensor2.py", line 8, in <module>
    BUCKET_NAME = Variable.get("redshift_bucket")
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/variable.py", line 140, in get
    raise KeyError(f'Variable {key} does not exist')
KeyError: 'Variable redshift_bucket does not exist'
[2022-12-04 18:33:45,514] {processor.py:654} WARNING - No viable dags retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:33:45,519] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.353 seconds
[2022-12-04 18:34:16,227] {processor.py:163} INFO - Started process (PID=572) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:34:16,228] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:34:16,228] {logging_mixin.py:109} INFO - [2022-12-04 18:34:16,228] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:34:16,388] {logging_mixin.py:109} INFO - [2022-12-04 18:34:16,388] {dagbag.py:334} ERROR - Failed to import: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 331, in _load_modules_from_file
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/s3_file_sensor2.py", line 8, in <module>
    BUCKET_NAME = Variable.get("redshift_bucket")
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/variable.py", line 140, in get
    raise KeyError(f'Variable {key} does not exist')
KeyError: 'Variable redshift_bucket does not exist'
[2022-12-04 18:34:16,389] {processor.py:654} WARNING - No viable dags retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:34:16,394] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.170 seconds
[2022-12-04 18:34:46,600] {processor.py:163} INFO - Started process (PID=626) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:34:46,601] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:34:46,601] {logging_mixin.py:109} INFO - [2022-12-04 18:34:46,601] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:34:46,764] {logging_mixin.py:109} INFO - [2022-12-04 18:34:46,764] {dagbag.py:334} ERROR - Failed to import: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 331, in _load_modules_from_file
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/s3_file_sensor2.py", line 8, in <module>
    BUCKET_NAME = Variable.get("redshift_bucket")
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/variable.py", line 140, in get
    raise KeyError(f'Variable {key} does not exist')
KeyError: 'Variable redshift_bucket does not exist'
[2022-12-04 18:34:46,765] {processor.py:654} WARNING - No viable dags retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:34:46,770] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.172 seconds
[2022-12-04 18:35:17,746] {processor.py:163} INFO - Started process (PID=685) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:35:17,747] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:35:17,747] {logging_mixin.py:109} INFO - [2022-12-04 18:35:17,747] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:35:17,905] {logging_mixin.py:109} INFO - [2022-12-04 18:35:17,905] {dagbag.py:334} ERROR - Failed to import: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 331, in _load_modules_from_file
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/s3_file_sensor2.py", line 8, in <module>
    BUCKET_NAME = Variable.get("redshift_bucket")
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/variable.py", line 140, in get
    raise KeyError(f'Variable {key} does not exist')
KeyError: 'Variable redshift_bucket does not exist'
[2022-12-04 18:35:17,906] {processor.py:654} WARNING - No viable dags retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:35:17,911] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.167 seconds
[2022-12-04 18:35:48,061] {processor.py:163} INFO - Started process (PID=746) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:35:48,063] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:35:48,063] {logging_mixin.py:109} INFO - [2022-12-04 18:35:48,063] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:35:48,229] {logging_mixin.py:109} INFO - [2022-12-04 18:35:48,229] {dagbag.py:334} ERROR - Failed to import: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 331, in _load_modules_from_file
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/s3_file_sensor2.py", line 8, in <module>
    BUCKET_NAME = Variable.get("redshift_bucket")
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/variable.py", line 140, in get
    raise KeyError(f'Variable {key} does not exist')
KeyError: 'Variable redshift_bucket does not exist'
[2022-12-04 18:35:48,230] {processor.py:654} WARNING - No viable dags retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:35:48,235] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.176 seconds
[2022-12-04 18:36:18,405] {processor.py:163} INFO - Started process (PID=798) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:36:18,406] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:36:18,406] {logging_mixin.py:109} INFO - [2022-12-04 18:36:18,406] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:36:18,565] {logging_mixin.py:109} INFO - [2022-12-04 18:36:18,564] {dagbag.py:334} ERROR - Failed to import: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 331, in _load_modules_from_file
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/s3_file_sensor2.py", line 8, in <module>
    BUCKET_NAME = Variable.get("redshift_bucket")
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/variable.py", line 140, in get
    raise KeyError(f'Variable {key} does not exist')
KeyError: 'Variable redshift_bucket does not exist'
[2022-12-04 18:36:18,565] {processor.py:654} WARNING - No viable dags retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:36:18,570] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.167 seconds
[2022-12-04 18:36:48,837] {processor.py:163} INFO - Started process (PID=859) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:36:48,838] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:36:48,838] {logging_mixin.py:109} INFO - [2022-12-04 18:36:48,838] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:36:49,000] {logging_mixin.py:109} INFO - [2022-12-04 18:36:48,999] {dagbag.py:334} ERROR - Failed to import: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 331, in _load_modules_from_file
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/s3_file_sensor2.py", line 8, in <module>
    BUCKET_NAME = Variable.get("redshift_bucket")
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/variable.py", line 140, in get
    raise KeyError(f'Variable {key} does not exist')
KeyError: 'Variable redshift_bucket does not exist'
[2022-12-04 18:36:49,000] {processor.py:654} WARNING - No viable dags retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:36:49,005] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.170 seconds
[2022-12-04 18:37:19,115] {processor.py:163} INFO - Started process (PID=911) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:37:19,116] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:37:19,116] {logging_mixin.py:109} INFO - [2022-12-04 18:37:19,116] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:37:19,276] {logging_mixin.py:109} INFO - [2022-12-04 18:37:19,275] {dagbag.py:334} ERROR - Failed to import: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 331, in _load_modules_from_file
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/s3_file_sensor2.py", line 8, in <module>
    BUCKET_NAME = Variable.get("redshift_bucket")
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/variable.py", line 140, in get
    raise KeyError(f'Variable {key} does not exist')
KeyError: 'Variable redshift_bucket does not exist'
[2022-12-04 18:37:19,276] {processor.py:654} WARNING - No viable dags retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:37:19,281] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.169 seconds
[2022-12-04 18:37:49,366] {processor.py:163} INFO - Started process (PID=972) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:37:49,367] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:37:49,367] {logging_mixin.py:109} INFO - [2022-12-04 18:37:49,367] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:37:49,526] {logging_mixin.py:109} INFO - [2022-12-04 18:37:49,525] {dagbag.py:334} ERROR - Failed to import: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 331, in _load_modules_from_file
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/s3_file_sensor2.py", line 8, in <module>
    BUCKET_NAME = Variable.get("redshift_bucket")
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/variable.py", line 140, in get
    raise KeyError(f'Variable {key} does not exist')
KeyError: 'Variable redshift_bucket does not exist'
[2022-12-04 18:37:49,526] {processor.py:654} WARNING - No viable dags retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:37:49,531] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.167 seconds
[2022-12-04 18:38:20,419] {processor.py:163} INFO - Started process (PID=1024) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:38:20,419] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:38:20,420] {logging_mixin.py:109} INFO - [2022-12-04 18:38:20,420] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:38:20,588] {logging_mixin.py:109} INFO - [2022-12-04 18:38:20,588] {dagbag.py:334} ERROR - Failed to import: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 331, in _load_modules_from_file
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/s3_file_sensor2.py", line 8, in <module>
    BUCKET_NAME = Variable.get("redshift_bucket")
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/variable.py", line 140, in get
    raise KeyError(f'Variable {key} does not exist')
KeyError: 'Variable redshift_bucket does not exist'
[2022-12-04 18:38:20,588] {processor.py:654} WARNING - No viable dags retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:38:20,593] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.177 seconds
[2022-12-04 18:38:50,834] {processor.py:163} INFO - Started process (PID=1085) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:38:50,835] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:38:50,835] {logging_mixin.py:109} INFO - [2022-12-04 18:38:50,835] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:38:50,920] {logging_mixin.py:109} INFO - [2022-12-04 18:38:50,920] {dagbag.py:334} ERROR - Failed to import: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 331, in _load_modules_from_file
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/s3_file_sensor2.py", line 8, in <module>
    BUCKET_NAME = Variable.get("redshift_bucket")
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/variable.py", line 140, in get
    raise KeyError(f'Variable {key} does not exist')
KeyError: 'Variable redshift_bucket does not exist'
[2022-12-04 18:38:50,920] {processor.py:654} WARNING - No viable dags retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:38:50,926] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.095 seconds
[2022-12-04 18:39:21,019] {processor.py:163} INFO - Started process (PID=1144) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:39:21,019] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:39:21,020] {logging_mixin.py:109} INFO - [2022-12-04 18:39:21,020] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:39:21,104] {logging_mixin.py:109} INFO - [2022-12-04 18:39:21,104] {dagbag.py:334} ERROR - Failed to import: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 331, in _load_modules_from_file
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/s3_file_sensor2.py", line 8, in <module>
    BUCKET_NAME = Variable.get("redshift_bucket")
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/variable.py", line 140, in get
    raise KeyError(f'Variable {key} does not exist')
KeyError: 'Variable redshift_bucket does not exist'
[2022-12-04 18:39:21,105] {processor.py:654} WARNING - No viable dags retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:39:21,110] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.094 seconds
[2022-12-04 18:39:44,084] {processor.py:163} INFO - Started process (PID=1186) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:39:44,085] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:39:44,085] {logging_mixin.py:109} INFO - [2022-12-04 18:39:44,085] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:39:44,174] {logging_mixin.py:109} INFO - [2022-12-04 18:39:44,173] {dagbag.py:334} ERROR - Failed to import: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 331, in _load_modules_from_file
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/s3_file_sensor2.py", line 8, in <module>
    BUCKET_NAME = Variable.get("redshift_bucket")
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/variable.py", line 140, in get
    raise KeyError(f'Variable {key} does not exist')
KeyError: 'Variable redshift_bucket does not exist'
[2022-12-04 18:39:44,174] {processor.py:654} WARNING - No viable dags retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:39:44,181] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.100 seconds
[2022-12-04 18:40:15,077] {processor.py:163} INFO - Started process (PID=1247) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:40:15,078] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:40:15,078] {logging_mixin.py:109} INFO - [2022-12-04 18:40:15,078] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:40:15,162] {logging_mixin.py:109} INFO - [2022-12-04 18:40:15,162] {dagbag.py:334} ERROR - Failed to import: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 331, in _load_modules_from_file
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/s3_file_sensor2.py", line 8, in <module>
    BUCKET_NAME = Variable.get("redshift_bucket")
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/variable.py", line 140, in get
    raise KeyError(f'Variable {key} does not exist')
KeyError: 'Variable redshift_bucket does not exist'
[2022-12-04 18:40:15,162] {processor.py:654} WARNING - No viable dags retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:40:15,168] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.093 seconds
[2022-12-04 18:40:45,373] {processor.py:163} INFO - Started process (PID=1308) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:40:45,374] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:40:45,374] {logging_mixin.py:109} INFO - [2022-12-04 18:40:45,374] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:40:45,462] {logging_mixin.py:109} INFO - [2022-12-04 18:40:45,461] {dagbag.py:334} ERROR - Failed to import: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 331, in _load_modules_from_file
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/s3_file_sensor2.py", line 8, in <module>
    BUCKET_NAME = Variable.get("redshift_bucket")
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/variable.py", line 140, in get
    raise KeyError(f'Variable {key} does not exist')
KeyError: 'Variable redshift_bucket does not exist'
[2022-12-04 18:40:45,462] {processor.py:654} WARNING - No viable dags retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:40:45,468] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.097 seconds
[2022-12-04 18:41:15,561] {processor.py:163} INFO - Started process (PID=1360) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:41:15,562] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:41:15,562] {logging_mixin.py:109} INFO - [2022-12-04 18:41:15,562] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:41:15,647] {logging_mixin.py:109} INFO - [2022-12-04 18:41:15,647] {dagbag.py:334} ERROR - Failed to import: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 331, in _load_modules_from_file
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/s3_file_sensor2.py", line 8, in <module>
    BUCKET_NAME = Variable.get("redshift_bucket")
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/variable.py", line 140, in get
    raise KeyError(f'Variable {key} does not exist')
KeyError: 'Variable redshift_bucket does not exist'
[2022-12-04 18:41:15,647] {processor.py:654} WARNING - No viable dags retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:41:15,653] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.095 seconds
[2022-12-04 18:41:46,490] {processor.py:163} INFO - Started process (PID=1421) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:41:46,491] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:41:46,491] {logging_mixin.py:109} INFO - [2022-12-04 18:41:46,491] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:41:46,591] {logging_mixin.py:109} INFO - [2022-12-04 18:41:46,590] {dagbag.py:334} ERROR - Failed to import: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 331, in _load_modules_from_file
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/s3_file_sensor2.py", line 8, in <module>
    BUCKET_NAME = Variable.get("redshift_bucket")
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/variable.py", line 140, in get
    raise KeyError(f'Variable {key} does not exist')
KeyError: 'Variable redshift_bucket does not exist'
[2022-12-04 18:41:46,591] {processor.py:654} WARNING - No viable dags retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:41:46,597] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.109 seconds
[2022-12-04 18:42:16,833] {processor.py:163} INFO - Started process (PID=1473) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:42:16,834] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:42:16,834] {logging_mixin.py:109} INFO - [2022-12-04 18:42:16,834] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:42:16,920] {logging_mixin.py:109} INFO - [2022-12-04 18:42:16,919] {dagbag.py:334} ERROR - Failed to import: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 331, in _load_modules_from_file
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/s3_file_sensor2.py", line 8, in <module>
    BUCKET_NAME = Variable.get("redshift_bucket")
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/variable.py", line 140, in get
    raise KeyError(f'Variable {key} does not exist')
KeyError: 'Variable redshift_bucket does not exist'
[2022-12-04 18:42:16,920] {processor.py:654} WARNING - No viable dags retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:42:16,926] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.095 seconds
[2022-12-04 18:42:47,085] {processor.py:163} INFO - Started process (PID=1534) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:42:47,086] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:42:47,087] {logging_mixin.py:109} INFO - [2022-12-04 18:42:47,087] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:42:47,172] {logging_mixin.py:109} INFO - [2022-12-04 18:42:47,172] {dagbag.py:334} ERROR - Failed to import: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 331, in _load_modules_from_file
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/s3_file_sensor2.py", line 8, in <module>
    BUCKET_NAME = Variable.get("redshift_bucket")
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/variable.py", line 140, in get
    raise KeyError(f'Variable {key} does not exist')
KeyError: 'Variable redshift_bucket does not exist'
[2022-12-04 18:42:47,172] {processor.py:654} WARNING - No viable dags retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:42:47,178] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.095 seconds
[2022-12-04 18:43:17,323] {processor.py:163} INFO - Started process (PID=1586) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:43:17,324] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:43:17,324] {logging_mixin.py:109} INFO - [2022-12-04 18:43:17,324] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:43:17,409] {logging_mixin.py:109} INFO - [2022-12-04 18:43:17,409] {dagbag.py:334} ERROR - Failed to import: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 331, in _load_modules_from_file
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/s3_file_sensor2.py", line 8, in <module>
    BUCKET_NAME = Variable.get("redshift_bucket")
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/variable.py", line 140, in get
    raise KeyError(f'Variable {key} does not exist')
KeyError: 'Variable redshift_bucket does not exist'
[2022-12-04 18:43:17,409] {processor.py:654} WARNING - No viable dags retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:43:17,415] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.094 seconds
[2022-12-04 18:44:33,141] {processor.py:163} INFO - Started process (PID=29) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:44:33,142] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:44:33,142] {logging_mixin.py:109} INFO - [2022-12-04 18:44:33,142] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:44:33,349] {logging_mixin.py:109} INFO - [2022-12-04 18:44:33,349] {dagbag.py:334} ERROR - Failed to import: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 331, in _load_modules_from_file
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/s3_file_sensor2.py", line 8, in <module>
    BUCKET_NAME = Variable.get("redshift_bucket")
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/variable.py", line 140, in get
    raise KeyError(f'Variable {key} does not exist')
KeyError: 'Variable redshift_bucket does not exist'
[2022-12-04 18:44:33,349] {processor.py:654} WARNING - No viable dags retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:44:33,354] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.216 seconds
[2022-12-04 18:45:03,542] {processor.py:163} INFO - Started process (PID=90) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:45:03,542] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:45:03,543] {logging_mixin.py:109} INFO - [2022-12-04 18:45:03,543] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:45:03,710] {logging_mixin.py:109} INFO - [2022-12-04 18:45:03,709] {dagbag.py:334} ERROR - Failed to import: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 331, in _load_modules_from_file
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/s3_file_sensor2.py", line 8, in <module>
    BUCKET_NAME = Variable.get("redshift_bucket")
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/variable.py", line 140, in get
    raise KeyError(f'Variable {key} does not exist')
KeyError: 'Variable redshift_bucket does not exist'
[2022-12-04 18:45:03,710] {processor.py:654} WARNING - No viable dags retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:45:03,715] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.176 seconds
[2022-12-04 18:45:34,219] {processor.py:163} INFO - Started process (PID=142) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:45:34,220] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:45:34,220] {logging_mixin.py:109} INFO - [2022-12-04 18:45:34,220] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:45:34,389] {logging_mixin.py:109} INFO - [2022-12-04 18:45:34,389] {dagbag.py:334} ERROR - Failed to import: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 331, in _load_modules_from_file
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/s3_file_sensor2.py", line 11, in <module>
    aws_access_key_id=Variable.get("aws_access_key_id")
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/variable.py", line 140, in get
    raise KeyError(f'Variable {key} does not exist')
KeyError: 'Variable aws_access_key_id does not exist'
[2022-12-04 18:45:34,389] {processor.py:654} WARNING - No viable dags retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:45:34,394] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.178 seconds
[2022-12-04 18:46:04,989] {processor.py:163} INFO - Started process (PID=203) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:46:04,990] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:46:04,990] {logging_mixin.py:109} INFO - [2022-12-04 18:46:04,990] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:46:05,160] {logging_mixin.py:109} INFO - [2022-12-04 18:46:05,160] {dagbag.py:334} ERROR - Failed to import: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 331, in _load_modules_from_file
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/s3_file_sensor2.py", line 11, in <module>
    aws_access_key_id=Variable.get("aws_access_key_id")
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/variable.py", line 140, in get
    raise KeyError(f'Variable {key} does not exist')
KeyError: 'Variable aws_access_key_id does not exist'
[2022-12-04 18:46:05,161] {processor.py:654} WARNING - No viable dags retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:46:05,166] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.179 seconds
[2022-12-04 18:46:35,717] {processor.py:163} INFO - Started process (PID=255) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:46:35,719] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:46:35,719] {logging_mixin.py:109} INFO - [2022-12-04 18:46:35,719] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:46:35,892] {logging_mixin.py:109} INFO - [2022-12-04 18:46:35,892] {dagbag.py:334} ERROR - Failed to import: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 331, in _load_modules_from_file
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/s3_file_sensor2.py", line 12, in <module>
    aws_secret_access_key=Variable.get("aws_access_key_value")
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/variable.py", line 140, in get
    raise KeyError(f'Variable {key} does not exist')
KeyError: 'Variable aws_access_key_value does not exist'
[2022-12-04 18:46:35,892] {processor.py:654} WARNING - No viable dags retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:46:35,897] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.182 seconds
[2022-12-04 18:47:06,508] {processor.py:163} INFO - Started process (PID=316) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:47:06,509] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:47:06,509] {logging_mixin.py:109} INFO - [2022-12-04 18:47:06,509] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:47:06,684] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:47:07,893] {logging_mixin.py:109} INFO - [2022-12-04 18:47:07,893] {manager.py:496} INFO - Created Permission View: can edit on DAG:second_test_s3_file_sensor
[2022-12-04 18:47:07,898] {logging_mixin.py:109} INFO - [2022-12-04 18:47:07,897] {manager.py:496} INFO - Created Permission View: can read on DAG:second_test_s3_file_sensor
[2022-12-04 18:47:07,898] {logging_mixin.py:109} INFO - [2022-12-04 18:47:07,898] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:47:07,903] {logging_mixin.py:109} INFO - [2022-12-04 18:47:07,902] {dag.py:2417} INFO - Creating ORM DAG for second_test_s3_file_sensor
[2022-12-04 18:47:07,908] {logging_mixin.py:109} INFO - [2022-12-04 18:47:07,908] {dag.py:2937} INFO - Setting next_dagrun for second_test_s3_file_sensor to 2022-12-03 00:00:00+00:00
[2022-12-04 18:47:07,915] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 1.410 seconds
[2022-12-04 18:47:38,739] {processor.py:163} INFO - Started process (PID=402) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:47:38,740] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:47:38,740] {logging_mixin.py:109} INFO - [2022-12-04 18:47:38,740] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:47:38,914] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:47:38,918] {logging_mixin.py:109} INFO - [2022-12-04 18:47:38,918] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:47:38,930] {logging_mixin.py:109} INFO - [2022-12-04 18:47:38,930] {dag.py:2937} INFO - Setting next_dagrun for second_test_s3_file_sensor to 2022-12-03 00:00:00+00:00
[2022-12-04 18:47:38,935] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.198 seconds
[2022-12-04 18:48:07,865] {processor.py:163} INFO - Started process (PID=445) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:48:07,866] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:48:07,866] {logging_mixin.py:109} INFO - [2022-12-04 18:48:07,866] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:48:08,038] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:48:08,056] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 48, 8, 48791, tzinfo=Timezone('UTC')), 'duration': 6}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:48:08,063] {logging_mixin.py:109} INFO - [2022-12-04 18:48:08,063] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:48:08,076] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.213 seconds
[2022-12-04 18:48:18,720] {processor.py:163} INFO - Started process (PID=488) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:48:18,721] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:48:18,722] {logging_mixin.py:109} INFO - [2022-12-04 18:48:18,722] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:48:18,901] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:48:18,917] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 48, 18, 911465, tzinfo=Timezone('UTC')), 'duration': 17}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:48:18,925] {logging_mixin.py:109} INFO - [2022-12-04 18:48:18,925] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:48:18,941] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.223 seconds
[2022-12-04 18:48:28,752] {processor.py:163} INFO - Started process (PID=498) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:48:28,753] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:48:28,753] {logging_mixin.py:109} INFO - [2022-12-04 18:48:28,753] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:48:28,921] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:48:28,937] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 48, 28, 931173, tzinfo=Timezone('UTC')), 'duration': 27}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:48:28,945] {logging_mixin.py:109} INFO - [2022-12-04 18:48:28,945] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:48:28,957] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.207 seconds
[2022-12-04 18:48:39,006] {processor.py:163} INFO - Started process (PID=509) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:48:39,006] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:48:39,007] {logging_mixin.py:109} INFO - [2022-12-04 18:48:39,007] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:48:39,181] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:48:39,199] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 48, 39, 191966, tzinfo=Timezone('UTC')), 'duration': 37}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:48:39,207] {logging_mixin.py:109} INFO - [2022-12-04 18:48:39,207] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:48:39,219] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.216 seconds
[2022-12-04 18:48:49,858] {processor.py:163} INFO - Started process (PID=551) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:48:49,858] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:48:49,859] {logging_mixin.py:109} INFO - [2022-12-04 18:48:49,859] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:48:50,026] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:48:50,042] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 48, 50, 35346, tzinfo=Timezone('UTC')), 'duration': 48}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:48:50,050] {logging_mixin.py:109} INFO - [2022-12-04 18:48:50,050] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:48:50,062] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.207 seconds
[2022-12-04 18:48:59,889] {processor.py:163} INFO - Started process (PID=561) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:48:59,890] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:48:59,891] {logging_mixin.py:109} INFO - [2022-12-04 18:48:59,891] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:49:00,062] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:49:00,079] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 49, 0, 72026, tzinfo=Timezone('UTC')), 'duration': 58}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:49:00,086] {logging_mixin.py:109} INFO - [2022-12-04 18:49:00,086] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:49:00,100] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.213 seconds
[2022-12-04 18:49:01,098] {processor.py:163} INFO - Started process (PID=562) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:49:01,098] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:49:01,099] {logging_mixin.py:109} INFO - [2022-12-04 18:49:01,099] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:49:01,276] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:49:01,400] {logging_mixin.py:109} INFO - [2022-12-04 18:49:01,400] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:49:01,416] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.321 seconds
[2022-12-04 18:49:09,962] {processor.py:163} INFO - Started process (PID=571) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:49:09,963] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:49:09,963] {logging_mixin.py:109} INFO - [2022-12-04 18:49:09,963] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:49:10,126] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:49:10,142] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 49, 10, 136050, tzinfo=Timezone('UTC')), 'duration': 68}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:49:10,147] {logging_mixin.py:109} INFO - [2022-12-04 18:49:10,147] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:49:10,160] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.199 seconds
[2022-12-04 18:49:20,398] {processor.py:163} INFO - Started process (PID=606) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:49:20,398] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:49:20,399] {logging_mixin.py:109} INFO - [2022-12-04 18:49:20,399] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:49:20,564] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:49:20,580] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 49, 20, 573953, tzinfo=Timezone('UTC')), 'duration': 79}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:49:20,585] {logging_mixin.py:109} INFO - [2022-12-04 18:49:20,585] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:49:20,597] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.202 seconds
[2022-12-04 18:49:30,430] {processor.py:163} INFO - Started process (PID=616) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:49:30,431] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:49:30,431] {logging_mixin.py:109} INFO - [2022-12-04 18:49:30,431] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:49:30,604] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:49:30,620] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 49, 30, 614126, tzinfo=Timezone('UTC')), 'duration': 89}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:49:30,625] {logging_mixin.py:109} INFO - [2022-12-04 18:49:30,625] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:49:30,638] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.210 seconds
[2022-12-04 18:49:41,021] {processor.py:163} INFO - Started process (PID=634) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:49:41,022] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:49:41,023] {logging_mixin.py:109} INFO - [2022-12-04 18:49:41,023] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:49:41,116] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:49:41,133] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 49, 41, 126186, tzinfo=Timezone('UTC')), 'duration': 99}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:49:41,141] {logging_mixin.py:109} INFO - [2022-12-04 18:49:41,141] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:49:41,154] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.135 seconds
[2022-12-04 18:49:51,156] {processor.py:163} INFO - Started process (PID=669) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:49:51,156] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:49:51,157] {logging_mixin.py:109} INFO - [2022-12-04 18:49:51,157] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:49:51,251] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:49:51,269] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 49, 51, 261148, tzinfo=Timezone('UTC')), 'duration': 110}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:49:51,277] {logging_mixin.py:109} INFO - [2022-12-04 18:49:51,277] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:49:51,290] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.137 seconds
[2022-12-04 18:50:01,194] {processor.py:163} INFO - Started process (PID=679) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:50:01,195] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:50:01,196] {logging_mixin.py:109} INFO - [2022-12-04 18:50:01,195] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:50:01,289] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:50:01,306] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 50, 1, 299747, tzinfo=Timezone('UTC')), 'duration': 120}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:50:01,315] {logging_mixin.py:109} INFO - [2022-12-04 18:50:01,315] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:50:01,328] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.136 seconds
[2022-12-04 18:50:11,581] {processor.py:163} INFO - Started process (PID=697) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:50:11,582] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:50:11,582] {logging_mixin.py:109} INFO - [2022-12-04 18:50:11,582] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:50:11,677] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:50:11,696] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 50, 11, 687679, tzinfo=Timezone('UTC')), 'duration': 130}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:50:11,704] {logging_mixin.py:109} INFO - [2022-12-04 18:50:11,704] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:50:11,718] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.139 seconds
[2022-12-04 18:50:21,809] {processor.py:163} INFO - Started process (PID=723) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:50:21,810] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:50:21,810] {logging_mixin.py:109} INFO - [2022-12-04 18:50:21,810] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:50:21,903] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:50:21,921] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 50, 21, 913081, tzinfo=Timezone('UTC')), 'duration': 140}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:50:21,929] {logging_mixin.py:109} INFO - [2022-12-04 18:50:21,929] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:50:21,942] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.135 seconds
[2022-12-04 18:50:25,824] {processor.py:163} INFO - Started process (PID=733) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:50:25,825] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:50:25,825] {logging_mixin.py:109} INFO - [2022-12-04 18:50:25,825] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:50:25,919] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:50:25,924] {logging_mixin.py:109} INFO - [2022-12-04 18:50:25,924] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:50:25,941] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.119 seconds
[2022-12-04 18:50:31,848] {processor.py:163} INFO - Started process (PID=734) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:50:31,849] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:50:31,849] {logging_mixin.py:109} INFO - [2022-12-04 18:50:31,849] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:50:31,943] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:50:31,961] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 50, 31, 953120, tzinfo=Timezone('UTC')), 'duration': 150}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:50:31,969] {logging_mixin.py:109} INFO - [2022-12-04 18:50:31,969] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:50:31,984] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.139 seconds
[2022-12-04 18:50:42,241] {processor.py:163} INFO - Started process (PID=752) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:50:42,242] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:50:42,242] {logging_mixin.py:109} INFO - [2022-12-04 18:50:42,242] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:50:42,335] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:50:42,353] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 50, 42, 345860, tzinfo=Timezone('UTC')), 'duration': 161}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:50:42,361] {logging_mixin.py:109} INFO - [2022-12-04 18:50:42,361] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:50:42,375] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.136 seconds
[2022-12-04 18:50:52,415] {processor.py:163} INFO - Started process (PID=787) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:50:52,415] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:50:52,416] {logging_mixin.py:109} INFO - [2022-12-04 18:50:52,416] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:50:52,507] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:50:52,525] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 50, 52, 517749, tzinfo=Timezone('UTC')), 'duration': 171}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:50:52,533] {logging_mixin.py:109} INFO - [2022-12-04 18:50:52,533] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:50:52,546] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.134 seconds
[2022-12-04 18:51:02,453] {processor.py:163} INFO - Started process (PID=797) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:51:02,453] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:51:02,454] {logging_mixin.py:109} INFO - [2022-12-04 18:51:02,454] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:51:02,546] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:51:02,562] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 51, 2, 555975, tzinfo=Timezone('UTC')), 'duration': 181}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:51:02,570] {logging_mixin.py:109} INFO - [2022-12-04 18:51:02,570] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:51:02,584] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.133 seconds
[2022-12-04 18:51:12,843] {processor.py:163} INFO - Started process (PID=815) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:51:12,844] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:51:12,844] {logging_mixin.py:109} INFO - [2022-12-04 18:51:12,844] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:51:12,937] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:51:12,954] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 51, 12, 947090, tzinfo=Timezone('UTC')), 'duration': 191}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:51:12,962] {logging_mixin.py:109} INFO - [2022-12-04 18:51:12,962] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:51:12,975] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.134 seconds
[2022-12-04 18:51:23,024] {processor.py:163} INFO - Started process (PID=850) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:51:23,025] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:51:23,025] {logging_mixin.py:109} INFO - [2022-12-04 18:51:23,025] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:51:23,117] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:51:23,136] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 51, 23, 127999, tzinfo=Timezone('UTC')), 'duration': 201}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:51:23,144] {logging_mixin.py:109} INFO - [2022-12-04 18:51:23,144] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:51:23,157] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.135 seconds
[2022-12-04 18:51:33,060] {processor.py:163} INFO - Started process (PID=851) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:51:33,061] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:51:33,061] {logging_mixin.py:109} INFO - [2022-12-04 18:51:33,061] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:51:33,152] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:51:33,170] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 51, 33, 162764, tzinfo=Timezone('UTC')), 'duration': 211}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:51:33,178] {logging_mixin.py:109} INFO - [2022-12-04 18:51:33,178] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:51:33,192] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.134 seconds
[2022-12-04 18:51:43,444] {processor.py:163} INFO - Started process (PID=869) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:51:43,445] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:51:43,445] {logging_mixin.py:109} INFO - [2022-12-04 18:51:43,445] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:51:43,538] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:51:43,556] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 51, 43, 548120, tzinfo=Timezone('UTC')), 'duration': 222}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:51:43,564] {logging_mixin.py:109} INFO - [2022-12-04 18:51:43,564] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:51:43,577] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.136 seconds
[2022-12-04 18:51:53,576] {processor.py:163} INFO - Started process (PID=904) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:51:53,577] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:51:53,577] {logging_mixin.py:109} INFO - [2022-12-04 18:51:53,577] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:51:53,670] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:51:53,687] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 51, 53, 680265, tzinfo=Timezone('UTC')), 'duration': 232}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:51:53,695] {logging_mixin.py:109} INFO - [2022-12-04 18:51:53,695] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:51:53,708] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.134 seconds
[2022-12-04 18:52:03,615] {processor.py:163} INFO - Started process (PID=914) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:52:03,616] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:52:03,616] {logging_mixin.py:109} INFO - [2022-12-04 18:52:03,616] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:52:03,709] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:52:03,727] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 52, 3, 719000, tzinfo=Timezone('UTC')), 'duration': 242}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:52:03,735] {logging_mixin.py:109} INFO - [2022-12-04 18:52:03,735] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:52:03,749] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.136 seconds
[2022-12-04 18:52:13,999] {processor.py:163} INFO - Started process (PID=932) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:52:14,000] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:52:14,000] {logging_mixin.py:109} INFO - [2022-12-04 18:52:14,000] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:52:14,092] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:52:14,110] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 52, 14, 102358, tzinfo=Timezone('UTC')), 'duration': 252}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:52:14,118] {logging_mixin.py:109} INFO - [2022-12-04 18:52:14,118] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:52:14,132] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.135 seconds
[2022-12-04 18:52:24,162] {processor.py:163} INFO - Started process (PID=967) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:52:24,163] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:52:24,163] {logging_mixin.py:109} INFO - [2022-12-04 18:52:24,163] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:52:24,255] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:52:24,271] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 52, 24, 264984, tzinfo=Timezone('UTC')), 'duration': 263}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:52:24,279] {logging_mixin.py:109} INFO - [2022-12-04 18:52:24,279] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:52:24,293] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.133 seconds
[2022-12-04 18:52:34,199] {processor.py:163} INFO - Started process (PID=977) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:52:34,199] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:52:34,200] {logging_mixin.py:109} INFO - [2022-12-04 18:52:34,200] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:52:34,292] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:52:34,310] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 52, 34, 302404, tzinfo=Timezone('UTC')), 'duration': 273}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:52:34,318] {logging_mixin.py:109} INFO - [2022-12-04 18:52:34,318] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:52:34,332] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.135 seconds
[2022-12-04 18:52:44,578] {processor.py:163} INFO - Started process (PID=986) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:52:44,579] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:52:44,579] {logging_mixin.py:109} INFO - [2022-12-04 18:52:44,579] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:52:44,671] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:52:44,689] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 52, 44, 681838, tzinfo=Timezone('UTC')), 'duration': 283}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:52:44,697] {logging_mixin.py:109} INFO - [2022-12-04 18:52:44,697] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:52:44,711] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.135 seconds
[2022-12-04 18:52:54,744] {processor.py:163} INFO - Started process (PID=1021) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:52:54,744] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:52:54,745] {logging_mixin.py:109} INFO - [2022-12-04 18:52:54,745] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:52:54,836] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:52:54,853] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 52, 54, 846697, tzinfo=Timezone('UTC')), 'duration': 293}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:52:54,861] {logging_mixin.py:109} INFO - [2022-12-04 18:52:54,861] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:52:54,875] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.133 seconds
[2022-12-04 18:53:04,782] {processor.py:163} INFO - Started process (PID=1031) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:53:04,783] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:53:04,784] {logging_mixin.py:109} INFO - [2022-12-04 18:53:04,784] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:53:04,875] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:53:04,893] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 53, 4, 885469, tzinfo=Timezone('UTC')), 'duration': 303}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:53:04,901] {logging_mixin.py:109} INFO - [2022-12-04 18:53:04,901] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:53:04,915] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.134 seconds
[2022-12-04 18:53:15,115] {processor.py:163} INFO - Started process (PID=1049) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:53:15,116] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:53:15,116] {logging_mixin.py:109} INFO - [2022-12-04 18:53:15,116] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:53:15,208] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:53:15,224] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 53, 15, 218136, tzinfo=Timezone('UTC')), 'duration': 314}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:53:15,232] {logging_mixin.py:109} INFO - [2022-12-04 18:53:15,232] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:53:15,246] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.133 seconds
[2022-12-04 18:53:25,246] {processor.py:163} INFO - Started process (PID=1084) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:53:25,247] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:53:25,247] {logging_mixin.py:109} INFO - [2022-12-04 18:53:25,247] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:53:25,339] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:53:25,357] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 53, 25, 349371, tzinfo=Timezone('UTC')), 'duration': 324}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:53:25,365] {logging_mixin.py:109} INFO - [2022-12-04 18:53:25,365] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:53:25,379] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.135 seconds
[2022-12-04 18:53:35,285] {processor.py:163} INFO - Started process (PID=1094) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:53:35,285] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:53:35,286] {logging_mixin.py:109} INFO - [2022-12-04 18:53:35,286] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:53:35,378] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:53:35,396] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 53, 35, 388463, tzinfo=Timezone('UTC')), 'duration': 334}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:53:35,404] {logging_mixin.py:109} INFO - [2022-12-04 18:53:35,404] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:53:35,417] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.135 seconds
[2022-12-04 18:53:45,664] {processor.py:163} INFO - Started process (PID=1112) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:53:45,665] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:53:45,666] {logging_mixin.py:109} INFO - [2022-12-04 18:53:45,666] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:53:45,759] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:53:45,777] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 53, 45, 769706, tzinfo=Timezone('UTC')), 'duration': 344}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:53:45,786] {logging_mixin.py:109} INFO - [2022-12-04 18:53:45,786] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:53:45,800] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.137 seconds
[2022-12-04 18:53:55,840] {processor.py:163} INFO - Started process (PID=1138) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:53:55,840] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:53:55,840] {logging_mixin.py:109} INFO - [2022-12-04 18:53:55,840] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:53:55,933] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:53:55,952] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 53, 55, 943871, tzinfo=Timezone('UTC')), 'duration': 354}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:53:55,960] {logging_mixin.py:109} INFO - [2022-12-04 18:53:55,960] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:53:55,975] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.137 seconds
[2022-12-04 18:54:05,875] {processor.py:163} INFO - Started process (PID=1148) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:54:05,875] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:54:05,876] {logging_mixin.py:109} INFO - [2022-12-04 18:54:05,875] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:54:05,967] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:54:05,984] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 54, 5, 977213, tzinfo=Timezone('UTC')), 'duration': 364}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:54:05,992] {logging_mixin.py:109} INFO - [2022-12-04 18:54:05,992] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:54:06,005] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.133 seconds
[2022-12-04 18:54:16,257] {processor.py:163} INFO - Started process (PID=1166) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:54:16,258] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:54:16,258] {logging_mixin.py:109} INFO - [2022-12-04 18:54:16,258] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:54:16,351] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:54:16,368] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 54, 16, 361479, tzinfo=Timezone('UTC')), 'duration': 375}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:54:16,376] {logging_mixin.py:109} INFO - [2022-12-04 18:54:16,376] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:54:16,390] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.135 seconds
[2022-12-04 18:54:26,293] {processor.py:163} INFO - Started process (PID=1201) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:54:26,294] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:54:26,295] {logging_mixin.py:109} INFO - [2022-12-04 18:54:26,295] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:54:26,386] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:54:26,404] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 54, 26, 396526, tzinfo=Timezone('UTC')), 'duration': 385}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:54:26,412] {logging_mixin.py:109} INFO - [2022-12-04 18:54:26,412] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:54:26,426] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.135 seconds
[2022-12-04 18:54:36,423] {processor.py:163} INFO - Started process (PID=1211) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:54:36,423] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:54:36,424] {logging_mixin.py:109} INFO - [2022-12-04 18:54:36,424] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:54:36,515] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:54:36,532] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 54, 36, 524855, tzinfo=Timezone('UTC')), 'duration': 395}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:54:36,540] {logging_mixin.py:109} INFO - [2022-12-04 18:54:36,540] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:54:36,554] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.133 seconds
[2022-12-04 18:54:46,711] {processor.py:163} INFO - Started process (PID=1229) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:54:46,711] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:54:46,712] {logging_mixin.py:109} INFO - [2022-12-04 18:54:46,712] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:54:46,803] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:54:46,819] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 54, 46, 812802, tzinfo=Timezone('UTC')), 'duration': 405}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:54:46,827] {logging_mixin.py:109} INFO - [2022-12-04 18:54:46,827] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:54:46,841] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.133 seconds
[2022-12-04 18:54:56,837] {processor.py:163} INFO - Started process (PID=1262) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:54:56,838] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:54:56,838] {logging_mixin.py:109} INFO - [2022-12-04 18:54:56,838] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:54:56,930] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:54:56,948] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 54, 56, 940628, tzinfo=Timezone('UTC')), 'duration': 415}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:54:56,956] {logging_mixin.py:109} INFO - [2022-12-04 18:54:56,956] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:54:56,970] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.134 seconds
[2022-12-04 18:55:06,874] {processor.py:163} INFO - Started process (PID=1265) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:55:06,874] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:55:06,875] {logging_mixin.py:109} INFO - [2022-12-04 18:55:06,875] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:55:06,967] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:55:06,985] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 55, 6, 976926, tzinfo=Timezone('UTC')), 'duration': 425}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:55:06,992] {logging_mixin.py:109} INFO - [2022-12-04 18:55:06,992] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:55:07,006] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.135 seconds
[2022-12-04 18:55:17,256] {processor.py:163} INFO - Started process (PID=1283) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:55:17,256] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:55:17,257] {logging_mixin.py:109} INFO - [2022-12-04 18:55:17,257] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:55:17,349] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:55:17,367] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 55, 17, 359723, tzinfo=Timezone('UTC')), 'duration': 436}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:55:17,375] {logging_mixin.py:109} INFO - [2022-12-04 18:55:17,375] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:55:17,390] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.136 seconds
[2022-12-04 18:55:27,421] {processor.py:163} INFO - Started process (PID=1318) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:55:27,421] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:55:27,422] {logging_mixin.py:109} INFO - [2022-12-04 18:55:27,422] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:55:27,512] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:55:27,530] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 55, 27, 522566, tzinfo=Timezone('UTC')), 'duration': 446}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:55:27,538] {logging_mixin.py:109} INFO - [2022-12-04 18:55:27,538] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:55:27,551] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.133 seconds
[2022-12-04 18:55:37,461] {processor.py:163} INFO - Started process (PID=1328) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:55:37,462] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:55:37,462] {logging_mixin.py:109} INFO - [2022-12-04 18:55:37,462] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:55:37,553] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:55:37,570] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 55, 37, 563406, tzinfo=Timezone('UTC')), 'duration': 456}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:55:37,578] {logging_mixin.py:109} INFO - [2022-12-04 18:55:37,578] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:55:37,591] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.133 seconds
[2022-12-04 18:55:47,839] {processor.py:163} INFO - Started process (PID=1346) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:55:47,839] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:55:47,840] {logging_mixin.py:109} INFO - [2022-12-04 18:55:47,839] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:55:47,932] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:55:47,949] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 55, 47, 941986, tzinfo=Timezone('UTC')), 'duration': 466}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:55:47,957] {logging_mixin.py:109} INFO - [2022-12-04 18:55:47,957] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:55:47,970] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.134 seconds
[2022-12-04 18:55:58,013] {processor.py:163} INFO - Started process (PID=1381) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:55:58,013] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:55:58,013] {logging_mixin.py:109} INFO - [2022-12-04 18:55:58,013] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:55:58,108] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:55:58,126] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 55, 58, 117950, tzinfo=Timezone('UTC')), 'duration': 476}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:55:58,134] {logging_mixin.py:109} INFO - [2022-12-04 18:55:58,133] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:55:58,149] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.139 seconds
[2022-12-04 18:56:08,047] {processor.py:163} INFO - Started process (PID=1382) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:56:08,048] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:56:08,049] {logging_mixin.py:109} INFO - [2022-12-04 18:56:08,049] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:56:08,141] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:56:08,158] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 56, 8, 151375, tzinfo=Timezone('UTC')), 'duration': 486}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:56:08,166] {logging_mixin.py:109} INFO - [2022-12-04 18:56:08,166] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:56:08,179] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.134 seconds
[2022-12-04 18:56:18,433] {processor.py:163} INFO - Started process (PID=1400) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:56:18,434] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:56:18,434] {logging_mixin.py:109} INFO - [2022-12-04 18:56:18,434] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:56:18,527] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:56:18,545] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 56, 18, 537453, tzinfo=Timezone('UTC')), 'duration': 497}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:56:18,553] {logging_mixin.py:109} INFO - [2022-12-04 18:56:18,553] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:56:18,567] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.135 seconds
[2022-12-04 18:56:28,625] {processor.py:163} INFO - Started process (PID=1435) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:56:28,625] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:56:28,626] {logging_mixin.py:109} INFO - [2022-12-04 18:56:28,626] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:56:28,717] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:56:28,734] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 56, 28, 726969, tzinfo=Timezone('UTC')), 'duration': 507}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:56:28,742] {logging_mixin.py:109} INFO - [2022-12-04 18:56:28,742] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:56:28,756] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.133 seconds
[2022-12-04 18:56:38,663] {processor.py:163} INFO - Started process (PID=1445) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:56:38,664] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:56:38,665] {logging_mixin.py:109} INFO - [2022-12-04 18:56:38,665] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:56:38,756] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:56:38,774] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 56, 38, 766274, tzinfo=Timezone('UTC')), 'duration': 517}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:56:38,782] {logging_mixin.py:109} INFO - [2022-12-04 18:56:38,782] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:56:38,795] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.134 seconds
[2022-12-04 18:56:49,047] {processor.py:163} INFO - Started process (PID=1463) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:56:49,048] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:56:49,048] {logging_mixin.py:109} INFO - [2022-12-04 18:56:49,048] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:56:49,140] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:56:49,158] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 56, 49, 149971, tzinfo=Timezone('UTC')), 'duration': 527}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:56:49,166] {logging_mixin.py:109} INFO - [2022-12-04 18:56:49,166] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:56:49,181] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.136 seconds
[2022-12-04 18:56:59,228] {processor.py:163} INFO - Started process (PID=1498) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:56:59,229] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:56:59,229] {logging_mixin.py:109} INFO - [2022-12-04 18:56:59,229] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:56:59,321] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:56:59,338] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 56, 59, 330879, tzinfo=Timezone('UTC')), 'duration': 538}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:56:59,346] {logging_mixin.py:109} INFO - [2022-12-04 18:56:59,346] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:56:59,361] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.135 seconds
[2022-12-04 18:57:09,267] {processor.py:163} INFO - Started process (PID=1508) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:57:09,268] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:57:09,268] {logging_mixin.py:109} INFO - [2022-12-04 18:57:09,268] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:57:09,361] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:57:09,380] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 57, 9, 372032, tzinfo=Timezone('UTC')), 'duration': 548}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:57:09,388] {logging_mixin.py:109} INFO - [2022-12-04 18:57:09,388] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:57:09,405] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.140 seconds
[2022-12-04 18:57:19,654] {processor.py:163} INFO - Started process (PID=1523) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:57:19,654] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:57:19,655] {logging_mixin.py:109} INFO - [2022-12-04 18:57:19,655] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:57:19,748] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:57:19,765] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 57, 19, 758344, tzinfo=Timezone('UTC')), 'duration': 558}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:57:19,773] {logging_mixin.py:109} INFO - [2022-12-04 18:57:19,773] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:57:19,789] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.138 seconds
[2022-12-04 18:57:29,748] {processor.py:163} INFO - Started process (PID=1552) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:57:29,749] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:57:29,749] {logging_mixin.py:109} INFO - [2022-12-04 18:57:29,749] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:57:29,840] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:57:29,857] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 57, 29, 849872, tzinfo=Timezone('UTC')), 'duration': 568}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:57:29,865] {logging_mixin.py:109} INFO - [2022-12-04 18:57:29,865] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:57:29,881] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.135 seconds
[2022-12-04 18:57:39,856] {processor.py:163} INFO - Started process (PID=1562) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:57:39,857] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:57:39,858] {logging_mixin.py:109} INFO - [2022-12-04 18:57:39,858] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:57:39,949] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:57:39,967] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 57, 39, 959316, tzinfo=Timezone('UTC')), 'duration': 578}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:57:39,975] {logging_mixin.py:109} INFO - [2022-12-04 18:57:39,975] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:57:39,990] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.136 seconds
[2022-12-04 18:57:50,164] {processor.py:163} INFO - Started process (PID=1580) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:57:50,165] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:57:50,165] {logging_mixin.py:109} INFO - [2022-12-04 18:57:50,165] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:57:50,257] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:57:50,275] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 57, 50, 267190, tzinfo=Timezone('UTC')), 'duration': 589}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:57:50,283] {logging_mixin.py:109} INFO - [2022-12-04 18:57:50,283] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:57:50,298] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.136 seconds
[2022-12-04 18:58:00,293] {processor.py:163} INFO - Started process (PID=1615) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:58:00,294] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:58:00,294] {logging_mixin.py:109} INFO - [2022-12-04 18:58:00,294] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:58:00,387] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:58:00,405] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 58, 0, 397561, tzinfo=Timezone('UTC')), 'duration': 599}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:58:00,413] {logging_mixin.py:109} INFO - [2022-12-04 18:58:00,413] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:58:00,428] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.137 seconds
[2022-12-04 18:58:10,332] {processor.py:163} INFO - Started process (PID=1625) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:58:10,333] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:58:10,333] {logging_mixin.py:109} INFO - [2022-12-04 18:58:10,333] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:58:10,425] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:58:10,442] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 58, 10, 435209, tzinfo=Timezone('UTC')), 'duration': 609}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:58:10,450] {logging_mixin.py:109} INFO - [2022-12-04 18:58:10,450] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:58:10,465] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.135 seconds
[2022-12-04 18:58:20,723] {processor.py:163} INFO - Started process (PID=1643) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:58:20,723] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:58:20,724] {logging_mixin.py:109} INFO - [2022-12-04 18:58:20,724] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:58:20,817] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:58:20,835] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 58, 20, 828068, tzinfo=Timezone('UTC')), 'duration': 619}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:58:20,844] {logging_mixin.py:109} INFO - [2022-12-04 18:58:20,844] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:58:20,860] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.140 seconds
[2022-12-04 18:58:30,920] {processor.py:163} INFO - Started process (PID=1669) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:58:30,920] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:58:30,921] {logging_mixin.py:109} INFO - [2022-12-04 18:58:30,921] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:58:31,029] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:58:31,048] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 58, 31, 39950, tzinfo=Timezone('UTC')), 'duration': 629}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:58:31,056] {logging_mixin.py:109} INFO - [2022-12-04 18:58:31,056] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:58:31,071] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.153 seconds
[2022-12-04 18:58:40,952] {processor.py:163} INFO - Started process (PID=1679) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:58:40,953] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:58:40,953] {logging_mixin.py:109} INFO - [2022-12-04 18:58:40,953] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:58:41,045] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:58:41,062] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 58, 41, 54843, tzinfo=Timezone('UTC')), 'duration': 639}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:58:41,070] {logging_mixin.py:109} INFO - [2022-12-04 18:58:41,070] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:58:41,085] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.135 seconds
[2022-12-04 18:58:51,207] {processor.py:163} INFO - Started process (PID=1697) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:58:51,207] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:58:51,208] {logging_mixin.py:109} INFO - [2022-12-04 18:58:51,208] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:58:51,300] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:58:51,318] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 58, 51, 310419, tzinfo=Timezone('UTC')), 'duration': 650}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:58:51,326] {logging_mixin.py:109} INFO - [2022-12-04 18:58:51,326] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:58:51,341] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.137 seconds
[2022-12-04 18:59:01,309] {processor.py:163} INFO - Started process (PID=1732) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:59:01,310] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:59:01,310] {logging_mixin.py:109} INFO - [2022-12-04 18:59:01,310] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:59:01,401] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:59:01,419] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 59, 1, 411120, tzinfo=Timezone('UTC')), 'duration': 660}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:59:01,427] {logging_mixin.py:109} INFO - [2022-12-04 18:59:01,427] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:59:01,443] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.136 seconds
[2022-12-04 18:59:11,345] {processor.py:163} INFO - Started process (PID=1742) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:59:11,346] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:59:11,346] {logging_mixin.py:109} INFO - [2022-12-04 18:59:11,346] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:59:11,438] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:59:11,456] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 59, 11, 448527, tzinfo=Timezone('UTC')), 'duration': 670}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:59:11,464] {logging_mixin.py:109} INFO - [2022-12-04 18:59:11,464] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:59:11,479] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.137 seconds
[2022-12-04 18:59:21,736] {processor.py:163} INFO - Started process (PID=1760) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:59:21,737] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:59:21,737] {logging_mixin.py:109} INFO - [2022-12-04 18:59:21,737] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:59:21,830] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:59:21,847] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 59, 21, 840612, tzinfo=Timezone('UTC')), 'duration': 680}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:59:21,855] {logging_mixin.py:109} INFO - [2022-12-04 18:59:21,855] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:59:21,871] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.136 seconds
[2022-12-04 18:59:31,928] {processor.py:163} INFO - Started process (PID=1795) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:59:31,928] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:59:31,929] {logging_mixin.py:109} INFO - [2022-12-04 18:59:31,929] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:59:32,020] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:59:32,040] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 59, 32, 30900, tzinfo=Timezone('UTC')), 'duration': 690}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:59:32,048] {logging_mixin.py:109} INFO - [2022-12-04 18:59:32,048] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:59:32,063] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.137 seconds
[2022-12-04 18:59:41,983] {processor.py:163} INFO - Started process (PID=1796) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:59:41,984] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:59:41,984] {logging_mixin.py:109} INFO - [2022-12-04 18:59:41,984] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:59:42,076] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:59:42,096] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 59, 42, 86573, tzinfo=Timezone('UTC')), 'duration': 700}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:59:42,104] {logging_mixin.py:109} INFO - [2022-12-04 18:59:42,104] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:59:42,117] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.137 seconds
[2022-12-04 18:59:52,340] {processor.py:163} INFO - Started process (PID=1814) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:59:52,341] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 18:59:52,341] {logging_mixin.py:109} INFO - [2022-12-04 18:59:52,341] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:59:52,433] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 18:59:52,453] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 18, 59, 52, 443871, tzinfo=Timezone('UTC')), 'duration': 711}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 18:59:52,461] {logging_mixin.py:109} INFO - [2022-12-04 18:59:52,461] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 18:59:52,475] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.136 seconds
[2022-12-04 19:00:02,522] {processor.py:163} INFO - Started process (PID=1849) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:00:02,523] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:00:02,523] {logging_mixin.py:109} INFO - [2022-12-04 19:00:02,523] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:00:02,614] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:00:02,633] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 19, 0, 2, 624517, tzinfo=Timezone('UTC')), 'duration': 721}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 19:00:02,641] {logging_mixin.py:109} INFO - [2022-12-04 19:00:02,641] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 19:00:02,655] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.136 seconds
[2022-12-04 19:00:12,560] {processor.py:163} INFO - Started process (PID=1859) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:00:12,561] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:00:12,561] {logging_mixin.py:109} INFO - [2022-12-04 19:00:12,561] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:00:12,653] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:00:12,672] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 19, 0, 12, 664041, tzinfo=Timezone('UTC')), 'duration': 731}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 19:00:12,680] {logging_mixin.py:109} INFO - [2022-12-04 19:00:12,680] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 19:00:12,693] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.135 seconds
[2022-12-04 19:00:22,946] {processor.py:163} INFO - Started process (PID=1877) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:00:22,946] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:00:22,947] {logging_mixin.py:109} INFO - [2022-12-04 19:00:22,947] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:00:23,037] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:00:23,056] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 19, 0, 23, 49064, tzinfo=Timezone('UTC')), 'duration': 741}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 19:00:23,064] {logging_mixin.py:109} INFO - [2022-12-04 19:00:23,064] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 19:00:23,077] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.134 seconds
[2022-12-04 19:00:33,079] {processor.py:163} INFO - Started process (PID=1912) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:00:33,079] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:00:33,080] {logging_mixin.py:109} INFO - [2022-12-04 19:00:33,079] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:00:33,170] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:00:33,189] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 19, 0, 33, 181339, tzinfo=Timezone('UTC')), 'duration': 751}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 19:00:33,197] {logging_mixin.py:109} INFO - [2022-12-04 19:00:33,197] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 19:00:33,210] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.134 seconds
[2022-12-04 19:00:43,117] {processor.py:163} INFO - Started process (PID=1922) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:00:43,118] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:00:43,118] {logging_mixin.py:109} INFO - [2022-12-04 19:00:43,118] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:00:43,210] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:00:43,230] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 19, 0, 43, 222184, tzinfo=Timezone('UTC')), 'duration': 762}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 19:00:43,238] {logging_mixin.py:109} INFO - [2022-12-04 19:00:43,238] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 19:00:43,251] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.137 seconds
[2022-12-04 19:00:53,501] {processor.py:163} INFO - Started process (PID=1931) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:00:53,501] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:00:53,502] {logging_mixin.py:109} INFO - [2022-12-04 19:00:53,502] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:00:53,593] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:00:53,612] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 19, 0, 53, 605252, tzinfo=Timezone('UTC')), 'duration': 772}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 19:00:53,620] {logging_mixin.py:109} INFO - [2022-12-04 19:00:53,620] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 19:00:53,634] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.135 seconds
[2022-12-04 19:01:03,576] {processor.py:163} INFO - Started process (PID=1966) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:01:03,577] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:01:03,577] {logging_mixin.py:109} INFO - [2022-12-04 19:01:03,577] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:01:03,668] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:01:03,687] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 19, 1, 3, 679874, tzinfo=Timezone('UTC')), 'duration': 782}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 19:01:03,695] {logging_mixin.py:109} INFO - [2022-12-04 19:01:03,695] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 19:01:03,709] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.135 seconds
[2022-12-04 19:01:13,610] {processor.py:163} INFO - Started process (PID=1976) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:01:13,611] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:01:13,611] {logging_mixin.py:109} INFO - [2022-12-04 19:01:13,611] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:01:13,702] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:01:13,719] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 19, 1, 13, 713173, tzinfo=Timezone('UTC')), 'duration': 792}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 19:01:13,728] {logging_mixin.py:109} INFO - [2022-12-04 19:01:13,728] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 19:01:13,741] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.133 seconds
[2022-12-04 19:01:23,994] {processor.py:163} INFO - Started process (PID=1994) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:01:23,994] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:01:23,994] {logging_mixin.py:109} INFO - [2022-12-04 19:01:23,994] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:01:24,086] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:01:24,104] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 19, 1, 24, 97874, tzinfo=Timezone('UTC')), 'duration': 802}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 19:01:24,113] {logging_mixin.py:109} INFO - [2022-12-04 19:01:24,113] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 19:01:24,126] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.135 seconds
[2022-12-04 19:01:34,160] {processor.py:163} INFO - Started process (PID=2029) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:01:34,161] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:01:34,161] {logging_mixin.py:109} INFO - [2022-12-04 19:01:34,161] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:01:34,252] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:01:34,270] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 19, 1, 34, 262925, tzinfo=Timezone('UTC')), 'duration': 813}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 19:01:34,278] {logging_mixin.py:109} INFO - [2022-12-04 19:01:34,278] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 19:01:34,292] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.134 seconds
[2022-12-04 19:01:44,198] {processor.py:163} INFO - Started process (PID=2039) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:01:44,199] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:01:44,199] {logging_mixin.py:109} INFO - [2022-12-04 19:01:44,199] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:01:44,290] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:01:44,309] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 19, 1, 44, 301677, tzinfo=Timezone('UTC')), 'duration': 823}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 19:01:44,317] {logging_mixin.py:109} INFO - [2022-12-04 19:01:44,317] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 19:01:44,331] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.135 seconds
[2022-12-04 19:01:54,582] {processor.py:163} INFO - Started process (PID=2057) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:01:54,583] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:01:54,583] {logging_mixin.py:109} INFO - [2022-12-04 19:01:54,583] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:01:54,676] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:01:54,695] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 19, 1, 54, 688102, tzinfo=Timezone('UTC')), 'duration': 833}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 19:01:54,703] {logging_mixin.py:109} INFO - [2022-12-04 19:01:54,703] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 19:01:54,718] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.138 seconds
[2022-12-04 19:02:04,767] {processor.py:163} INFO - Started process (PID=2083) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:02:04,767] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:02:04,768] {logging_mixin.py:109} INFO - [2022-12-04 19:02:04,768] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:02:04,859] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:02:04,877] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 19, 2, 4, 870546, tzinfo=Timezone('UTC')), 'duration': 843}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 19:02:04,885] {logging_mixin.py:109} INFO - [2022-12-04 19:02:04,885] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 19:02:04,899] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.134 seconds
[2022-12-04 19:02:14,805] {processor.py:163} INFO - Started process (PID=2093) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:02:14,805] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:02:14,806] {logging_mixin.py:109} INFO - [2022-12-04 19:02:14,806] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:02:14,896] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:02:14,914] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 19, 2, 14, 907820, tzinfo=Timezone('UTC')), 'duration': 853}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 19:02:14,922] {logging_mixin.py:109} INFO - [2022-12-04 19:02:14,922] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 19:02:14,936] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.133 seconds
[2022-12-04 19:02:25,189] {processor.py:163} INFO - Started process (PID=2111) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:02:25,189] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:02:25,190] {logging_mixin.py:109} INFO - [2022-12-04 19:02:25,190] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:02:25,281] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:02:25,299] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 19, 2, 25, 292705, tzinfo=Timezone('UTC')), 'duration': 864}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 19:02:25,307] {logging_mixin.py:109} INFO - [2022-12-04 19:02:25,307] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 19:02:25,321] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.134 seconds
[2022-12-04 19:02:35,316] {processor.py:163} INFO - Started process (PID=2146) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:02:35,317] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:02:35,317] {logging_mixin.py:109} INFO - [2022-12-04 19:02:35,317] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:02:35,409] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:02:35,428] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 19, 2, 35, 420001, tzinfo=Timezone('UTC')), 'duration': 874}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 19:02:35,435] {logging_mixin.py:109} INFO - [2022-12-04 19:02:35,435] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 19:02:35,449] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.135 seconds
[2022-12-04 19:02:45,354] {processor.py:163} INFO - Started process (PID=2156) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:02:45,355] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:02:45,355] {logging_mixin.py:109} INFO - [2022-12-04 19:02:45,355] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:02:45,447] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:02:45,465] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 19, 2, 45, 458814, tzinfo=Timezone('UTC')), 'duration': 884}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 19:02:45,474] {logging_mixin.py:109} INFO - [2022-12-04 19:02:45,474] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 19:02:45,487] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.135 seconds
[2022-12-04 19:02:55,763] {processor.py:163} INFO - Started process (PID=2174) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:02:55,763] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:02:55,764] {logging_mixin.py:109} INFO - [2022-12-04 19:02:55,763] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:02:55,856] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:02:55,874] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 19, 2, 55, 866699, tzinfo=Timezone('UTC')), 'duration': 894}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 19:02:55,882] {logging_mixin.py:109} INFO - [2022-12-04 19:02:55,882] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 19:02:55,896] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.136 seconds
[2022-12-04 19:03:05,931] {processor.py:163} INFO - Started process (PID=2209) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:03:05,932] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:03:05,932] {logging_mixin.py:109} INFO - [2022-12-04 19:03:05,932] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:03:06,025] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:03:06,043] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 19, 3, 6, 35006, tzinfo=Timezone('UTC')), 'duration': 904}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 19:03:06,051] {logging_mixin.py:109} INFO - [2022-12-04 19:03:06,051] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 19:03:06,065] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.135 seconds
[2022-12-04 19:03:15,968] {processor.py:163} INFO - Started process (PID=2210) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:03:15,969] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:03:15,969] {logging_mixin.py:109} INFO - [2022-12-04 19:03:15,969] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:03:16,062] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:03:16,078] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 19, 3, 16, 71955, tzinfo=Timezone('UTC')), 'duration': 914}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 19:03:16,086] {logging_mixin.py:109} INFO - [2022-12-04 19:03:16,086] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 19:03:16,100] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.134 seconds
[2022-12-04 19:03:26,349] {processor.py:163} INFO - Started process (PID=2228) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:03:26,350] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:03:26,350] {logging_mixin.py:109} INFO - [2022-12-04 19:03:26,350] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:03:26,448] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:03:26,466] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 19, 3, 26, 458382, tzinfo=Timezone('UTC')), 'duration': 925}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 19:03:26,474] {logging_mixin.py:109} INFO - [2022-12-04 19:03:26,474] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 19:03:26,489] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.141 seconds
[2022-12-04 19:03:36,498] {processor.py:163} INFO - Started process (PID=2263) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:03:36,499] {processor.py:642} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:03:36,499] {logging_mixin.py:109} INFO - [2022-12-04 19:03:36,499] {dagbag.py:500} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:03:36,592] {processor.py:652} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:03:36,609] {processor.py:584} ERROR - Error executing TaskCallbackRequest callback for file: /opt/airflow/dags/s3_file_sensor2.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 575, in execute_callbacks
    self._execute_task_callbacks(dagbag, request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/processor.py", line 607, in _execute_task_callbacks
    ti.handle_failure_with_callback(error=request.msg, test_mode=self.UNIT_TEST_MODE)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1839, in handle_failure_with_callback
    self.handle_failure(error=error, test_mode=test_mode, force_fail=force_fail, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1829, in handle_failure
    session.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2540, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2682, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2642, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1136, in _emit_insert_statements
    statement, params
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column "execution_date" of relation "task_fail" does not exist
LINE 1: INSERT INTO task_fail (task_id, dag_id, execution_date, star...
                                                ^

[SQL: INSERT INTO task_fail (task_id, dag_id, execution_date, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 's3_file_check', 'dag_id': 'second_test_s3_file_sensor', 'execution_date': datetime.datetime(2022, 12, 3, 0, 0, tzinfo=Timezone('UTC')), 'start_date': datetime.datetime(2022, 12, 4, 18, 48, 1, 197748, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2022, 12, 4, 19, 3, 36, 602176, tzinfo=Timezone('UTC')), 'duration': 935}]
(Background on this error at: http://sqlalche.me/e/13/f405)
[2022-12-04 19:03:36,617] {logging_mixin.py:109} INFO - [2022-12-04 19:03:36,617] {dag.py:2398} INFO - Sync 1 DAGs
[2022-12-04 19:03:36,630] {processor.py:171} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.134 seconds
[2022-12-04 19:09:53,371] {processor.py:153} INFO - Started process (PID=46) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:09:53,371] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:09:53,372] {logging_mixin.py:115} INFO - [2022-12-04 19:09:53,372] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:09:53,662] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:09:53,746] {logging_mixin.py:115} INFO - [2022-12-04 19:09:53,746] {manager.py:508} INFO - Created Permission View: can delete on DAG:second_test_s3_file_sensor
[2022-12-04 19:09:53,752] {logging_mixin.py:115} INFO - [2022-12-04 19:09:53,752] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:09:53,775] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.406 seconds
[2022-12-04 19:10:03,422] {processor.py:153} INFO - Started process (PID=91) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:10:03,423] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:10:03,423] {logging_mixin.py:115} INFO - [2022-12-04 19:10:03,423] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:10:03,501] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:10:03,512] {logging_mixin.py:115} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/configuration.py:470 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
[2022-12-04 19:10:03,532] {processor.py:611} INFO - Executed failure callback for <TaskInstance: second_test_s3_file_sensor.s3_file_check scheduled__2022-12-03T00:00:00+00:00 [up_for_retry]> in state up_for_retry
[2022-12-04 19:10:03,534] {logging_mixin.py:115} INFO - [2022-12-04 19:10:03,534] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:10:03,554] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.134 seconds
[2022-12-04 19:10:33,780] {processor.py:153} INFO - Started process (PID=154) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:10:33,781] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:10:33,781] {logging_mixin.py:115} INFO - [2022-12-04 19:10:33,781] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:10:33,858] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:10:33,877] {logging_mixin.py:115} INFO - [2022-12-04 19:10:33,877] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:10:33,900] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.123 seconds
[2022-12-04 19:11:04,125] {processor.py:153} INFO - Started process (PID=217) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:11:04,125] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:11:04,125] {logging_mixin.py:115} INFO - [2022-12-04 19:11:04,125] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:11:04,209] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:11:04,228] {logging_mixin.py:115} INFO - [2022-12-04 19:11:04,228] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:11:04,251] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.128 seconds
[2022-12-04 19:11:34,490] {processor.py:153} INFO - Started process (PID=271) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:11:34,491] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:11:34,491] {logging_mixin.py:115} INFO - [2022-12-04 19:11:34,491] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:11:34,568] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:11:34,587] {logging_mixin.py:115} INFO - [2022-12-04 19:11:34,587] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:11:34,610] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.122 seconds
[2022-12-04 19:12:04,883] {processor.py:153} INFO - Started process (PID=334) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:12:04,883] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:12:04,883] {logging_mixin.py:115} INFO - [2022-12-04 19:12:04,883] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:12:04,961] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:12:04,980] {logging_mixin.py:115} INFO - [2022-12-04 19:12:04,980] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:12:05,003] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.122 seconds
[2022-12-04 19:12:35,338] {processor.py:153} INFO - Started process (PID=397) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:12:35,338] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:12:35,338] {logging_mixin.py:115} INFO - [2022-12-04 19:12:35,338] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:12:35,415] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:12:35,434] {logging_mixin.py:115} INFO - [2022-12-04 19:12:35,434] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:12:35,456] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.121 seconds
[2022-12-04 19:13:05,666] {processor.py:153} INFO - Started process (PID=451) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:13:05,666] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:13:05,666] {logging_mixin.py:115} INFO - [2022-12-04 19:13:05,666] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:13:05,743] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:13:05,762] {logging_mixin.py:115} INFO - [2022-12-04 19:13:05,762] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:13:05,785] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.122 seconds
[2022-12-04 19:13:35,973] {processor.py:153} INFO - Started process (PID=514) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:13:35,973] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:13:35,974] {logging_mixin.py:115} INFO - [2022-12-04 19:13:35,974] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:13:36,051] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:13:36,069] {logging_mixin.py:115} INFO - [2022-12-04 19:13:36,069] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:13:36,093] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.122 seconds
[2022-12-04 19:14:06,333] {processor.py:153} INFO - Started process (PID=577) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:14:06,333] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:14:06,333] {logging_mixin.py:115} INFO - [2022-12-04 19:14:06,333] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:14:06,410] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:14:06,429] {logging_mixin.py:115} INFO - [2022-12-04 19:14:06,429] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:14:06,452] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.122 seconds
[2022-12-04 19:14:36,625] {processor.py:153} INFO - Started process (PID=631) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:14:36,626] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:14:36,626] {logging_mixin.py:115} INFO - [2022-12-04 19:14:36,626] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:14:36,704] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:14:36,723] {logging_mixin.py:115} INFO - [2022-12-04 19:14:36,723] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:14:36,746] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.123 seconds
[2022-12-04 19:15:06,981] {processor.py:153} INFO - Started process (PID=694) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:15:06,982] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:15:06,982] {logging_mixin.py:115} INFO - [2022-12-04 19:15:06,982] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:15:07,059] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:15:07,078] {logging_mixin.py:115} INFO - [2022-12-04 19:15:07,078] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:15:07,101] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.122 seconds
[2022-12-04 19:15:37,422] {processor.py:153} INFO - Started process (PID=757) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:15:37,422] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:15:37,423] {logging_mixin.py:115} INFO - [2022-12-04 19:15:37,423] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:15:37,499] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:15:37,518] {logging_mixin.py:115} INFO - [2022-12-04 19:15:37,518] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:15:37,540] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.121 seconds
[2022-12-04 19:16:07,817] {processor.py:153} INFO - Started process (PID=820) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:16:07,818] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:16:07,818] {logging_mixin.py:115} INFO - [2022-12-04 19:16:07,818] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:16:07,894] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:16:07,913] {logging_mixin.py:115} INFO - [2022-12-04 19:16:07,913] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:16:07,936] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.121 seconds
[2022-12-04 19:16:38,275] {processor.py:153} INFO - Started process (PID=874) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:16:38,275] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:16:38,276] {logging_mixin.py:115} INFO - [2022-12-04 19:16:38,276] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:16:38,351] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:16:38,370] {logging_mixin.py:115} INFO - [2022-12-04 19:16:38,370] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:16:38,393] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.120 seconds
[2022-12-04 19:17:08,580] {processor.py:153} INFO - Started process (PID=937) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:17:08,581] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:17:08,581] {logging_mixin.py:115} INFO - [2022-12-04 19:17:08,581] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:17:08,657] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:17:08,676] {logging_mixin.py:115} INFO - [2022-12-04 19:17:08,676] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:17:08,699] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.121 seconds
[2022-12-04 19:17:38,848] {processor.py:153} INFO - Started process (PID=1000) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:17:38,849] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:17:38,849] {logging_mixin.py:115} INFO - [2022-12-04 19:17:38,849] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:17:38,926] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:17:38,946] {logging_mixin.py:115} INFO - [2022-12-04 19:17:38,946] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:17:38,969] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.123 seconds
[2022-12-04 19:18:09,368] {processor.py:153} INFO - Started process (PID=1054) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:18:09,369] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:18:09,369] {logging_mixin.py:115} INFO - [2022-12-04 19:18:09,369] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:18:09,446] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:18:09,465] {logging_mixin.py:115} INFO - [2022-12-04 19:18:09,465] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:18:09,488] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.122 seconds
[2022-12-04 19:18:39,779] {processor.py:153} INFO - Started process (PID=1117) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:18:39,780] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:18:39,780] {logging_mixin.py:115} INFO - [2022-12-04 19:18:39,780] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:18:39,857] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:18:39,876] {logging_mixin.py:115} INFO - [2022-12-04 19:18:39,876] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:18:39,899] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.123 seconds
[2022-12-04 19:19:10,170] {processor.py:153} INFO - Started process (PID=1180) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:19:10,170] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:19:10,171] {logging_mixin.py:115} INFO - [2022-12-04 19:19:10,171] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:19:10,247] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:19:10,266] {logging_mixin.py:115} INFO - [2022-12-04 19:19:10,266] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:19:10,289] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.122 seconds
[2022-12-04 19:19:40,479] {processor.py:153} INFO - Started process (PID=1234) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:19:40,480] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:19:40,480] {logging_mixin.py:115} INFO - [2022-12-04 19:19:40,480] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:19:40,556] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:19:40,575] {logging_mixin.py:115} INFO - [2022-12-04 19:19:40,575] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:19:40,598] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.122 seconds
[2022-12-04 19:20:10,953] {processor.py:153} INFO - Started process (PID=1299) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:20:10,953] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:20:10,954] {logging_mixin.py:115} INFO - [2022-12-04 19:20:10,954] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:20:11,029] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:20:11,048] {logging_mixin.py:115} INFO - [2022-12-04 19:20:11,048] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:20:11,071] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.120 seconds
[2022-12-04 19:20:41,395] {processor.py:153} INFO - Started process (PID=1364) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:20:41,396] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:20:41,396] {logging_mixin.py:115} INFO - [2022-12-04 19:20:41,396] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:20:41,473] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:20:41,492] {logging_mixin.py:115} INFO - [2022-12-04 19:20:41,492] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:20:41,515] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.123 seconds
[2022-12-04 19:21:11,942] {processor.py:153} INFO - Started process (PID=1427) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:21:11,942] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:21:11,942] {logging_mixin.py:115} INFO - [2022-12-04 19:21:11,942] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:21:12,020] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:21:12,040] {logging_mixin.py:115} INFO - [2022-12-04 19:21:12,039] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:21:12,063] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.124 seconds
[2022-12-04 19:21:42,933] {processor.py:153} INFO - Started process (PID=1485) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:21:42,933] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:21:42,934] {logging_mixin.py:115} INFO - [2022-12-04 19:21:42,934] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:21:43,010] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:21:43,029] {logging_mixin.py:115} INFO - [2022-12-04 19:21:43,029] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:21:43,052] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.122 seconds
[2022-12-04 19:22:13,469] {processor.py:153} INFO - Started process (PID=1550) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:22:13,469] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:22:13,469] {logging_mixin.py:115} INFO - [2022-12-04 19:22:13,469] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:22:13,547] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:22:13,566] {logging_mixin.py:115} INFO - [2022-12-04 19:22:13,566] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:22:13,589] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.123 seconds
[2022-12-04 19:22:43,960] {processor.py:153} INFO - Started process (PID=1615) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:22:43,960] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:22:43,961] {logging_mixin.py:115} INFO - [2022-12-04 19:22:43,961] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:22:44,039] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:22:44,058] {logging_mixin.py:115} INFO - [2022-12-04 19:22:44,058] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:22:44,081] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.124 seconds
[2022-12-04 19:23:14,102] {processor.py:153} INFO - Started process (PID=1671) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:23:14,103] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:23:14,103] {logging_mixin.py:115} INFO - [2022-12-04 19:23:14,103] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:23:14,180] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:23:14,198] {logging_mixin.py:115} INFO - [2022-12-04 19:23:14,198] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:23:14,222] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.122 seconds
[2022-12-04 19:23:44,470] {processor.py:153} INFO - Started process (PID=1736) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:23:44,470] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:23:44,471] {logging_mixin.py:115} INFO - [2022-12-04 19:23:44,471] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:23:44,548] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:23:44,567] {logging_mixin.py:115} INFO - [2022-12-04 19:23:44,567] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:23:44,590] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.122 seconds
[2022-12-04 19:24:14,982] {processor.py:153} INFO - Started process (PID=1801) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:24:14,982] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:24:14,983] {logging_mixin.py:115} INFO - [2022-12-04 19:24:14,983] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:24:15,059] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:24:15,078] {logging_mixin.py:115} INFO - [2022-12-04 19:24:15,078] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:24:15,101] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.121 seconds
[2022-12-04 19:24:46,095] {processor.py:153} INFO - Started process (PID=1857) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:24:46,096] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:24:46,096] {logging_mixin.py:115} INFO - [2022-12-04 19:24:46,096] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:24:46,173] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:24:46,192] {logging_mixin.py:115} INFO - [2022-12-04 19:24:46,192] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:24:46,215] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.123 seconds
[2022-12-04 19:25:16,537] {processor.py:153} INFO - Started process (PID=1922) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:25:16,537] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:25:16,537] {logging_mixin.py:115} INFO - [2022-12-04 19:25:16,537] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:25:16,614] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:25:16,633] {logging_mixin.py:115} INFO - [2022-12-04 19:25:16,633] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:25:16,656] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.122 seconds
[2022-12-04 19:25:46,943] {processor.py:153} INFO - Started process (PID=1987) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:25:46,943] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:25:46,943] {logging_mixin.py:115} INFO - [2022-12-04 19:25:46,943] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:25:47,022] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:25:47,042] {logging_mixin.py:115} INFO - [2022-12-04 19:25:47,042] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:25:47,065] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.125 seconds
[2022-12-04 19:26:17,631] {processor.py:153} INFO - Started process (PID=2052) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:26:17,632] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:26:17,632] {logging_mixin.py:115} INFO - [2022-12-04 19:26:17,632] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:26:17,711] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:26:17,730] {logging_mixin.py:115} INFO - [2022-12-04 19:26:17,730] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:26:17,753] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.124 seconds
[2022-12-04 19:26:48,297] {processor.py:153} INFO - Started process (PID=2108) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:26:48,297] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:26:48,298] {logging_mixin.py:115} INFO - [2022-12-04 19:26:48,298] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:26:48,374] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:26:48,392] {logging_mixin.py:115} INFO - [2022-12-04 19:26:48,392] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:26:48,415] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.121 seconds
[2022-12-04 19:27:18,884] {processor.py:153} INFO - Started process (PID=2173) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:27:18,885] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:27:18,885] {logging_mixin.py:115} INFO - [2022-12-04 19:27:18,885] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:27:18,962] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:27:18,981] {logging_mixin.py:115} INFO - [2022-12-04 19:27:18,981] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:27:19,004] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.123 seconds
[2022-12-04 19:27:49,426] {processor.py:153} INFO - Started process (PID=2238) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:27:49,426] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:27:49,427] {logging_mixin.py:115} INFO - [2022-12-04 19:27:49,427] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:27:49,503] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:27:49,522] {logging_mixin.py:115} INFO - [2022-12-04 19:27:49,522] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:27:49,545] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.122 seconds
[2022-12-04 19:28:20,043] {processor.py:153} INFO - Started process (PID=2294) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:28:20,043] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:28:20,044] {logging_mixin.py:115} INFO - [2022-12-04 19:28:20,044] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:28:20,120] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:28:20,139] {logging_mixin.py:115} INFO - [2022-12-04 19:28:20,139] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:28:20,162] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.121 seconds
[2022-12-04 19:28:50,654] {processor.py:153} INFO - Started process (PID=2359) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:28:50,655] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:28:50,655] {logging_mixin.py:115} INFO - [2022-12-04 19:28:50,655] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:28:50,731] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:28:50,750] {logging_mixin.py:115} INFO - [2022-12-04 19:28:50,750] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:28:50,773] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.121 seconds
[2022-12-04 19:29:21,291] {processor.py:153} INFO - Started process (PID=2424) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:29:21,291] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:29:21,292] {logging_mixin.py:115} INFO - [2022-12-04 19:29:21,292] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:29:21,367] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:29:21,386] {logging_mixin.py:115} INFO - [2022-12-04 19:29:21,386] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:29:21,409] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.120 seconds
[2022-12-04 19:29:51,903] {processor.py:153} INFO - Started process (PID=2489) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:29:51,904] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:29:51,904] {logging_mixin.py:115} INFO - [2022-12-04 19:29:51,904] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:29:51,981] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:29:52,001] {logging_mixin.py:115} INFO - [2022-12-04 19:29:52,001] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:29:52,026] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.125 seconds
[2022-12-04 19:30:22,693] {processor.py:153} INFO - Started process (PID=2545) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:30:22,694] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:30:22,694] {logging_mixin.py:115} INFO - [2022-12-04 19:30:22,694] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:30:22,771] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:30:22,790] {logging_mixin.py:115} INFO - [2022-12-04 19:30:22,790] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:30:22,814] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.123 seconds
[2022-12-04 19:30:53,365] {processor.py:153} INFO - Started process (PID=2610) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:30:53,365] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:30:53,366] {logging_mixin.py:115} INFO - [2022-12-04 19:30:53,365] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:30:53,442] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:30:53,462] {logging_mixin.py:115} INFO - [2022-12-04 19:30:53,462] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:30:53,486] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.123 seconds
[2022-12-04 19:31:23,873] {processor.py:153} INFO - Started process (PID=2675) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:31:23,874] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:31:23,874] {logging_mixin.py:115} INFO - [2022-12-04 19:31:23,874] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:31:23,951] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:31:23,969] {logging_mixin.py:115} INFO - [2022-12-04 19:31:23,969] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:31:23,993] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.122 seconds
[2022-12-04 19:31:54,272] {processor.py:153} INFO - Started process (PID=2731) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:31:54,272] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:31:54,272] {logging_mixin.py:115} INFO - [2022-12-04 19:31:54,272] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:31:54,348] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:31:54,367] {logging_mixin.py:115} INFO - [2022-12-04 19:31:54,367] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:31:54,392] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.123 seconds
[2022-12-04 19:32:24,974] {processor.py:153} INFO - Started process (PID=2796) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:32:24,975] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:32:24,976] {logging_mixin.py:115} INFO - [2022-12-04 19:32:24,976] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:32:25,053] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:32:25,072] {logging_mixin.py:115} INFO - [2022-12-04 19:32:25,072] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:32:25,096] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.124 seconds
[2022-12-04 19:32:55,595] {processor.py:153} INFO - Started process (PID=2861) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:32:55,595] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:32:55,596] {logging_mixin.py:115} INFO - [2022-12-04 19:32:55,596] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:32:55,672] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:32:55,691] {logging_mixin.py:115} INFO - [2022-12-04 19:32:55,691] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:32:55,715] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.122 seconds
[2022-12-04 19:33:26,222] {processor.py:153} INFO - Started process (PID=2926) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:33:26,223] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:33:26,223] {logging_mixin.py:115} INFO - [2022-12-04 19:33:26,223] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:33:26,301] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:33:26,320] {logging_mixin.py:115} INFO - [2022-12-04 19:33:26,320] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:33:26,345] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.125 seconds
[2022-12-04 19:33:56,902] {processor.py:153} INFO - Started process (PID=2982) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:33:56,903] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:33:56,903] {logging_mixin.py:115} INFO - [2022-12-04 19:33:56,903] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:33:56,979] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:33:56,998] {logging_mixin.py:115} INFO - [2022-12-04 19:33:56,998] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:33:57,022] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.122 seconds
[2022-12-04 19:34:27,591] {processor.py:153} INFO - Started process (PID=3047) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:34:27,592] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:34:27,592] {logging_mixin.py:115} INFO - [2022-12-04 19:34:27,592] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:34:27,669] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:34:27,688] {logging_mixin.py:115} INFO - [2022-12-04 19:34:27,688] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:34:27,712] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.123 seconds
[2022-12-04 19:34:58,280] {processor.py:153} INFO - Started process (PID=3112) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:34:58,281] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:34:58,281] {logging_mixin.py:115} INFO - [2022-12-04 19:34:58,281] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:34:58,357] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:34:58,376] {logging_mixin.py:115} INFO - [2022-12-04 19:34:58,376] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:34:58,399] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.122 seconds
[2022-12-04 19:35:28,959] {processor.py:153} INFO - Started process (PID=3168) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:35:28,959] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:35:28,960] {logging_mixin.py:115} INFO - [2022-12-04 19:35:28,960] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:35:29,037] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:35:29,056] {logging_mixin.py:115} INFO - [2022-12-04 19:35:29,056] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:35:29,080] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.123 seconds
[2022-12-04 19:35:35,985] {processor.py:153} INFO - Started process (PID=3178) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:35:35,985] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:35:35,986] {logging_mixin.py:115} INFO - [2022-12-04 19:35:35,986] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:35:36,063] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:35:36,114] {logging_mixin.py:115} INFO - [2022-12-04 19:35:36,114] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:35:36,138] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.155 seconds
[2022-12-04 19:36:06,696] {processor.py:153} INFO - Started process (PID=3243) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:36:06,697] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:36:06,697] {logging_mixin.py:115} INFO - [2022-12-04 19:36:06,697] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:36:06,773] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:36:06,792] {logging_mixin.py:115} INFO - [2022-12-04 19:36:06,792] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:36:06,815] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.121 seconds
[2022-12-04 19:36:37,317] {processor.py:153} INFO - Started process (PID=3306) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:36:37,317] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:36:37,318] {logging_mixin.py:115} INFO - [2022-12-04 19:36:37,317] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:36:37,394] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:36:37,413] {logging_mixin.py:115} INFO - [2022-12-04 19:36:37,413] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:36:37,437] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.123 seconds
[2022-12-04 19:37:07,865] {processor.py:153} INFO - Started process (PID=3364) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:37:07,866] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:37:07,866] {logging_mixin.py:115} INFO - [2022-12-04 19:37:07,866] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:37:07,943] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:37:07,962] {logging_mixin.py:115} INFO - [2022-12-04 19:37:07,962] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:37:07,986] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.123 seconds
[2022-12-04 19:37:38,208] {processor.py:153} INFO - Started process (PID=3429) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:37:38,208] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:37:38,209] {logging_mixin.py:115} INFO - [2022-12-04 19:37:38,209] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:37:38,285] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:37:38,304] {logging_mixin.py:115} INFO - [2022-12-04 19:37:38,304] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:37:38,327] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.122 seconds
[2022-12-04 19:38:08,523] {processor.py:153} INFO - Started process (PID=3494) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:38:08,524] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:38:08,524] {logging_mixin.py:115} INFO - [2022-12-04 19:38:08,524] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:38:08,601] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:38:08,619] {logging_mixin.py:115} INFO - [2022-12-04 19:38:08,619] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:38:08,643] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.122 seconds
[2022-12-04 19:38:38,842] {processor.py:153} INFO - Started process (PID=3550) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:38:38,842] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:38:38,843] {logging_mixin.py:115} INFO - [2022-12-04 19:38:38,843] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:38:38,923] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:38:38,950] {logging_mixin.py:115} INFO - [2022-12-04 19:38:38,950] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:38:38,980] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.140 seconds
[2022-12-04 19:39:09,336] {processor.py:153} INFO - Started process (PID=3615) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:39:09,337] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:39:09,337] {logging_mixin.py:115} INFO - [2022-12-04 19:39:09,337] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:39:09,413] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:39:09,432] {logging_mixin.py:115} INFO - [2022-12-04 19:39:09,432] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:39:09,456] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.122 seconds
[2022-12-04 19:39:39,793] {processor.py:153} INFO - Started process (PID=3680) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:39:39,794] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:39:39,794] {logging_mixin.py:115} INFO - [2022-12-04 19:39:39,794] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:39:39,870] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:39:39,889] {logging_mixin.py:115} INFO - [2022-12-04 19:39:39,889] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:39:39,913] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.122 seconds
[2022-12-04 19:40:10,313] {processor.py:153} INFO - Started process (PID=3736) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:40:10,314] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:40:10,314] {logging_mixin.py:115} INFO - [2022-12-04 19:40:10,314] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:40:10,390] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:40:10,409] {logging_mixin.py:115} INFO - [2022-12-04 19:40:10,409] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:40:10,433] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.122 seconds
[2022-12-04 19:40:40,655] {processor.py:153} INFO - Started process (PID=3801) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:40:40,655] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:40:40,656] {logging_mixin.py:115} INFO - [2022-12-04 19:40:40,656] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:40:40,732] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:40:40,750] {logging_mixin.py:115} INFO - [2022-12-04 19:40:40,750] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:40:40,774] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.122 seconds
[2022-12-04 19:41:10,962] {processor.py:153} INFO - Started process (PID=3866) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:41:10,962] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:41:10,963] {logging_mixin.py:115} INFO - [2022-12-04 19:41:10,963] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:41:11,039] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:41:11,058] {logging_mixin.py:115} INFO - [2022-12-04 19:41:11,058] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:41:11,082] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.123 seconds
[2022-12-04 19:41:41,243] {processor.py:153} INFO - Started process (PID=3922) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:41:41,243] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:41:41,244] {logging_mixin.py:115} INFO - [2022-12-04 19:41:41,244] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:41:41,319] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:41:41,338] {logging_mixin.py:115} INFO - [2022-12-04 19:41:41,338] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:41:41,362] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.122 seconds
[2022-12-04 19:42:11,544] {processor.py:153} INFO - Started process (PID=3987) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:42:11,544] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:42:11,544] {logging_mixin.py:115} INFO - [2022-12-04 19:42:11,544] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:42:11,621] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:42:11,640] {logging_mixin.py:115} INFO - [2022-12-04 19:42:11,640] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:42:11,664] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.123 seconds
[2022-12-04 19:42:41,924] {processor.py:153} INFO - Started process (PID=4052) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:42:41,925] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:42:41,925] {logging_mixin.py:115} INFO - [2022-12-04 19:42:41,925] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:42:42,001] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:42:42,020] {logging_mixin.py:115} INFO - [2022-12-04 19:42:42,020] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:42:42,044] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.122 seconds
[2022-12-04 19:43:12,364] {processor.py:153} INFO - Started process (PID=4117) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:43:12,365] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:43:12,365] {logging_mixin.py:115} INFO - [2022-12-04 19:43:12,365] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:43:12,441] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:43:12,460] {logging_mixin.py:115} INFO - [2022-12-04 19:43:12,460] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:43:12,485] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.123 seconds
[2022-12-04 19:43:42,819] {processor.py:153} INFO - Started process (PID=4173) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:43:42,820] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:43:42,820] {logging_mixin.py:115} INFO - [2022-12-04 19:43:42,820] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:43:42,899] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:43:42,918] {logging_mixin.py:115} INFO - [2022-12-04 19:43:42,918] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:43:42,942] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.125 seconds
[2022-12-04 19:44:13,213] {processor.py:153} INFO - Started process (PID=4238) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:44:13,213] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:44:13,213] {logging_mixin.py:115} INFO - [2022-12-04 19:44:13,213] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:44:13,289] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:44:13,308] {logging_mixin.py:115} INFO - [2022-12-04 19:44:13,308] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:44:13,332] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.122 seconds
[2022-12-04 19:44:43,539] {processor.py:153} INFO - Started process (PID=4302) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:44:43,539] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:44:43,540] {logging_mixin.py:115} INFO - [2022-12-04 19:44:43,540] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:44:43,628] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:44:43,647] {logging_mixin.py:115} INFO - [2022-12-04 19:44:43,647] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:44:43,671] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.135 seconds
[2022-12-04 19:45:13,906] {processor.py:153} INFO - Started process (PID=4358) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:45:13,907] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:45:13,907] {logging_mixin.py:115} INFO - [2022-12-04 19:45:13,907] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:45:13,983] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:45:14,003] {logging_mixin.py:115} INFO - [2022-12-04 19:45:14,003] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:45:14,027] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.123 seconds
[2022-12-04 19:45:44,357] {processor.py:153} INFO - Started process (PID=4421) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:45:44,357] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:45:44,358] {logging_mixin.py:115} INFO - [2022-12-04 19:45:44,357] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:45:44,433] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:45:44,453] {logging_mixin.py:115} INFO - [2022-12-04 19:45:44,453] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:45:44,477] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.122 seconds
[2022-12-04 19:46:20,031] {processor.py:153} INFO - Started process (PID=45) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:46:20,032] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:46:20,033] {logging_mixin.py:115} INFO - [2022-12-04 19:46:20,033] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:46:20,185] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:46:20,213] {logging_mixin.py:115} INFO - [2022-12-04 19:46:20,213] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:46:20,247] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.218 seconds
[2022-12-04 19:46:51,146] {processor.py:153} INFO - Started process (PID=110) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:46:51,146] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:46:51,147] {logging_mixin.py:115} INFO - [2022-12-04 19:46:51,147] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:46:51,222] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:46:51,241] {logging_mixin.py:115} INFO - [2022-12-04 19:46:51,241] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:46:51,265] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.122 seconds
[2022-12-04 19:47:22,029] {processor.py:153} INFO - Started process (PID=175) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:47:22,029] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:47:22,030] {logging_mixin.py:115} INFO - [2022-12-04 19:47:22,030] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:47:22,105] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:47:22,124] {logging_mixin.py:115} INFO - [2022-12-04 19:47:22,124] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:47:22,148] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.122 seconds
[2022-12-04 19:47:52,897] {processor.py:153} INFO - Started process (PID=231) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:47:52,897] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:47:52,898] {logging_mixin.py:115} INFO - [2022-12-04 19:47:52,898] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:47:52,972] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:47:52,991] {logging_mixin.py:115} INFO - [2022-12-04 19:47:52,991] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:47:53,014] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.120 seconds
[2022-12-04 19:48:23,774] {processor.py:153} INFO - Started process (PID=297) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:48:23,774] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:48:23,775] {logging_mixin.py:115} INFO - [2022-12-04 19:48:23,775] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:48:23,849] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:48:23,868] {logging_mixin.py:115} INFO - [2022-12-04 19:48:23,868] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:48:23,891] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.120 seconds
[2022-12-04 19:48:54,660] {processor.py:153} INFO - Started process (PID=362) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:48:54,660] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:48:54,660] {logging_mixin.py:115} INFO - [2022-12-04 19:48:54,660] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:48:54,734] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:48:54,753] {logging_mixin.py:115} INFO - [2022-12-04 19:48:54,753] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:48:54,777] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.119 seconds
[2022-12-04 19:49:25,554] {processor.py:153} INFO - Started process (PID=427) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:49:25,555] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:49:25,555] {logging_mixin.py:115} INFO - [2022-12-04 19:49:25,555] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:49:25,630] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:49:25,649] {logging_mixin.py:115} INFO - [2022-12-04 19:49:25,649] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:49:25,673] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.121 seconds
[2022-12-04 19:49:56,263] {processor.py:153} INFO - Started process (PID=483) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:49:56,264] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:49:56,264] {logging_mixin.py:115} INFO - [2022-12-04 19:49:56,264] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:49:56,339] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:49:56,358] {logging_mixin.py:115} INFO - [2022-12-04 19:49:56,358] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:49:56,382] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.121 seconds
[2022-12-04 19:50:26,855] {processor.py:153} INFO - Started process (PID=548) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:50:26,856] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:50:26,856] {logging_mixin.py:115} INFO - [2022-12-04 19:50:26,856] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:50:26,931] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:50:26,950] {logging_mixin.py:115} INFO - [2022-12-04 19:50:26,950] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:50:26,969] {logging_mixin.py:115} INFO - [2022-12-04 19:50:26,969] {dag.py:2919} INFO - Setting next_dagrun for second_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 19:50:26,976] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.124 seconds
[2022-12-04 19:50:57,441] {processor.py:153} INFO - Started process (PID=613) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:50:57,442] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:50:57,442] {logging_mixin.py:115} INFO - [2022-12-04 19:50:57,442] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:50:57,516] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:50:57,535] {logging_mixin.py:115} INFO - [2022-12-04 19:50:57,535] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:50:57,553] {logging_mixin.py:115} INFO - [2022-12-04 19:50:57,553] {dag.py:2919} INFO - Setting next_dagrun for second_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 19:50:57,560] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.121 seconds
[2022-12-04 19:51:28,068] {processor.py:153} INFO - Started process (PID=678) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:51:28,068] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:51:28,069] {logging_mixin.py:115} INFO - [2022-12-04 19:51:28,069] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:51:28,145] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:51:28,165] {logging_mixin.py:115} INFO - [2022-12-04 19:51:28,165] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:51:28,183] {logging_mixin.py:115} INFO - [2022-12-04 19:51:28,183] {dag.py:2919} INFO - Setting next_dagrun for second_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 19:51:28,190] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.125 seconds
[2022-12-04 19:51:58,776] {processor.py:153} INFO - Started process (PID=734) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:51:58,776] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:51:58,776] {logging_mixin.py:115} INFO - [2022-12-04 19:51:58,776] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:51:58,851] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:51:58,870] {logging_mixin.py:115} INFO - [2022-12-04 19:51:58,870] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:51:58,887] {logging_mixin.py:115} INFO - [2022-12-04 19:51:58,887] {dag.py:2919} INFO - Setting next_dagrun for second_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 19:51:58,894] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.121 seconds
[2022-12-04 19:52:29,425] {processor.py:153} INFO - Started process (PID=799) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:52:29,425] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:52:29,426] {logging_mixin.py:115} INFO - [2022-12-04 19:52:29,426] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:52:29,501] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:52:29,521] {logging_mixin.py:115} INFO - [2022-12-04 19:52:29,520] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:52:29,538] {logging_mixin.py:115} INFO - [2022-12-04 19:52:29,538] {dag.py:2919} INFO - Setting next_dagrun for second_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 19:52:29,546] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.124 seconds
[2022-12-04 19:53:00,372] {processor.py:153} INFO - Started process (PID=864) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:53:00,372] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:53:00,373] {logging_mixin.py:115} INFO - [2022-12-04 19:53:00,373] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:53:00,448] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:53:00,468] {logging_mixin.py:115} INFO - [2022-12-04 19:53:00,468] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:53:00,485] {logging_mixin.py:115} INFO - [2022-12-04 19:53:00,485] {dag.py:2919} INFO - Setting next_dagrun for second_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 19:53:00,493] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.124 seconds
[2022-12-04 19:53:31,052] {processor.py:153} INFO - Started process (PID=920) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:53:31,053] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:53:31,053] {logging_mixin.py:115} INFO - [2022-12-04 19:53:31,053] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:53:31,128] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:53:31,148] {logging_mixin.py:115} INFO - [2022-12-04 19:53:31,148] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:53:31,165] {logging_mixin.py:115} INFO - [2022-12-04 19:53:31,165] {dag.py:2919} INFO - Setting next_dagrun for second_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 19:53:31,173] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.123 seconds
[2022-12-04 19:54:01,813] {processor.py:153} INFO - Started process (PID=985) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:54:01,814] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:54:01,814] {logging_mixin.py:115} INFO - [2022-12-04 19:54:01,814] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:54:01,892] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:54:01,913] {logging_mixin.py:115} INFO - [2022-12-04 19:54:01,913] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:54:01,934] {logging_mixin.py:115} INFO - [2022-12-04 19:54:01,934] {dag.py:2919} INFO - Setting next_dagrun for second_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 19:54:01,947] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.136 seconds
[2022-12-04 19:54:32,305] {processor.py:153} INFO - Started process (PID=1050) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:54:32,306] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:54:32,306] {logging_mixin.py:115} INFO - [2022-12-04 19:54:32,306] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:54:32,382] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:54:32,402] {logging_mixin.py:115} INFO - [2022-12-04 19:54:32,402] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:54:32,419] {logging_mixin.py:115} INFO - [2022-12-04 19:54:32,419] {dag.py:2919} INFO - Setting next_dagrun for second_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 19:54:32,427] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.125 seconds
[2022-12-04 19:55:02,857] {processor.py:153} INFO - Started process (PID=1115) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:55:02,857] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:55:02,858] {logging_mixin.py:115} INFO - [2022-12-04 19:55:02,858] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:55:02,934] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:55:02,954] {logging_mixin.py:115} INFO - [2022-12-04 19:55:02,954] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:55:02,972] {logging_mixin.py:115} INFO - [2022-12-04 19:55:02,972] {dag.py:2919} INFO - Setting next_dagrun for second_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 19:55:02,980] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.125 seconds
[2022-12-04 19:55:33,302] {processor.py:153} INFO - Started process (PID=1171) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:55:33,302] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:55:33,302] {logging_mixin.py:115} INFO - [2022-12-04 19:55:33,302] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:55:33,378] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:55:33,398] {logging_mixin.py:115} INFO - [2022-12-04 19:55:33,398] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:55:33,415] {logging_mixin.py:115} INFO - [2022-12-04 19:55:33,415] {dag.py:2919} INFO - Setting next_dagrun for second_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 19:55:33,423] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.124 seconds
[2022-12-04 19:56:03,741] {processor.py:153} INFO - Started process (PID=1236) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:56:03,742] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:56:03,742] {logging_mixin.py:115} INFO - [2022-12-04 19:56:03,742] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:56:03,818] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:56:03,839] {logging_mixin.py:115} INFO - [2022-12-04 19:56:03,839] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:56:03,856] {logging_mixin.py:115} INFO - [2022-12-04 19:56:03,856] {dag.py:2919} INFO - Setting next_dagrun for second_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 19:56:03,864] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.125 seconds
[2022-12-04 19:56:34,160] {processor.py:153} INFO - Started process (PID=1301) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:56:34,161] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:56:34,161] {logging_mixin.py:115} INFO - [2022-12-04 19:56:34,161] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:56:34,237] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:56:34,256] {logging_mixin.py:115} INFO - [2022-12-04 19:56:34,256] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:56:34,274] {logging_mixin.py:115} INFO - [2022-12-04 19:56:34,274] {dag.py:2919} INFO - Setting next_dagrun for second_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 19:56:34,282] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.123 seconds
[2022-12-04 19:57:05,201] {processor.py:153} INFO - Started process (PID=1358) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:57:05,201] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:57:05,202] {logging_mixin.py:115} INFO - [2022-12-04 19:57:05,201] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:57:05,277] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:57:05,297] {logging_mixin.py:115} INFO - [2022-12-04 19:57:05,297] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:57:05,314] {logging_mixin.py:115} INFO - [2022-12-04 19:57:05,314] {dag.py:2919} INFO - Setting next_dagrun for second_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 19:57:05,322] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.124 seconds
[2022-12-04 19:57:36,243] {processor.py:153} INFO - Started process (PID=1423) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:57:36,244] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:57:36,244] {logging_mixin.py:115} INFO - [2022-12-04 19:57:36,244] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:57:36,319] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:57:36,338] {logging_mixin.py:115} INFO - [2022-12-04 19:57:36,338] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:57:36,355] {logging_mixin.py:115} INFO - [2022-12-04 19:57:36,355] {dag.py:2919} INFO - Setting next_dagrun for second_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 19:57:36,362] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.121 seconds
[2022-12-04 19:58:06,519] {processor.py:153} INFO - Started process (PID=1488) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:58:06,520] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:58:06,520] {logging_mixin.py:115} INFO - [2022-12-04 19:58:06,520] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:58:06,596] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:58:06,616] {logging_mixin.py:115} INFO - [2022-12-04 19:58:06,616] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:58:06,633] {logging_mixin.py:115} INFO - [2022-12-04 19:58:06,633] {dag.py:2919} INFO - Setting next_dagrun for second_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 19:58:06,640] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.124 seconds
[2022-12-04 19:58:36,974] {processor.py:153} INFO - Started process (PID=1553) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:58:36,974] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:58:36,975] {logging_mixin.py:115} INFO - [2022-12-04 19:58:36,975] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:58:37,051] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:58:37,072] {logging_mixin.py:115} INFO - [2022-12-04 19:58:37,071] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:58:37,089] {logging_mixin.py:115} INFO - [2022-12-04 19:58:37,089] {dag.py:2919} INFO - Setting next_dagrun for second_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 19:58:37,097] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.125 seconds
[2022-12-04 19:59:07,483] {processor.py:153} INFO - Started process (PID=1609) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:59:07,483] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:59:07,484] {logging_mixin.py:115} INFO - [2022-12-04 19:59:07,484] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:59:07,559] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:59:07,579] {logging_mixin.py:115} INFO - [2022-12-04 19:59:07,579] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:59:07,596] {logging_mixin.py:115} INFO - [2022-12-04 19:59:07,596] {dag.py:2919} INFO - Setting next_dagrun for second_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 19:59:07,603] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.123 seconds
[2022-12-04 19:59:37,869] {processor.py:153} INFO - Started process (PID=1674) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:59:37,870] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 19:59:37,870] {logging_mixin.py:115} INFO - [2022-12-04 19:59:37,870] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:59:37,946] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 19:59:37,971] {logging_mixin.py:115} INFO - [2022-12-04 19:59:37,971] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 19:59:37,998] {logging_mixin.py:115} INFO - [2022-12-04 19:59:37,998] {dag.py:2919} INFO - Setting next_dagrun for second_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 19:59:38,010] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.143 seconds
[2022-12-04 20:00:08,074] {processor.py:153} INFO - Started process (PID=1741) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:00:08,074] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:00:08,075] {logging_mixin.py:115} INFO - [2022-12-04 20:00:08,075] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:00:08,151] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:00:08,171] {logging_mixin.py:115} INFO - [2022-12-04 20:00:08,171] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:00:08,187] {logging_mixin.py:115} INFO - [2022-12-04 20:00:08,187] {dag.py:2919} INFO - Setting next_dagrun for second_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 20:00:08,195] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.123 seconds
[2022-12-04 20:00:39,105] {processor.py:153} INFO - Started process (PID=1797) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:00:39,106] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:00:39,106] {logging_mixin.py:115} INFO - [2022-12-04 20:00:39,106] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:00:39,182] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:00:39,201] {logging_mixin.py:115} INFO - [2022-12-04 20:00:39,201] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:00:39,218] {logging_mixin.py:115} INFO - [2022-12-04 20:00:39,218] {dag.py:2919} INFO - Setting next_dagrun for second_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 20:00:39,226] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.123 seconds
[2022-12-04 20:01:07,483] {processor.py:153} INFO - Started process (PID=1860) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:01:07,483] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:01:07,484] {logging_mixin.py:115} INFO - [2022-12-04 20:01:07,484] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:01:08,000] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:01:08,047] {logging_mixin.py:115} INFO - [2022-12-04 20:01:08,046] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:01:08,061] {logging_mixin.py:115} INFO - [2022-12-04 20:01:08,061] {dag.py:2919} INFO - Setting next_dagrun for second_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 20:01:08,072] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.591 seconds
[2022-12-04 20:01:38,731] {processor.py:153} INFO - Started process (PID=1926) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:01:38,731] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:01:38,732] {logging_mixin.py:115} INFO - [2022-12-04 20:01:38,732] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:01:39,257] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:01:39,274] {logging_mixin.py:115} INFO - [2022-12-04 20:01:39,273] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:01:39,290] {logging_mixin.py:115} INFO - [2022-12-04 20:01:39,290] {dag.py:2919} INFO - Setting next_dagrun for second_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 20:01:39,298] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.570 seconds
[2022-12-04 20:02:10,190] {processor.py:153} INFO - Started process (PID=1985) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:02:10,190] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:02:10,191] {logging_mixin.py:115} INFO - [2022-12-04 20:02:10,191] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:02:10,714] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:02:10,731] {logging_mixin.py:115} INFO - [2022-12-04 20:02:10,731] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:02:10,747] {logging_mixin.py:115} INFO - [2022-12-04 20:02:10,747] {dag.py:2919} INFO - Setting next_dagrun for second_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 20:02:10,755] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.567 seconds
[2022-12-04 20:02:40,793] {processor.py:153} INFO - Started process (PID=2058) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:02:40,794] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:02:40,794] {logging_mixin.py:115} INFO - [2022-12-04 20:02:40,794] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:02:41,306] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:02:41,323] {logging_mixin.py:115} INFO - [2022-12-04 20:02:41,323] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:02:41,340] {logging_mixin.py:115} INFO - [2022-12-04 20:02:41,340] {dag.py:2919} INFO - Setting next_dagrun for second_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 20:02:41,348] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.557 seconds
[2022-12-04 20:03:11,392] {processor.py:153} INFO - Started process (PID=2131) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:03:11,392] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:03:11,393] {logging_mixin.py:115} INFO - [2022-12-04 20:03:11,392] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:03:11,901] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:03:11,919] {logging_mixin.py:115} INFO - [2022-12-04 20:03:11,918] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:03:11,935] {logging_mixin.py:115} INFO - [2022-12-04 20:03:11,935] {dag.py:2919} INFO - Setting next_dagrun for second_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 20:03:11,942] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.553 seconds
[2022-12-04 20:03:42,710] {processor.py:153} INFO - Started process (PID=2201) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:03:42,711] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:03:42,711] {logging_mixin.py:115} INFO - [2022-12-04 20:03:42,711] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:03:43,224] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:03:43,242] {logging_mixin.py:115} INFO - [2022-12-04 20:03:43,241] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:03:43,258] {logging_mixin.py:115} INFO - [2022-12-04 20:03:43,258] {dag.py:2919} INFO - Setting next_dagrun for second_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 20:03:43,266] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.558 seconds
[2022-12-04 20:04:13,338] {processor.py:153} INFO - Started process (PID=2265) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:04:13,338] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:04:13,339] {logging_mixin.py:115} INFO - [2022-12-04 20:04:13,339] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:04:13,854] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:04:13,871] {logging_mixin.py:115} INFO - [2022-12-04 20:04:13,871] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:04:13,888] {logging_mixin.py:115} INFO - [2022-12-04 20:04:13,888] {dag.py:2919} INFO - Setting next_dagrun for second_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 20:04:13,895] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.559 seconds
[2022-12-04 20:04:44,615] {processor.py:153} INFO - Started process (PID=2334) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:04:44,615] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:04:44,615] {logging_mixin.py:115} INFO - [2022-12-04 20:04:44,615] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:04:45,116] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:04:45,133] {logging_mixin.py:115} INFO - [2022-12-04 20:04:45,133] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:04:45,150] {logging_mixin.py:115} INFO - [2022-12-04 20:04:45,149] {dag.py:2919} INFO - Setting next_dagrun for second_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 20:04:45,157] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.544 seconds
[2022-12-04 20:04:54,902] {processor.py:153} INFO - Started process (PID=2349) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:04:54,903] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:04:54,903] {logging_mixin.py:115} INFO - [2022-12-04 20:04:54,903] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:04:55,420] {processor.py:651} INFO - DAG(s) dict_keys(['second_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:04:55,445] {logging_mixin.py:115} INFO - [2022-12-04 20:04:55,444] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:04:55,461] {logging_mixin.py:115} INFO - [2022-12-04 20:04:55,461] {dag.py:2919} INFO - Setting next_dagrun for second_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 20:04:55,469] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.569 seconds
[2022-12-04 20:05:06,509] {processor.py:153} INFO - Started process (PID=2362) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:05:06,509] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:05:06,510] {logging_mixin.py:115} INFO - [2022-12-04 20:05:06,510] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:05:07,031] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:05:07,091] {logging_mixin.py:115} INFO - [2022-12-04 20:05:07,090] {manager.py:508} INFO - Created Permission View: can delete on DAG:third_test_s3_file_sensor
[2022-12-04 20:05:07,097] {logging_mixin.py:115} INFO - [2022-12-04 20:05:07,097] {manager.py:508} INFO - Created Permission View: can read on DAG:third_test_s3_file_sensor
[2022-12-04 20:05:07,101] {logging_mixin.py:115} INFO - [2022-12-04 20:05:07,101] {manager.py:508} INFO - Created Permission View: can edit on DAG:third_test_s3_file_sensor
[2022-12-04 20:05:07,102] {logging_mixin.py:115} INFO - [2022-12-04 20:05:07,101] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:05:07,108] {logging_mixin.py:115} INFO - [2022-12-04 20:05:07,108] {dag.py:2390} INFO - Creating ORM DAG for third_test_s3_file_sensor
[2022-12-04 20:05:07,117] {logging_mixin.py:115} INFO - [2022-12-04 20:05:07,117] {dag.py:2919} INFO - Setting next_dagrun for third_test_s3_file_sensor to 2022-12-03T00:00:00+00:00, run_after=2022-12-04T00:00:00+00:00
[2022-12-04 20:05:07,127] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.621 seconds
[2022-12-04 20:05:24,172] {processor.py:153} INFO - Started process (PID=2417) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:05:24,172] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:05:24,173] {logging_mixin.py:115} INFO - [2022-12-04 20:05:24,173] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:05:24,670] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:05:24,673] {logging_mixin.py:115} INFO - [2022-12-04 20:05:24,672] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:05:24,689] {logging_mixin.py:115} INFO - [2022-12-04 20:05:24,689] {dag.py:2919} INFO - Setting next_dagrun for third_test_s3_file_sensor to 2022-12-03T00:00:00+00:00, run_after=2022-12-04T00:00:00+00:00
[2022-12-04 20:05:24,698] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.528 seconds
[2022-12-04 20:05:54,939] {processor.py:153} INFO - Started process (PID=2474) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:05:54,940] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:05:54,940] {logging_mixin.py:115} INFO - [2022-12-04 20:05:54,940] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:05:55,475] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:05:55,493] {logging_mixin.py:115} INFO - [2022-12-04 20:05:55,492] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:05:55,509] {logging_mixin.py:115} INFO - [2022-12-04 20:05:55,509] {dag.py:2919} INFO - Setting next_dagrun for third_test_s3_file_sensor to 2022-12-03T00:00:00+00:00, run_after=2022-12-04T00:00:00+00:00
[2022-12-04 20:05:55,515] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.579 seconds
[2022-12-04 20:06:26,300] {processor.py:153} INFO - Started process (PID=2540) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:06:26,300] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:06:26,301] {logging_mixin.py:115} INFO - [2022-12-04 20:06:26,301] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:06:26,875] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:06:26,895] {logging_mixin.py:115} INFO - [2022-12-04 20:06:26,894] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:06:26,910] {logging_mixin.py:115} INFO - [2022-12-04 20:06:26,910] {dag.py:2919} INFO - Setting next_dagrun for third_test_s3_file_sensor to 2022-12-03T00:00:00+00:00, run_after=2022-12-04T00:00:00+00:00
[2022-12-04 20:06:26,918] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.620 seconds
[2022-12-04 20:06:57,576] {processor.py:153} INFO - Started process (PID=2606) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:06:57,577] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:06:57,577] {logging_mixin.py:115} INFO - [2022-12-04 20:06:57,577] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:06:58,080] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:06:58,098] {logging_mixin.py:115} INFO - [2022-12-04 20:06:58,098] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:06:58,114] {logging_mixin.py:115} INFO - [2022-12-04 20:06:58,114] {dag.py:2919} INFO - Setting next_dagrun for third_test_s3_file_sensor to 2022-12-03T00:00:00+00:00, run_after=2022-12-04T00:00:00+00:00
[2022-12-04 20:06:58,121] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.547 seconds
[2022-12-04 20:07:28,845] {processor.py:153} INFO - Started process (PID=2672) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:07:28,845] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:07:28,846] {logging_mixin.py:115} INFO - [2022-12-04 20:07:28,846] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:07:29,348] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:07:29,366] {logging_mixin.py:115} INFO - [2022-12-04 20:07:29,366] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:07:29,382] {logging_mixin.py:115} INFO - [2022-12-04 20:07:29,381] {dag.py:2919} INFO - Setting next_dagrun for third_test_s3_file_sensor to 2022-12-03T00:00:00+00:00, run_after=2022-12-04T00:00:00+00:00
[2022-12-04 20:07:29,389] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.546 seconds
[2022-12-04 20:08:00,201] {processor.py:153} INFO - Started process (PID=2729) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:08:00,201] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:08:00,202] {logging_mixin.py:115} INFO - [2022-12-04 20:08:00,202] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:08:00,717] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:08:00,735] {logging_mixin.py:115} INFO - [2022-12-04 20:08:00,735] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:08:00,750] {logging_mixin.py:115} INFO - [2022-12-04 20:08:00,750] {dag.py:2919} INFO - Setting next_dagrun for third_test_s3_file_sensor to 2022-12-03T00:00:00+00:00, run_after=2022-12-04T00:00:00+00:00
[2022-12-04 20:08:00,757] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.558 seconds
[2022-12-04 20:08:31,078] {processor.py:153} INFO - Started process (PID=2795) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:08:31,079] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:08:31,079] {logging_mixin.py:115} INFO - [2022-12-04 20:08:31,079] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:08:31,577] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:08:31,596] {logging_mixin.py:115} INFO - [2022-12-04 20:08:31,595] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:08:31,611] {logging_mixin.py:115} INFO - [2022-12-04 20:08:31,611] {dag.py:2919} INFO - Setting next_dagrun for third_test_s3_file_sensor to 2022-12-03T00:00:00+00:00, run_after=2022-12-04T00:00:00+00:00
[2022-12-04 20:08:31,618] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.542 seconds
[2022-12-04 20:09:02,346] {processor.py:153} INFO - Started process (PID=2861) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:09:02,347] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:09:02,347] {logging_mixin.py:115} INFO - [2022-12-04 20:09:02,347] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:09:02,852] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:09:02,871] {logging_mixin.py:115} INFO - [2022-12-04 20:09:02,870] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:09:02,886] {logging_mixin.py:115} INFO - [2022-12-04 20:09:02,886] {dag.py:2919} INFO - Setting next_dagrun for third_test_s3_file_sensor to 2022-12-03T00:00:00+00:00, run_after=2022-12-04T00:00:00+00:00
[2022-12-04 20:09:02,893] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.549 seconds
[2022-12-04 20:09:33,792] {processor.py:153} INFO - Started process (PID=2927) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:09:33,792] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:09:33,793] {logging_mixin.py:115} INFO - [2022-12-04 20:09:33,792] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:09:34,316] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:09:34,335] {logging_mixin.py:115} INFO - [2022-12-04 20:09:34,334] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:09:34,351] {logging_mixin.py:115} INFO - [2022-12-04 20:09:34,350] {dag.py:2919} INFO - Setting next_dagrun for third_test_s3_file_sensor to 2022-12-03T00:00:00+00:00, run_after=2022-12-04T00:00:00+00:00
[2022-12-04 20:09:34,358] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.569 seconds
[2022-12-04 20:10:04,627] {processor.py:153} INFO - Started process (PID=2984) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:10:04,628] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:10:04,628] {logging_mixin.py:115} INFO - [2022-12-04 20:10:04,628] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:10:05,143] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:10:05,162] {logging_mixin.py:115} INFO - [2022-12-04 20:10:05,161] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:10:05,178] {logging_mixin.py:115} INFO - [2022-12-04 20:10:05,178] {dag.py:2919} INFO - Setting next_dagrun for third_test_s3_file_sensor to 2022-12-03T00:00:00+00:00, run_after=2022-12-04T00:00:00+00:00
[2022-12-04 20:10:05,185] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.560 seconds
[2022-12-04 20:10:35,892] {processor.py:153} INFO - Started process (PID=3050) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:10:35,892] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:10:35,893] {logging_mixin.py:115} INFO - [2022-12-04 20:10:35,893] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:10:36,403] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:10:36,421] {logging_mixin.py:115} INFO - [2022-12-04 20:10:36,421] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:10:36,437] {logging_mixin.py:115} INFO - [2022-12-04 20:10:36,436] {dag.py:2919} INFO - Setting next_dagrun for third_test_s3_file_sensor to 2022-12-03T00:00:00+00:00, run_after=2022-12-04T00:00:00+00:00
[2022-12-04 20:10:36,444] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.554 seconds
[2022-12-04 20:11:50,814] {processor.py:153} INFO - Started process (PID=46) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:11:50,815] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:11:50,815] {logging_mixin.py:115} INFO - [2022-12-04 20:11:50,815] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:11:52,173] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:11:52,195] {logging_mixin.py:115} INFO - [2022-12-04 20:11:52,194] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:11:52,211] {logging_mixin.py:115} INFO - [2022-12-04 20:11:52,211] {dag.py:2919} INFO - Setting next_dagrun for third_test_s3_file_sensor to 2022-12-03T00:00:00+00:00, run_after=2022-12-04T00:00:00+00:00
[2022-12-04 20:11:52,218] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 1.406 seconds
[2022-12-04 20:12:22,809] {processor.py:153} INFO - Started process (PID=112) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:12:22,810] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:12:22,810] {logging_mixin.py:115} INFO - [2022-12-04 20:12:22,810] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:12:23,323] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:12:23,341] {logging_mixin.py:115} INFO - [2022-12-04 20:12:23,341] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:12:23,356] {logging_mixin.py:115} INFO - [2022-12-04 20:12:23,356] {dag.py:2919} INFO - Setting next_dagrun for third_test_s3_file_sensor to 2022-12-03T00:00:00+00:00, run_after=2022-12-04T00:00:00+00:00
[2022-12-04 20:12:23,363] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.556 seconds
[2022-12-04 20:12:53,769] {processor.py:153} INFO - Started process (PID=182) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:12:53,770] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:12:53,770] {logging_mixin.py:115} INFO - [2022-12-04 20:12:53,770] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:12:54,294] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:12:54,313] {logging_mixin.py:115} INFO - [2022-12-04 20:12:54,313] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:12:54,336] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.569 seconds
[2022-12-04 20:13:24,837] {processor.py:153} INFO - Started process (PID=248) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:13:24,837] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:13:24,838] {logging_mixin.py:115} INFO - [2022-12-04 20:13:24,837] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:13:25,362] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:13:25,382] {logging_mixin.py:115} INFO - [2022-12-04 20:13:25,381] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:13:25,404] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.569 seconds
[2022-12-04 20:13:55,699] {processor.py:153} INFO - Started process (PID=305) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:13:55,700] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:13:55,700] {logging_mixin.py:115} INFO - [2022-12-04 20:13:55,700] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:13:56,208] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:13:56,226] {logging_mixin.py:115} INFO - [2022-12-04 20:13:56,226] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:13:56,247] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.550 seconds
[2022-12-04 20:14:26,565] {processor.py:153} INFO - Started process (PID=371) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:14:26,565] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:14:26,566] {logging_mixin.py:115} INFO - [2022-12-04 20:14:26,566] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:14:27,079] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:14:27,098] {logging_mixin.py:115} INFO - [2022-12-04 20:14:27,097] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:14:27,119] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.556 seconds
[2022-12-04 20:14:57,472] {processor.py:153} INFO - Started process (PID=437) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:14:57,472] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:14:57,473] {logging_mixin.py:115} INFO - [2022-12-04 20:14:57,472] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:14:57,991] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:14:58,009] {logging_mixin.py:115} INFO - [2022-12-04 20:14:58,009] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:14:58,030] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.561 seconds
[2022-12-04 20:15:28,413] {processor.py:153} INFO - Started process (PID=503) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:15:28,414] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:15:28,414] {logging_mixin.py:115} INFO - [2022-12-04 20:15:28,414] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:15:28,920] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:15:28,938] {logging_mixin.py:115} INFO - [2022-12-04 20:15:28,938] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:15:28,959] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.548 seconds
[2022-12-04 20:15:59,323] {processor.py:153} INFO - Started process (PID=560) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:15:59,324] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:15:59,324] {logging_mixin.py:115} INFO - [2022-12-04 20:15:59,324] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:15:59,830] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:15:59,849] {logging_mixin.py:115} INFO - [2022-12-04 20:15:59,848] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:15:59,869] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.549 seconds
[2022-12-04 20:16:30,231] {processor.py:153} INFO - Started process (PID=626) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:16:30,231] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:16:30,232] {logging_mixin.py:115} INFO - [2022-12-04 20:16:30,232] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:16:30,741] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:16:30,760] {logging_mixin.py:115} INFO - [2022-12-04 20:16:30,759] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:16:30,780] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.552 seconds
[2022-12-04 20:18:08,636] {processor.py:153} INFO - Started process (PID=46) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:18:08,637] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:18:08,637] {logging_mixin.py:115} INFO - [2022-12-04 20:18:08,637] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:18:09,983] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:18:10,004] {logging_mixin.py:115} INFO - [2022-12-04 20:18:10,003] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:18:10,025] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 1.392 seconds
[2022-12-04 20:18:40,719] {processor.py:153} INFO - Started process (PID=116) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:18:40,719] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:18:40,720] {logging_mixin.py:115} INFO - [2022-12-04 20:18:40,720] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:18:41,260] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:18:41,279] {logging_mixin.py:115} INFO - [2022-12-04 20:18:41,279] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:18:41,301] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.585 seconds
[2022-12-04 20:19:11,672] {processor.py:153} INFO - Started process (PID=182) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:19:11,672] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:19:11,673] {logging_mixin.py:115} INFO - [2022-12-04 20:19:11,673] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:19:12,187] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:19:12,206] {logging_mixin.py:115} INFO - [2022-12-04 20:19:12,205] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:19:12,227] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.557 seconds
[2022-12-04 20:19:42,538] {processor.py:153} INFO - Started process (PID=248) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:19:42,538] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:19:42,538] {logging_mixin.py:115} INFO - [2022-12-04 20:19:42,538] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:19:43,050] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:19:43,069] {logging_mixin.py:115} INFO - [2022-12-04 20:19:43,068] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:19:43,089] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.554 seconds
[2022-12-04 20:20:13,578] {processor.py:153} INFO - Started process (PID=305) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:20:13,578] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:20:13,579] {logging_mixin.py:115} INFO - [2022-12-04 20:20:13,579] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:20:14,087] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:20:14,106] {logging_mixin.py:115} INFO - [2022-12-04 20:20:14,105] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:20:14,126] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.551 seconds
[2022-12-04 20:20:44,527] {processor.py:153} INFO - Started process (PID=371) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:20:44,527] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:20:44,527] {logging_mixin.py:115} INFO - [2022-12-04 20:20:44,527] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:20:45,040] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:20:45,059] {logging_mixin.py:115} INFO - [2022-12-04 20:20:45,059] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:20:45,080] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.555 seconds
[2022-12-04 20:21:15,457] {processor.py:153} INFO - Started process (PID=437) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:21:15,457] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:21:15,458] {logging_mixin.py:115} INFO - [2022-12-04 20:21:15,458] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:21:15,972] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:21:15,991] {logging_mixin.py:115} INFO - [2022-12-04 20:21:15,990] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:21:16,011] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.557 seconds
[2022-12-04 20:21:46,395] {processor.py:153} INFO - Started process (PID=503) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:21:46,396] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:21:46,397] {logging_mixin.py:115} INFO - [2022-12-04 20:21:46,396] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:21:46,910] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:21:46,929] {logging_mixin.py:115} INFO - [2022-12-04 20:21:46,928] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:21:46,950] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.557 seconds
[2022-12-04 20:22:17,420] {processor.py:153} INFO - Started process (PID=560) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:22:17,421] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:22:17,421] {logging_mixin.py:115} INFO - [2022-12-04 20:22:17,421] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:22:17,927] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:22:17,945] {logging_mixin.py:115} INFO - [2022-12-04 20:22:17,945] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:22:17,966] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.548 seconds
[2022-12-04 20:22:48,314] {processor.py:153} INFO - Started process (PID=626) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:22:48,315] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:22:48,315] {logging_mixin.py:115} INFO - [2022-12-04 20:22:48,315] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:22:48,827] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:22:48,845] {logging_mixin.py:115} INFO - [2022-12-04 20:22:48,845] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:22:48,866] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.554 seconds
[2022-12-04 20:23:19,233] {processor.py:153} INFO - Started process (PID=696) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:23:19,233] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:23:19,234] {logging_mixin.py:115} INFO - [2022-12-04 20:23:19,234] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:23:19,746] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:23:19,765] {logging_mixin.py:115} INFO - [2022-12-04 20:23:19,764] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:23:19,780] {logging_mixin.py:115} INFO - [2022-12-04 20:23:19,780] {dag.py:2919} INFO - Setting next_dagrun for third_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 20:23:19,787] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.556 seconds
[2022-12-04 20:23:50,187] {processor.py:153} INFO - Started process (PID=762) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:23:50,187] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:23:50,188] {logging_mixin.py:115} INFO - [2022-12-04 20:23:50,188] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:23:50,698] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:23:50,717] {logging_mixin.py:115} INFO - [2022-12-04 20:23:50,716] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:23:50,732] {logging_mixin.py:115} INFO - [2022-12-04 20:23:50,732] {dag.py:2919} INFO - Setting next_dagrun for third_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 20:23:50,739] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.555 seconds
[2022-12-04 20:24:21,120] {processor.py:153} INFO - Started process (PID=819) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:24:21,121] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:24:21,121] {logging_mixin.py:115} INFO - [2022-12-04 20:24:21,121] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:24:21,634] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:24:21,653] {logging_mixin.py:115} INFO - [2022-12-04 20:24:21,652] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:24:21,668] {logging_mixin.py:115} INFO - [2022-12-04 20:24:21,668] {dag.py:2919} INFO - Setting next_dagrun for third_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 20:24:21,675] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.557 seconds
[2022-12-04 20:24:25,977] {processor.py:153} INFO - Started process (PID=867) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:24:25,978] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:24:25,978] {logging_mixin.py:115} INFO - [2022-12-04 20:24:25,978] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:24:26,496] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:24:26,543] {logging_mixin.py:115} INFO - [2022-12-04 20:24:26,542] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:24:26,557] {logging_mixin.py:115} INFO - [2022-12-04 20:24:26,557] {dag.py:2919} INFO - Setting next_dagrun for third_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 20:24:26,566] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.592 seconds
[2022-12-04 20:24:31,593] {processor.py:153} INFO - Started process (PID=869) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:24:31,594] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:24:31,594] {logging_mixin.py:115} INFO - [2022-12-04 20:24:31,594] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:24:32,086] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:24:32,089] {logging_mixin.py:115} INFO - [2022-12-04 20:24:32,088] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:24:32,105] {logging_mixin.py:115} INFO - [2022-12-04 20:24:32,105] {dag.py:2919} INFO - Setting next_dagrun for third_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 20:24:32,113] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.522 seconds
[2022-12-04 20:24:37,010] {processor.py:153} INFO - Started process (PID=880) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:24:37,011] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:24:37,011] {logging_mixin.py:115} INFO - [2022-12-04 20:24:37,011] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:24:37,509] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:24:37,512] {logging_mixin.py:115} INFO - [2022-12-04 20:24:37,512] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:24:37,529] {logging_mixin.py:115} INFO - [2022-12-04 20:24:37,529] {dag.py:2919} INFO - Setting next_dagrun for third_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 20:24:37,537] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.529 seconds
[2022-12-04 20:25:07,686] {processor.py:153} INFO - Started process (PID=937) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:25:07,687] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:25:07,687] {logging_mixin.py:115} INFO - [2022-12-04 20:25:07,687] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:25:08,196] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:25:08,215] {logging_mixin.py:115} INFO - [2022-12-04 20:25:08,215] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:25:08,231] {logging_mixin.py:115} INFO - [2022-12-04 20:25:08,231] {dag.py:2919} INFO - Setting next_dagrun for third_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 20:25:08,238] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.554 seconds
[2022-12-04 20:25:38,948] {processor.py:153} INFO - Started process (PID=1003) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:25:38,949] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:25:38,949] {logging_mixin.py:115} INFO - [2022-12-04 20:25:38,949] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:25:39,479] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:25:39,498] {logging_mixin.py:115} INFO - [2022-12-04 20:25:39,497] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:25:39,513] {logging_mixin.py:115} INFO - [2022-12-04 20:25:39,513] {dag.py:2919} INFO - Setting next_dagrun for third_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 20:25:39,520] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.574 seconds
[2022-12-04 20:26:10,116] {processor.py:153} INFO - Started process (PID=1069) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:26:10,117] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:26:10,117] {logging_mixin.py:115} INFO - [2022-12-04 20:26:10,117] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:26:10,621] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:26:10,639] {logging_mixin.py:115} INFO - [2022-12-04 20:26:10,638] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:26:10,654] {logging_mixin.py:115} INFO - [2022-12-04 20:26:10,654] {dag.py:2919} INFO - Setting next_dagrun for third_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 20:26:10,661] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.547 seconds
[2022-12-04 20:26:41,395] {processor.py:153} INFO - Started process (PID=1135) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:26:41,395] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:26:41,396] {logging_mixin.py:115} INFO - [2022-12-04 20:26:41,395] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:26:41,895] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:26:41,914] {logging_mixin.py:115} INFO - [2022-12-04 20:26:41,914] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:26:41,929] {logging_mixin.py:115} INFO - [2022-12-04 20:26:41,929] {dag.py:2919} INFO - Setting next_dagrun for third_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 20:26:41,937] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.544 seconds
[2022-12-04 20:27:12,597] {processor.py:153} INFO - Started process (PID=1201) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:27:12,598] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:27:12,598] {logging_mixin.py:115} INFO - [2022-12-04 20:27:12,598] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:27:13,113] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:27:13,132] {logging_mixin.py:115} INFO - [2022-12-04 20:27:13,131] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:27:13,147] {logging_mixin.py:115} INFO - [2022-12-04 20:27:13,147] {dag.py:2919} INFO - Setting next_dagrun for third_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 20:27:13,154] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.559 seconds
[2022-12-04 20:27:43,722] {processor.py:153} INFO - Started process (PID=1258) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:27:43,722] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:27:43,722] {logging_mixin.py:115} INFO - [2022-12-04 20:27:43,722] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:27:44,228] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:27:44,247] {logging_mixin.py:115} INFO - [2022-12-04 20:27:44,246] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:27:44,262] {logging_mixin.py:115} INFO - [2022-12-04 20:27:44,262] {dag.py:2919} INFO - Setting next_dagrun for third_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 20:27:44,269] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.550 seconds
[2022-12-04 20:28:14,901] {processor.py:153} INFO - Started process (PID=1325) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:28:14,901] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:28:14,902] {logging_mixin.py:115} INFO - [2022-12-04 20:28:14,902] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:28:15,422] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:28:15,441] {logging_mixin.py:115} INFO - [2022-12-04 20:28:15,440] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:28:15,456] {logging_mixin.py:115} INFO - [2022-12-04 20:28:15,456] {dag.py:2919} INFO - Setting next_dagrun for third_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 20:28:15,463] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.565 seconds
[2022-12-04 20:28:45,987] {processor.py:153} INFO - Started process (PID=1391) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:28:45,988] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:28:45,988] {logging_mixin.py:115} INFO - [2022-12-04 20:28:45,988] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:28:46,517] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:28:46,536] {logging_mixin.py:115} INFO - [2022-12-04 20:28:46,535] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:28:46,551] {logging_mixin.py:115} INFO - [2022-12-04 20:28:46,551] {dag.py:2919} INFO - Setting next_dagrun for third_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 20:28:46,558] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.573 seconds
[2022-12-04 20:29:16,976] {processor.py:153} INFO - Started process (PID=1457) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:29:16,977] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:29:16,977] {logging_mixin.py:115} INFO - [2022-12-04 20:29:16,977] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:29:17,510] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:29:17,529] {logging_mixin.py:115} INFO - [2022-12-04 20:29:17,528] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:29:17,544] {logging_mixin.py:115} INFO - [2022-12-04 20:29:17,544] {dag.py:2919} INFO - Setting next_dagrun for third_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 20:29:17,551] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.577 seconds
[2022-12-04 20:29:47,844] {processor.py:153} INFO - Started process (PID=1514) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:29:47,844] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:29:47,845] {logging_mixin.py:115} INFO - [2022-12-04 20:29:47,845] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:29:48,357] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:29:48,375] {logging_mixin.py:115} INFO - [2022-12-04 20:29:48,374] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:29:48,390] {logging_mixin.py:115} INFO - [2022-12-04 20:29:48,390] {dag.py:2919} INFO - Setting next_dagrun for third_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 20:29:48,397] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.556 seconds
[2022-12-04 20:30:18,584] {processor.py:153} INFO - Started process (PID=1580) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:30:18,584] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:30:18,585] {logging_mixin.py:115} INFO - [2022-12-04 20:30:18,585] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:30:19,099] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:30:19,118] {logging_mixin.py:115} INFO - [2022-12-04 20:30:19,117] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:30:19,133] {logging_mixin.py:115} INFO - [2022-12-04 20:30:19,133] {dag.py:2919} INFO - Setting next_dagrun for third_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 20:30:19,140] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.558 seconds
[2022-12-04 20:30:49,368] {processor.py:153} INFO - Started process (PID=1646) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:30:49,368] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:30:49,369] {logging_mixin.py:115} INFO - [2022-12-04 20:30:49,369] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:30:49,886] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:30:49,904] {logging_mixin.py:115} INFO - [2022-12-04 20:30:49,904] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:30:49,920] {logging_mixin.py:115} INFO - [2022-12-04 20:30:49,920] {dag.py:2919} INFO - Setting next_dagrun for third_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 20:30:49,927] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.561 seconds
[2022-12-04 20:31:20,121] {processor.py:153} INFO - Started process (PID=1709) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:31:20,122] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:31:20,122] {logging_mixin.py:115} INFO - [2022-12-04 20:31:20,122] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:31:20,632] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:31:20,652] {logging_mixin.py:115} INFO - [2022-12-04 20:31:20,651] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:31:20,667] {logging_mixin.py:115} INFO - [2022-12-04 20:31:20,667] {dag.py:2919} INFO - Setting next_dagrun for third_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 20:31:20,675] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.556 seconds
[2022-12-04 20:31:50,827] {processor.py:153} INFO - Started process (PID=1769) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:31:50,828] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:31:50,828] {logging_mixin.py:115} INFO - [2022-12-04 20:31:50,828] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:31:51,343] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:31:51,361] {logging_mixin.py:115} INFO - [2022-12-04 20:31:51,361] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:31:51,376] {logging_mixin.py:115} INFO - [2022-12-04 20:31:51,376] {dag.py:2919} INFO - Setting next_dagrun for third_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 20:31:51,383] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.559 seconds
[2022-12-04 20:32:21,496] {processor.py:153} INFO - Started process (PID=1835) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:32:21,497] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:32:21,497] {logging_mixin.py:115} INFO - [2022-12-04 20:32:21,497] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:32:22,006] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:32:22,025] {logging_mixin.py:115} INFO - [2022-12-04 20:32:22,024] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:32:22,040] {logging_mixin.py:115} INFO - [2022-12-04 20:32:22,040] {dag.py:2919} INFO - Setting next_dagrun for third_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 20:32:22,047] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.553 seconds
[2022-12-04 20:32:52,205] {processor.py:153} INFO - Started process (PID=1901) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:32:52,206] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:32:52,206] {logging_mixin.py:115} INFO - [2022-12-04 20:32:52,206] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:32:52,708] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:32:52,726] {logging_mixin.py:115} INFO - [2022-12-04 20:32:52,726] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:32:52,741] {logging_mixin.py:115} INFO - [2022-12-04 20:32:52,741] {dag.py:2919} INFO - Setting next_dagrun for third_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 20:32:52,748] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.546 seconds
[2022-12-04 20:33:23,071] {processor.py:153} INFO - Started process (PID=1958) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:33:23,072] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:33:23,072] {logging_mixin.py:115} INFO - [2022-12-04 20:33:23,072] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:33:23,590] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:33:23,608] {logging_mixin.py:115} INFO - [2022-12-04 20:33:23,607] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:33:23,623] {logging_mixin.py:115} INFO - [2022-12-04 20:33:23,623] {dag.py:2919} INFO - Setting next_dagrun for third_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 20:33:23,630] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.562 seconds
[2022-12-04 20:33:53,778] {processor.py:153} INFO - Started process (PID=2024) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:33:53,779] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:33:53,779] {logging_mixin.py:115} INFO - [2022-12-04 20:33:53,779] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:33:54,286] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:33:54,304] {logging_mixin.py:115} INFO - [2022-12-04 20:33:54,303] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:33:54,319] {logging_mixin.py:115} INFO - [2022-12-04 20:33:54,319] {dag.py:2919} INFO - Setting next_dagrun for third_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 20:33:54,326] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.550 seconds
[2022-12-04 20:34:24,518] {processor.py:153} INFO - Started process (PID=2090) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:34:24,518] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:34:24,519] {logging_mixin.py:115} INFO - [2022-12-04 20:34:24,519] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:34:25,019] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:34:25,036] {logging_mixin.py:115} INFO - [2022-12-04 20:34:25,036] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:34:25,051] {logging_mixin.py:115} INFO - [2022-12-04 20:34:25,051] {dag.py:2919} INFO - Setting next_dagrun for third_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 20:34:25,058] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.543 seconds
[2022-12-04 20:34:55,253] {processor.py:153} INFO - Started process (PID=2156) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:34:55,253] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:34:55,254] {logging_mixin.py:115} INFO - [2022-12-04 20:34:55,254] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:34:55,792] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:34:55,811] {logging_mixin.py:115} INFO - [2022-12-04 20:34:55,810] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:34:55,826] {logging_mixin.py:115} INFO - [2022-12-04 20:34:55,826] {dag.py:2919} INFO - Setting next_dagrun for third_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 20:34:55,834] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.584 seconds
[2022-12-04 20:35:25,940] {processor.py:153} INFO - Started process (PID=2214) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:35:25,940] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:35:25,941] {logging_mixin.py:115} INFO - [2022-12-04 20:35:25,941] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:35:26,450] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:35:26,467] {logging_mixin.py:115} INFO - [2022-12-04 20:35:26,467] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:35:26,482] {logging_mixin.py:115} INFO - [2022-12-04 20:35:26,482] {dag.py:2919} INFO - Setting next_dagrun for third_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 20:35:26,489] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.552 seconds
[2022-12-04 20:35:56,735] {processor.py:153} INFO - Started process (PID=2280) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:35:56,736] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:35:56,736] {logging_mixin.py:115} INFO - [2022-12-04 20:35:56,736] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:35:57,253] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:35:57,271] {logging_mixin.py:115} INFO - [2022-12-04 20:35:57,270] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:35:57,286] {logging_mixin.py:115} INFO - [2022-12-04 20:35:57,286] {dag.py:2919} INFO - Setting next_dagrun for third_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 20:35:57,293] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.560 seconds
[2022-12-04 20:36:27,464] {processor.py:153} INFO - Started process (PID=2346) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:36:27,464] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:36:27,465] {logging_mixin.py:115} INFO - [2022-12-04 20:36:27,465] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:36:27,988] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:36:28,006] {logging_mixin.py:115} INFO - [2022-12-04 20:36:28,005] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:36:28,021] {logging_mixin.py:115} INFO - [2022-12-04 20:36:28,021] {dag.py:2919} INFO - Setting next_dagrun for third_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 20:36:28,027] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.566 seconds
[2022-12-04 20:36:58,100] {processor.py:153} INFO - Started process (PID=2403) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:36:58,101] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:36:58,101] {logging_mixin.py:115} INFO - [2022-12-04 20:36:58,101] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:36:58,622] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:36:58,640] {logging_mixin.py:115} INFO - [2022-12-04 20:36:58,639] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:36:58,655] {logging_mixin.py:115} INFO - [2022-12-04 20:36:58,655] {dag.py:2919} INFO - Setting next_dagrun for third_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 20:36:58,662] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.564 seconds
[2022-12-04 20:37:28,900] {processor.py:153} INFO - Started process (PID=2469) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:37:28,901] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:37:28,901] {logging_mixin.py:115} INFO - [2022-12-04 20:37:28,901] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:37:29,417] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:37:29,434] {logging_mixin.py:115} INFO - [2022-12-04 20:37:29,434] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:37:29,449] {logging_mixin.py:115} INFO - [2022-12-04 20:37:29,449] {dag.py:2919} INFO - Setting next_dagrun for third_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 20:37:29,456] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.559 seconds
[2022-12-04 20:37:59,598] {processor.py:153} INFO - Started process (PID=2535) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:37:59,598] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:37:59,598] {logging_mixin.py:115} INFO - [2022-12-04 20:37:59,598] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:38:00,111] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:38:00,128] {logging_mixin.py:115} INFO - [2022-12-04 20:38:00,128] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:38:00,144] {logging_mixin.py:115} INFO - [2022-12-04 20:38:00,144] {dag.py:2919} INFO - Setting next_dagrun for third_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 20:38:00,150] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.555 seconds
[2022-12-04 20:38:30,450] {processor.py:153} INFO - Started process (PID=2601) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:38:30,451] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:38:30,451] {logging_mixin.py:115} INFO - [2022-12-04 20:38:30,451] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:38:30,960] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:38:30,977] {logging_mixin.py:115} INFO - [2022-12-04 20:38:30,977] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:38:30,992] {logging_mixin.py:115} INFO - [2022-12-04 20:38:30,992] {dag.py:2919} INFO - Setting next_dagrun for third_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 20:38:31,000] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.552 seconds
[2022-12-04 20:39:01,150] {processor.py:153} INFO - Started process (PID=2658) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:39:01,150] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:39:01,150] {logging_mixin.py:115} INFO - [2022-12-04 20:39:01,150] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:39:01,664] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:39:01,682] {logging_mixin.py:115} INFO - [2022-12-04 20:39:01,681] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:39:01,697] {logging_mixin.py:115} INFO - [2022-12-04 20:39:01,697] {dag.py:2919} INFO - Setting next_dagrun for third_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 20:39:01,704] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.557 seconds
[2022-12-04 20:39:31,838] {processor.py:153} INFO - Started process (PID=2724) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:39:31,839] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:39:31,839] {logging_mixin.py:115} INFO - [2022-12-04 20:39:31,839] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:39:32,333] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:39:32,351] {logging_mixin.py:115} INFO - [2022-12-04 20:39:32,350] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:39:32,366] {logging_mixin.py:115} INFO - [2022-12-04 20:39:32,366] {dag.py:2919} INFO - Setting next_dagrun for third_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 20:39:32,373] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.538 seconds
[2022-12-04 20:40:02,499] {processor.py:153} INFO - Started process (PID=2790) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:40:02,499] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:40:02,500] {logging_mixin.py:115} INFO - [2022-12-04 20:40:02,500] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:40:03,014] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:40:03,032] {logging_mixin.py:115} INFO - [2022-12-04 20:40:03,031] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:40:03,047] {logging_mixin.py:115} INFO - [2022-12-04 20:40:03,047] {dag.py:2919} INFO - Setting next_dagrun for third_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 20:40:03,054] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.557 seconds
[2022-12-04 20:40:33,138] {processor.py:153} INFO - Started process (PID=2854) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:40:33,139] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:40:33,139] {logging_mixin.py:115} INFO - [2022-12-04 20:40:33,139] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:40:33,645] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:40:33,662] {logging_mixin.py:115} INFO - [2022-12-04 20:40:33,662] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:40:33,678] {logging_mixin.py:115} INFO - [2022-12-04 20:40:33,678] {dag.py:2919} INFO - Setting next_dagrun for third_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 20:40:33,685] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.549 seconds
[2022-12-04 20:41:03,845] {processor.py:153} INFO - Started process (PID=2913) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:41:03,846] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:41:03,846] {logging_mixin.py:115} INFO - [2022-12-04 20:41:03,846] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:41:04,364] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:41:04,382] {logging_mixin.py:115} INFO - [2022-12-04 20:41:04,381] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:41:04,397] {logging_mixin.py:115} INFO - [2022-12-04 20:41:04,397] {dag.py:2919} INFO - Setting next_dagrun for third_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 20:41:04,404] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.562 seconds
[2022-12-04 20:41:34,530] {processor.py:153} INFO - Started process (PID=2979) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:41:34,531] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:41:34,531] {logging_mixin.py:115} INFO - [2022-12-04 20:41:34,531] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:41:35,042] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:41:35,060] {logging_mixin.py:115} INFO - [2022-12-04 20:41:35,059] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:41:35,075] {logging_mixin.py:115} INFO - [2022-12-04 20:41:35,075] {dag.py:2919} INFO - Setting next_dagrun for third_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 20:41:35,082] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.554 seconds
[2022-12-04 20:42:05,297] {processor.py:153} INFO - Started process (PID=3045) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:42:05,297] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:42:05,298] {logging_mixin.py:115} INFO - [2022-12-04 20:42:05,297] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:42:05,815] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:42:05,833] {logging_mixin.py:115} INFO - [2022-12-04 20:42:05,832] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:42:05,848] {logging_mixin.py:115} INFO - [2022-12-04 20:42:05,848] {dag.py:2919} INFO - Setting next_dagrun for third_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 20:42:05,855] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.561 seconds
[2022-12-04 20:42:36,064] {processor.py:153} INFO - Started process (PID=3102) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:42:36,065] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:42:36,065] {logging_mixin.py:115} INFO - [2022-12-04 20:42:36,065] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:42:36,563] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:42:36,581] {logging_mixin.py:115} INFO - [2022-12-04 20:42:36,580] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:42:36,596] {logging_mixin.py:115} INFO - [2022-12-04 20:42:36,596] {dag.py:2919} INFO - Setting next_dagrun for third_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 20:42:36,603] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.542 seconds
[2022-12-04 20:43:06,753] {processor.py:153} INFO - Started process (PID=3168) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:43:06,753] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:43:06,753] {logging_mixin.py:115} INFO - [2022-12-04 20:43:06,753] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:43:07,266] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:43:07,284] {logging_mixin.py:115} INFO - [2022-12-04 20:43:07,283] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:43:07,299] {logging_mixin.py:115} INFO - [2022-12-04 20:43:07,299] {dag.py:2919} INFO - Setting next_dagrun for third_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 20:43:07,306] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.556 seconds
[2022-12-04 20:43:37,515] {processor.py:153} INFO - Started process (PID=3234) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:43:37,516] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:43:37,516] {logging_mixin.py:115} INFO - [2022-12-04 20:43:37,516] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:43:38,027] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:43:38,044] {logging_mixin.py:115} INFO - [2022-12-04 20:43:38,044] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:43:38,059] {logging_mixin.py:115} INFO - [2022-12-04 20:43:38,059] {dag.py:2919} INFO - Setting next_dagrun for third_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 20:43:38,066] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.553 seconds
[2022-12-04 20:44:08,222] {processor.py:153} INFO - Started process (PID=3300) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:44:08,222] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:44:08,223] {logging_mixin.py:115} INFO - [2022-12-04 20:44:08,223] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:44:08,763] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:44:08,781] {logging_mixin.py:115} INFO - [2022-12-04 20:44:08,780] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:44:08,796] {logging_mixin.py:115} INFO - [2022-12-04 20:44:08,796] {dag.py:2919} INFO - Setting next_dagrun for third_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 20:44:08,803] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.583 seconds
[2022-12-04 20:44:38,960] {processor.py:153} INFO - Started process (PID=3357) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:44:38,961] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:44:38,961] {logging_mixin.py:115} INFO - [2022-12-04 20:44:38,961] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:44:39,457] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:44:39,475] {logging_mixin.py:115} INFO - [2022-12-04 20:44:39,474] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:44:39,491] {logging_mixin.py:115} INFO - [2022-12-04 20:44:39,491] {dag.py:2919} INFO - Setting next_dagrun for third_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 20:44:39,498] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.540 seconds
[2022-12-04 20:45:09,721] {processor.py:153} INFO - Started process (PID=3423) to work on /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:45:09,722] {processor.py:641} INFO - Processing file /opt/airflow/dags/s3_file_sensor2.py for tasks to queue
[2022-12-04 20:45:09,722] {logging_mixin.py:115} INFO - [2022-12-04 20:45:09,722] {dagbag.py:507} INFO - Filling up the DagBag from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:45:10,221] {processor.py:651} INFO - DAG(s) dict_keys(['third_test_s3_file_sensor']) retrieved from /opt/airflow/dags/s3_file_sensor2.py
[2022-12-04 20:45:10,239] {logging_mixin.py:115} INFO - [2022-12-04 20:45:10,238] {dag.py:2371} INFO - Sync 1 DAGs
[2022-12-04 20:45:10,254] {logging_mixin.py:115} INFO - [2022-12-04 20:45:10,254] {dag.py:2919} INFO - Setting next_dagrun for third_test_s3_file_sensor to 2022-12-04T00:00:00+00:00, run_after=2022-12-05T00:00:00+00:00
[2022-12-04 20:45:10,261] {processor.py:161} INFO - Processing /opt/airflow/dags/s3_file_sensor2.py took 0.543 seconds
